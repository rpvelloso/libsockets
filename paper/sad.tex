\sloppy
\documentclass{ufscThesis}



\usepackage{graphicx}
\usepackage[labelsep=endash]{caption}
\usepackage{algorithmicx}
\usepackage[Algoritmo,ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{pdfpages}
\usepackage{enumerate}

\newtheorem{definition}{Definição}
\renewcommand{\lstlistingname}{Listagem}

\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}

\titulo{Seminário de Andamento de Doutorado: Extração de Dados Estruturados da
Web}
\autor{Roberto Panerai Velloso}
\data{21}{maio}{2016}
\orientador[Orientadora]{Profa. Dra. Carina Friedrich Dorneles}
\coordenador[Coordenadora do Programa]{Profa. Dra. Carina Friedrich Dorneles}

\documento{Tese}
\grau{Doutor em Ciência da Computação}
\departamento{Departamento de Informática e Estatística}
\curso{Programa de Pós-Graduação em Ciência da Computação}

\numerodemembrosnabanca{3}
\orientadornabanca{nao}
% \bancaMembroA{Profa. Dra. Carina Friedrich Dorneles\\Universidade Federal de %
% Santa Catarina}
\bancaMembroA[Universidade Federal do Rio Grande do Sul]{Profa. Dra. Renata
Galante} \bancaMembroB[\ABNTinstituicaodata]{Prof. Dr. Renato Fileto}
\bancaMembroC[\ABNTinstituicaodata]{Prof. Dr. Ronaldo dos Santos Mello}

\dedicatoria{}

\agradecimento{}

\epigrafe{}{}

\textoResumo{ A extração de dados estruturados, apesar de ser um assunto
bastante estudado, ainda representa um problema em aberto e também apresenta
outras questões ainda não totalmente esgotadas como, por exemplo, a utilização
de métricas mais adequadas e \textit{baseline} e/ou \textit{datasets}
padronizados para facilitar a comparação entre as diversas propostas. Durante o
levantamento do estado da arte a respeito do tema ``extração de dados
estruturados da \textit{web}'', foram identificados, entre os trabalhos mais
relevantes, três  grupos  principais  de  abordagens:
extração de listas e tabelas; extração a partir de uma única página e extração a
partir de múltiplas páginas. Em vista dos desafios existentes, esta pesquisa
apresenta uma proposta, inédita, que aborda o problema de extração a partir de
uma única página. A abordagem converte a árvore DOM da página \textit{web} para
uma sequência de \textit{tag paths} e trata esta sequência como um ``sinal
discreto'' a partir do qual são identificadas as regiões com conteúdo
estruturado, através da utilização de técnicas de processamento de sinais
discretos (FFT, autocorrelação, regressão linear, etc) e, em seguida, é
realizada a filtragem, identificação e alinhamento dos registros das regiões
identificadas.
}

\palavrasChave{}

\textAbstract{}

\keywords{}

\begin{document}

\rowcolors{2}{gray!25}{white}

\capa 
% \folhaderosto[semficha]
% \includepdf[pages={1}]{img/Ficha_Catalografica.pdf} \folhaaprovacao
% \paginadedicatoria \paginaagradecimento \paginaepigrafe
\paginaresumo
% \paginaabstract

\listadefiguras 
\listadetabelas
% \listadeabreviaturas 
% \listadesimbolos
\sumario

\chapter{Introdução}
A extração de dados estruturados da web vem sendo estudada já há algum tempo em
função de sua importância, que se deve à grande quantidade de informação
estruturada disponível para extração que, de outra forma, não poderia ser
utilizada de maneira relacional, apenas textual (ou não estruturada). Os
objetivos que motivam a extração dessas informações são inúmeros, assim como os
desafios envolvidos na realização desta tarefa. De acordo com
\cite{structured2011}, a extração de dados estruturados é uma tarefa árdua, pois
as informações são provenientes de diversas fontes e são publicadas de maneira
desorganizada.

\section{Contexto}

Apesar de bastante estudada, a extração de dados estruturados ainda é um
problema em aberto. Diversas formulações foram sugeridas para o problema e foram
propostas várias abordagens para cada formulação.

Extrair dados estruturados a partir de fontes não estruturadas (ou
semiestruturadas) não é uma tarefa trivial. A estrutura dos dados tem que ser
corretamente identificada, de maneira automática e eficiente, a partir de um
documento que foi criado para ser utilizado por pessoas e não por máquinas,
\textit{i.e.}, as informações são disponibilizadas de maneira a facilitar a
leitura e não para serem processada por sistemas computacionais.

Além da estrutura sintática dos dados, para que as informações possam ser
utilizadas, é necessário identificar a qual entidade os dados se referem e quais
atributos da entidade estão representados nas instâncias identificadas.  Para
realizar esta tarefa, a utilização de taxonomias, ontologias, bases de
conhecimento e afins, é imprescindível.

Umas vez que as informações foram estruturadas, a entidade e os atributos
identificados e os dados integrados, é possível utilizar essas bases de dados de
maneira relacional para, por exemplo, contextualizar consultas por palavras
chaves utilizando as relações, construir bases de dados para determinados
domínios (e.g., comparação de preços de produtos), realizar meta-pesquisa, entre
outras aplicações.

As propostas existentes abordam várias formas do problema de extração
estruturada. De acordo com o levantamento realizado, foram identificadas três
formulações distintas que abrangem os trabalhos mais relevantes nesta área:
extração de tabelas/listas, extração de registros, dedução de \textit{template}.

Em \cite{listExtract2009,topklists2013,tegra2015} o problema é formulado como
extração de tabelas e/ou listas. São abordagens com escopo mais limitado, que
tem por objetivo extrair apenas tabelas formatadas com a \textit{tag}
\texttt{<table>}, listas de itens formatadas com \textit{tag}
\texttt{<ul/ol>} ou formatadas por \textit{templates} específicos.

Em \cite{MDR03,NET05,TPC09} o problema é formulado como extração de registros a
partir de uma única página. Estas técnicas procuram por padrões no HTML e/ou
árvore DOM do documento, onde ao menos dois registros são necessários para que
um padrão possa ser identificado. Algumas propostas empregam, também,
informações visuais obtidas através da ``renderização'' da página
HTML\cite{vips03,depta05,viper05,vide10} De acordo com \cite{structured2011},
essa formulação do problema pode ser considerada como uma generalização do
problema de extração de tabelas.

Em \cite{RRunner01,exalg2003,fivatech2010} o problema é formulado como dedução
de \textit{template}, onde um ou mais registros são extraídos a partir da
identificação do \textit{template} de um \textit{site} específico. Geralmente
estas técnicas necessitam de uma amostra (algumas páginas), de cada
\textit{site}, que é utilizada para treinamento do algoritmo de dedução do
\textit{template}.

\section{Identificação do Problema}
O problema que esta pesquisa busca solucionar é o de extração de dados
estruturados a partir de fontes semiestruturadas de forma completamente
automática e computacionalmente eficiente, para que seja possível utilizar a
abordagem em larga escala.

Alguns requisitos fundamentais para abordar este problema, em escala
\textit{web}, são:
nível de automação, generalidade e precisão. Em função da quantidade maciça de
informação estruturada disponível, as propostas de abordagem devem evitar
qualquer intervenção manual no processo de extração, pois isto inviabilizaria
sua aplicação em larga escala. Em relação à generalidade da abordagem, as
informações disponíveis na \textit{web} possuem formatos heterogêneos, mesmo
quando se considera apenas um domínio de conhecimento, por esta razão, as
propostas de abordagem, para este problema, devem ser gerais o suficiente para
que funcionem a contento nos mais diversos \textit{templates} e domínios. Devido
à enorme quantidade de dados, a precisão do processo de extração deve ser
priorizada, em detrimento do \textit{recall} se necessário, pois uma baixa
precisão resultaria em uma enorme quantidade de ``ruído'' sendo extraído e
integrado à base relacional comprometendo, assim, sua utilização.

Apesar da quantidade e da variedade de abordagens existentes, em levantamentos
recentes \cite{survey2013, survey2014} foi observado que nenhuma das propostas
existentes resolve completamente o problema. O problema de extração estruturada
é bastante amplo, de difícil solução e permanece em aberto.

\section{Objetivos}
O objetivo geral desta pesquisa é desenvolver um método automático (não
supervisionado) de extração de dados estruturados a partir de informação
semiestruturada contida em páginas \textit{web}.

Os objetivos específicos são:
\begin{itemize}
\item desenvolver um método de segmentação de um documento HTML que identifique
as regiões da página com conteúdo semiestruturado;
\item desenvolver um método que identifique quais das regiões semiestruturadas
possuem conteúdo e quais possuem ruído (menus, template, anúncios, etc.);
\item desenvolver um método de estruturação da informação semiestruturada
identificada (\textit{e.g.} alinhamento de registros);
\item desenvolver um \textit{framework} de extração estruturada;
\item criar \textit{datasets} com ampla cobertura para avaliação de métodos de
extração;
\item propor novas métricas para avaliação dos resultados.
\end{itemize}

\section{Contribuições}
As contribuições desta pesquisa são:
\begin{itemize}
  \item desenvolvimento de um novo método de extração estruturada;
  \item criação de \textit{datasets} com ampla cobertura para avaliação de
  métodos de extração estruturada;
  \item proposição de novas métricas;
  \item publicação dos resultados da pesquisa em eventos e periódicos;
  \item desenvolvimento de um \textit{framework} de extração estruturada. 
\end{itemize}

\section{Metodologia}

A condução e o desenvolvimento desta pesquisa segue a metodologia proposta em
\cite{peffers2007design}, composta de seis etapas:

\begin{enumerate}
  \item identificação do problema: através de levantamento bibliográfico,
  foi identificado o estado da arte e quais problemas ainda estão em aberto
  nesta área de pesquisa;
  \item definição dos objetivos: a partir das questões identificadas no
  levantamento bibliográfico, foi definido o objetivo geral da pesquisa;
  \item desenvolvimento: a abordagem proposta foi desenvolvida a
  partir dos objetivos específicos, buscando atingir o objetivo geral;
  \item demonstração: como validação preliminar, da abordagem desenvolvida, são
  realizadas provas de conceito e, a partir desses resultados, a abordagem é
  refinada;
  \item avaliação: realização de testes mais amplos e completos e comparação com
  outros trabalhos;
  \item comunicação: publicação dos resultados.
\end{enumerate}

\section{Abordagem proposta}
Esta pesquisa apresenta uma proposta de abordagem inédita para o problema de
estração estruturada. A técnica utiliza uma representação alternativa do
documento HTML e da árvore DOM, a sequência de \textit{tag paths}, semelhante à
utilizada em \cite{TPC09, SuffixTree12}. Esta forma de representação pode ser
vista como um ``sinal discreto'' e, assim, podem
ser empregadas técnicas tipicamente utilizadas no processamento de sinais
discretos \cite{oppenheim1989discrete} como: transformada rápida de Fourier (FFT
- \textit{fast Fourier transform}\cite{fft1965}), autocorrelação e derivada
discreta.
Não foi identificada, durante o levantamento bibliográfico, nenhuma abordagem
semelhante para o problema de extração de dados estruturados.

A técnica é composta das seguintes etapas:
\begin{enumerate}[a)]
  \item limpeza do código HTML e construção da árvore DOM;
  \item construção da sequência de \textit{tag paths}; 
  \item contorno da sequência, derivada e segmentação; 
  \item filtragem das regiões; 
  \item identificação e alinhamento dos registros.
\end{enumerate}

\section{Estrutura do trabalho}


% Nenhum abordagem esgota o limite da extração sintática, todas utilizam %algum
% expediente que compromete a técnica: alta complexidade %computacional, regras
% heurísticas efêmeras, regras heurísticas %dependentes do HTML,
% \textit{supervised machine learning (user labeled %examples)}, bases de
% conhecimento populadas manualmente para domínios %específicos, aplicação com
% escopo limitado (list e table).

% *** abordagens de \textit{schema matching}: principais abordagens são %da
% indústria, são parecidas, e utilizam tabelas extraídas pra montar %uma
% taxonomia.

% *** extração estruturada utilizando NLP: deep dive (??? ver se entra)

Este trabalho está organizado da seguinte forma: o Capítulo \ref{ch:teoria}
apresenta a fundamentação teórica com os conceitos mais importantes para o
entendimento dos trabalhos relacionados e da abordagem proposta; o Capítulo
\ref{ch:trab} apresenta os resumos dos trabalhos relacionados ao tema desta
pesquisa e; o Capítulo \ref{ch:prop} apresenta os problemas em aberto, os
desafios e as perspectivas nesta área de pesquisa e detalha a abordagem
proposta.

\chapter{Fundamentação Teórica}\label{ch:teoria}
Neste capítulo são apresentados alguns conceitos fundamentais para o
entendimento dos trabalhos relacionados e da abordagem proposta no Capítulo
\ref{ch:prop}.

\begin{definition}[\textbf{Árvore DOM}].
\label{def:domtree}
A árvore DOM (\textit{Document Object Model}) é uma estrutura hierárquica,
derivada do código HTML, que representa uma página web.
Cada nó desta árvore representa um ``objeto'' do documento HTML. A especificação
de referência é mantida pelo consórcio $W3C$ \cite{w3c} que é o órgão
encarregado de manter os padrões e especificações da \textit{web}.
\end{definition}
\textbf{Exemplo.} Na Figura \ref{fig:ex1} um pequeno trecho de código HTML é
utilizado para ilustrar como uma árvore DOM é construída a partir do código.

\begin{definition}[\textbf{\textit{tag path}}].
\label{def:tp} 
Um \textit{tag path} (TP) é uma $string$ descrevendo
o caminho absoluto, a partir da raiz da árvore DOM, até um dado nó da árvore. Seja
``$n_i$'' um determinado nó da árvore DOM, onde ``i'' representa a
posição/ordem, em profundidade, do nó $``n_i"$ na árvore, então é dito que
$TP_i$ é a $string$ descrevendo o caminho, a partir da raiz da árvore DOM, até o
nó $``n_i"$.
\end{definition}
\textbf{Exemplo.} Na Figura \ref{fig:ex1}, o caminho absoluto $TP_4$ do nó
$body$ até a célula da tabela $td_4$ é $TP_4=``body/table/tr/td"$.

\begin{definition}[\textbf{Sequência de \textit{tag paths}}].
\label{def:tps} 
Uma sequência de \textit{tag paths} (TPS) de
uma árvore DOM com ``n'' nós é definida como sendo a sequência ordenada
$TPS[1..n] = (TP_1,TP_2,TP_3, ...,TP_{n-1},TP_n)$ onde dois \textit{tag paths}
$TP_i$ e $TP_j$, com $i\neq{}j$, são considerados iguais apenas se seus caminhos
e suas definições de estilo são iguais, caso contrário eles são considerados
diferentes. Os caminhos da árvore DOM são percorridos em profundidade e para
cada caminho é atribuído um código, de maneira incremental.
\end{definition}
\textbf{Exemplo.} Na Figura \ref{fig:ex1} é exibida a $TPS$ de um trecho
de código HTML, onde cada $TP$ recebe um código, incremental, produzindo $TPS =
(1,2,3,4,4,3,4,4)$.
Na sequência de \textit{tag paths} os \textit{tag paths} não são representados
pelas suas \textit{strings}, mas sim por códigos (ou símbolos), com objetivo de
facilitar sua manipulação, onde \textit{tag paths} iguais recebem o mesmo
código. Esta definição é similar à utilizada em \citeonline{SuffixTree12}, onde
cada \textit{tag path} é representado na sequência por um símbolo, exceto que
neste trabalho foram incorporadas as definições de estilo das \textit{tags}.
Como as definições CSS são utilizadas para diferenciar e caracterizar as
diversas regiões de uma página, é muito importante levar em consideração essas
informações na momento de segmentar a página.

\begin{definition}[\textbf{Alfabeto da TPS}].
\label{def:alpha} Seja $\Sigma$ o conjunto de todos os símbolos de uma
sequência $TPS$ de tamanho ``n'', $\Sigma$ é dito o alfabeto de $TPS$
que é definido como $\Sigma = \{\forall{\alpha} | \exists{TPS[i]=\alpha} \wedge
1 \leq i \leq n\}$, onde $\alpha$ é um símbolo do alfabeto.
\end{definition}
\textbf{Exemplo.} Na Figura \ref{fig:ex1}, a $TPS$ é formada apenas pelos
símbolos ``1'',``2'', ``3'' e ``4'', então seu alfabeto é $\Sigma=\{1,2,3,4\}$.
Informalmente falando, o alfabeto contém todos os símbolos distintos de uma
determinada sequência. 

\begin{definition}[\textbf{Fast Fourier Transform}].
\label{def:fft}
A FFT (fast Fourier transform) é um algoritmo eficiente (com complexidade de
tempo $O(nlogn)$) para o cálculo da DFT (discrete Fourier
transform) que realiza a decomposição de uma sequência discreta de valores em
seus componentes de frequência (\textit{i.e.} espectro) e é definida pela
Equação \ref{eq:dft} onde $X_k$ são os componente de frequência, $x_n$ é a
sequência, de tamanho $N$, a ser decomposta e ``$i$'' é o número imaginário. A
operação inversa (\textit{i.e.} transformar $X_k$ de volta para $x_n$) é
definida pela Equação \ref{eq:idft}.
\end{definition}
\begin{equation}\label{eq:dft}
X_k = \sum_{n \leftarrow 0}^{N-1}{x[n] \cdot e^\frac{-2 \pi i k n}{N}}
\end{equation}

\begin{equation}\label{eq:idft}
x_n = \frac{1}{N} \cdot \sum_{n \leftarrow 0}^{N-1}{X[k] \cdot e^\frac{2 \pi i
k n}{N}}
\end{equation}
\textbf{Motivação.} A FFT é utilizada nas Definições
\ref{def:xcorr} e \ref{def:power} para calcular a autocorrelação e o espectro de
potência, respectivamente, de forma eficiente.

\begin{definition}[\textbf{Autocorrelação}].
\label{def:xcorr}
A autocorrelação é a correlação de uma sequência discreta com ela própria
deslocada nos intervalos de $[1..n-1]$, onde $n$ é o tamanho da sequência. A
autocorrelação pode ser utilizada, entre outras aplicações, para verificar se
uma sequência é cíclica, com período ``$p$'', \textit{i.e.}, se ela se parece com
ela mesma quando deslocada de ``$p$''. A autocorrelação de uma sequência pode
ser calculada de forma eficiente utilizando-se a FFT, como mostra a Equação
\ref{eq:xcorr}, onde $X_k$ é a FFT de $x_n$, $X_k^*$ é o conjugado complexo de
$X_k$ e a autocorrelação $C_{x_n}$ de $x_n$ é igual a transformada inversa
do produto de $X_k$ com seu conjugado complexo. O produto $(X_k \cdot X_k^*)$ é
calculado em tempo $O(n)$, $X_k$ é calculado em $O(nlogn)$ assim como a
transformação inversa, portanto a complexidade de tempo para cálculo da
autocorrelação é $O(nlogn)$.
\end{definition}
\begin{equation}\label{eq:xcorr}
C_{x_n} = InverseFFT(X_k \cdot X_k^*)
\end{equation}
\textbf{Exemplo.} A Figura \ref{fig:fft}b apresenta a autocorrelação da
sequência da Figura \ref{fig:fft}a. Como a sequência é cíclica, sua
autocorrelação apresenta picos intervalados pelo período.

\begin{definition}[\textbf{Espectro de potência}].
\label{def:power}
O espectro de potência de uma sequência é calculado a partir da sua FFT,
conforme a Equação \ref{eq:power}, e indica o quanto cada componente de
frequência contribui na formação da sequência. O módulo de $X_k$, que é uma
sequência de valores complexos, é calculado conforme a Equação
\ref{eq:complex}, para cada posição de $X_k$.
\end{definition}
\begin{equation}\label{eq:power}
P_{x_n} = |X_k|
\end{equation}
\begin{equation}\label{eq:complex}
|a + i \cdot b|=\sqrt{a^2 + b^2}
\end{equation}
\textbf{Exemplo.} A Figura \ref{fig:fft}c apresenta o espectro de potência da
sequência da Figura \ref{fig:fft}a. O componente de frequência com maior
potência coincide, neste caso, com o período principal da sequência original.

\begin{definition}[\textbf{Derivada discreta}].
\label{def:diff}
A derivada de uma sequência discreta, também chamada de ``diferença'', é
apenas o cálculo da diferença entre valores vizinhos da sequência, conforme
definido na Equação \ref{eq:diff}, onde ``$d$'' é a derivada discreta
da sequência ``$x$''.
\end{definition}
\begin{equation}\label{eq:diff}
d[n] = x[n] - x[n+1]
\end{equation}
\\

As Definições \ref{def:fft}, \ref{def:xcorr}, \ref{def:power} e \ref{def:diff}
foram apresentadas neste texto de maneira breve e superficial e sua exposição foi
direcionada para a aplicação nesta pesquisa, pois o detalhamento completo dessas
definições e da teoria de processamento de sinais é muito extenso e foge do
escopo desta pesquisa.
A área de processamento de sinais tem ampla e variada aplicação científica.
Informações completas e detalhadas a respeito destas definições e da teoria de
processamento de sinais discretos podem ser encontradas em
\cite{oppenheim1989discrete,oppenheim1997sh}.

\chapter{Trabalhos Relacionados}\label{ch:trab}

Durante o levantamento foram identificadas duas tendências principais de
pesquisa: uma delas, mais adotada pela indústria (\textit{e.g.},
\cite{webtables2008,tablesMS2012,listExtract2009,tegra2015,2015webtables,relationalWeb2008,topklists2013}),
com foco no uso de dados maciços e a outra, mais acadêmica (\textit{e.g.},
\cite{RRunner01, vips03, viper05, MDR03, NET05, TPC09, vide10}), com foco maior
na utilização de regras e aprendizado de máquina. A extração de dados
estruturados, na visão tanto da academia como da indústria, é tida como uma
etapa intermediária para se alcançar o objetivo final: enriquecer as aplicações
e os métodos de pesquisa existentes (\textit{e.g.}, \cite{2016discovering})
como, por exemplo, responder perguntas em linguagem natural, \textit{i.e.}, a
partir de uma pesquisa por palavra chave, ao invés de retornar ao usuário uma
lista de resultados, retornar a resposta para a pergunta que foi feita.

A linha de pesquisa adotada pela indústria trata de problemas bem específicos,
com escopo bem delimitado e aplicabilidade imediata (\textit{e.g.}, extrair
apenas tabelas formatadas com \textit{tag} \texttt{<table>}), enquanto a
academia se preocupa com problemas mais gerais, muitas vezes produzindo
resultados intermediários, que podem servir para o desenvolvimento de trabalhos
futuros. Em contrapartida, as pesquisas desenvolvidas pela acadêmia podem não
produzir resultados concretos imediatamente, diferente do que ocorre,
geralmente, na indústria.

Uma questão problemática identificada durante o levantamento foi a comparação
entre as diversas propostas existentes devido, principalmente, aos seguintes
fatores:
\begin{itemize}
    \item a maioria das propostas não teve sua implementação disponibilizada publicamente;
    \item os \textit{datasets} utilizados são diferentes, não existe padronização e, muitas vezes, também não são disponibilizados publicamente como no caso dos trabalhos desenvolvidos pela indústria que dependem de uma quantidade maciça de dados;
    \item as métricas normalmente utilizadas (\textit{recall}, precisão e \textit{f-measure}) não são adequadas para medir a eficácia dos trabalhos de extração estruturada e alinhamento de registros, conforme exposto em \cite{ariex2016,survey2013}.
\end{itemize}

De acordo com levantamentos recentes (\cite{survey2013,survey2014}) as propostas
existentes possuem as mais diversas características, porém nenhuma é
universalmente aplicável; os critérios de comparação entre os trabalhos não são
claros; poucas implementações são disponibilizadas publicamente e;
consequentemente, o problema de extração permanece em aberto.

A seguir são apresentados os resumos dos trabalhos mais relevantes identificados
nesta área de pesquisa organizados da seguinte maneira: a Seção \ref{sec:tab}
apresenta os trabalhos de extração de tabelas e listas; a Seção \ref{sec:page}
contém os trabalhos de extração de registros a partir de uma única página e; na
Seção \ref{sec:site} estão os trabalhos de extração a partir de múltiplas
páginas.

\section{Extração de tabelas e listas}\label{sec:tab}
As pesquisas apresentadas a seguir realizam a extração dos dados estruturados a
partir de três fontes distintas: (1) tabelas formatadas com a \textit{tag}
\texttt{<table>}; (2) listas formatadas com as \textit{tags}
\texttt{<ol><ul>},etc. e; (3) tabelas apresentadas na forma de páginas de
tópicos.

No primeiro caso o principal desafio é a identificação de quais tabelas
representam, de fato, relações. Na extração de listas o problema se
torna um pouco mais complexo, pois é necessário identificar os campos dos
registos e alinhá-los. E na extração de páginas de tópicos é explorado o
\textit{template} desse tipo de documento, que apresenta certa regularidade.

Em \cite{relationalWeb2008, webtables2008}, pesquisa pioneira, desenvolvida no
Google, são extraídas apenas tabelas declaradas com a \textit{tag}
\texttt{<table>}. As tabelas sem conteúdo relacional são
filtradas utilizando um classificador treinado com exemplos anotados por
usuários, resultando num total de $154$ milhões (1.1\% do total) de tabelas
relacionais extraídas do índice do motor de busca do Google. Essas tabelas são
utilizadas na construção de um banco de dados com estatísticas de atributos
(ACSDb -  \textit{attribute correlation statistics database}, disponível em
\cite{acsdb}) que é utilizado para \textit{ranking} das tabelas, \textit{schema
auto-complete} e encontrar sinônimos de atributos. Além desta contribuição os
autores também apresentam uma abordagem para indexar e pesquisar resultados
relacionais a partir de palavras-chave utilizando um índice distribuído próprio
para conteúdo relacional. Em \cite{2015webtables}, são apresentadas várias
aplicações, melhoramentos e desdobramentos atuais desenvolvidos em cima da
pesquisa incial.

Em \cite{listExtract2009}, pesquisa desenvolvida em conjunto com o Google, o
objetivo da abordagem é extrair tabelas relacionais a partir de listas contidas
em documentos HTML, de forma não supervisionada, com objetivo de aplicação em
larga escala. A entrada do algoritmo é uma lista, onde cada linha é uma
\textit{string}. A abordagem consiste de três fases: (1) \textbf{separação} de
cada linha da lista, de forma independente das demais linhas, em
\textit{tokens}; (2) \textbf{alinhamento} dos \textit{tokens}, determinação do
número de campos da tabela a ser extraída e correção dos registros com
quantidade de campos superior à determinada; (3) \textbf{refinamento} da
extração a partir da detecção de campos inconsistentes e realinhamento das
inconsistências encontradas.
A separação das linhas da lista em \textit{tokens} é realizada, principalmente,
utilizando um modelo de linguagem natural, obtido a partir do processamento de
milhões de documentos, e; uma base de aproximadamente 154 milhões de tabelas
HTML extraídas da web. O modelo de linguagem natural e a base de tabelas são
utilizados para calcular um \textit{score} para um conjunto de \textit{tokens} e
decidir, de maneira \textit{greedy}, se este conjunto representa, ou não, um
campo na tabela a ser extraída. O alinhamento destes campos é realizado com um
algoritmo de alinhamento de múltiplas sequências (MSA) de maneira iterativa, um
registro por vez.
O escopo da abordagem é apenas a extração da tabela, sem realizar a
identificação da entidade e o \textit{schema matching} (\textit{i.e.} atribuir
nomes às colunas da tabela).
O resultado obtido pelo algoritmo é de 0.63 \textit{f-measure}.

Em \cite{tablesMS2012}, pesquisa desenvolvida no centro de pesquisas da
Microsoft na Asia, o objetivo é ``entender'' tabelas extraídas da web.
A abordagem proposta utiliza uma base de dados de conceitos, entidades e
atributos (\textit{Probase} \cite{probase2012}, disponível em \cite{probase}),
construída a partir de milhões de documentos indexados da \textit{web}. A
\textit{Probase} é uma taxonomia utilizada para identificar a qual entidade uma
tabela se refere e quais atributos ela contém. O foco da pesquisa é na
identificação da entidade e seus atributos e não na estruturação dos dados, pois
as tabelas extraídas já são formatadas utilizando a \textit{tag}
\texttt{<table>}. As tabelas identificadas com sucesso são inseridas na
\textit{Probase}, realimentando a taxonomia com objetivo de melhorar seu
desempenho. O algoritmo assume que apenas uma entidade é representada por
tabela, o que não se verifica em alguns casos. O resultado obtido pelo algoritmo
é de 87.3\% de tabelas identificadas corretamente.

Em \cite{tegra2015}, pesquisa desenvolvida no centro de pesquisa da Microsoft, o
objetivo é o mesmo da abordagem proposta em \cite{listExtract2009} (extrair
tabelas relacionais a partir de listas, sem supervisão) que é utilizada como
\textit{baseline} para comparação. A contribuição deste trabalho com relação ao
original é que, no lugar de utilizar um modelo de linguagem natural para dividir
as linhas em campos, é adotada uma estratégia de otimização global para
encontrar a melhor divisão das linhas, considerando-as em conjunto e não
individualmente. O lado negativo desta abordagem é que a complexidade
computacional sobe bastante, porém apresentando resultados qualitativamente
superiores. O \textit{score} a ser otimizado pelo algoritmo é calculado a partir
de consultas a uma base com aproximadamente 100 milhões de tabelas HTML
extraídas da web que são utilizadas para medir a ``coerência'' dos possíveis
campos identificados em cada linha da lista.
Com objetivo de reduzir a complexidade computacional do problema de otimização,
os autores utilizam uma aproximação do problema (abrindo mão do ``ótimo
global''), com garantias de margem de erro, em conjunto com poda A*. Ainda assim
o tempo de execução se mostrou superior ao \textit{baseline}.
Da mesma forma que a abordagem proposta em \cite{listExtract2009}, o escopo
desta abordagem é apenas a extração da tabela, sem realizar a identificação da
entidade e o \textit{schema matching} (\textit{i.e.}, atribuir nomes às colunas
da tabela).
Os autores também apresentam uma alternativa supervisionada de utilização do
mesmo algoritmo.
O resultado obtido pelo algoritmo, na versão não supervisionada, é de 0.97
\textit{f-measure}.

Em \cite{topklists2013}, pesquisa desenvolvida no centro de pesquisas da
Microsoft, o objetivo da abordagem é a extração de informação estruturada de
páginas com um \textit{layout} pré-determinado, especificamente documentos que
apresentem conteúdo do tipo ``\textit{top-k list}'' (\textit{e.g.},
\textit{rankings} do tipo melhores 10 filmes de 2015, maiores 20 empresas em
faturamento, etc.). A partir do título da página, utilizando um classificador
CRF treinado com milhares de exemplos, é determinado se a página possui o
\textit{layout} esperado ou não e, caso possua, o tamanho da lista é determinado
e o conteúdo do documento é contextualizado utilizando a taxonomia
\textit{Probase}\cite{probase2012} (mesma utilizada em \cite{tablesMS2012}). Com
a informação do tamanho da lista (que é obtida a partir do título da página) os
nós do documento HTML são ``clusterizados'' de acordo com a frequência dos seus
\textit{tag paths} e aqueles com frequência igual ao tamanho da lista são
extraídos e o \textit{schema} identificado com o auxílio, novamente, da
taxonomia \textit{Probase}. Caso mais de uma lista seja extraída elas são
ordenadas de acordo com sua correlação com o contexto da página e apenas a lista
melhor classificada é extraída e as demais são descartadas (a abordagem assume
que essas páginas tem apenas uma lista com conteúdo). Os resultados obtidos
foram precisão de 92.0\% e \textit{recall} de 72.3\% em 1.7 milhões de listas.
Por ser o primeiro trabalho deste tipo, na época da publicação, não foi
apresentada nenhuma comparação com outras abordagens.

\section{Extração a partir de uma única página}\label{sec:page}
A extração de registros a partir de uma única página é realizada, geralmente,
através da identificação de padrões na árvore DOM. Esta forma de extração é
considerada em \cite{structured2011} como a generalização da extração de tabelas
sendo, portanto, de maior complexidade, pois não existe uma declaração explícita
da tabulação dos dados.

Uma desvantagem desta abordagem é que são necessários ao menos dois registros em
uma mesma página para que o padrão seja passível de identificação.

Em \cite{MDR03} é proposta uma técnica de extração de registros chamada MDR
(\textit{mining data records}). A abordagem consiste em localizar as regiões do
documento que contém subestruturas semelhantes. A árvore DOM do documento HTML é
percorrida no sentido da raiz para as folhas (\textit{top-down}) e, a cada nó,
as subárvores vizinhas são comparadas, utilizando uma medida de distância para
árvores (\textit{tree edit distance}).
Caso as estruturas sejam suficientemente semelhantes (de acordo com um limiar de
semelhança previamente estipulado) a região é identificada e os registros
contidos nela são alinhados com um algoritmo de alinhamento de árvores
(\textit{partial tree alignment}). O algoritmo foi desenvolvido a partir de
observações gerais a respeito de páginas que contêm registros: (1) os registros
ocupam a mesma região do documento e; (2) os registros tem estrutura semelhante
entre si. Não são utilizadas bases de conhecimento previamente definidas nem
conjuntos de regras heurísticas.
Apesar da técnica não ser recente ainda tem grande relevância, pois é uma das
poucas pesquisas com implementação publicamente disponível e ainda são
encontrados trabalhos recentes que a utilizam como \textit{baseline}
(\textit{e.g.}, \cite{autorm2015}). Na publicação original os resultados obtidos
foram 99,8\% de \textit{recall} e 100\% de precisão, porém em diversas pesquisas
subsequentes que utilizaram esta técnica como \textit{baseline} em outros
\textit{datasets} os resultados obtidos foram inferiores e, em alguns casos,
ficaram muito aquém dos valores originalmente reportados.

Em \cite{depta05} é proposta uma técnica de extração de registros chamada DEPTA
(\textit{Data Extraction based Partial Tree Alignment}). A técnica é, na
verdade, um melhoramento do algoritmo MDR\cite{MDR03}. Nesta proposta é
utilizado um navegador \textit{web} para corrigir o HTML mal formado e obter
informações visuais e de \textit{layout}, a partir da renderização da página,
que são utilizadas para melhorar as etapa de extração dos registros e
alinhamento dos campos. No algoritmo de alinhamento dos campos foram feitas
pequenas modificação para evitar que campos opcionais (campos que não aparecem
em todos os registros) comprometam o alinhamento do restante do registro. Os
resultados obtidos foram 98.18\% \textit{recall} 99.68\% precisão. No mesmo
\textit{dataset} MDR obteve \textit{recall} 86.64\% e 97.10\% precisão.

Em \cite{NET05} é proposta uma técnica de extração de registros chamada NET
(\textit{Nested data extraction using tree matching}). O algoritmo recebe, como
entrada, apenas um página, não necessita de treinamento e não utiliza regras
heurísticas. A extração é realizada percorrendo-se a árvore DOM do documento
HTML das folhas para a raiz (\textit{post-order traversal}).
Conforme a árvore é percorrida, de baixo para cima, os nós e suas subárvores
vizinhas são comparadas, de maneira semelhante como em \cite{MDR03}, com a
diferença que neste caso as subárvore são colpsadas e transformadas em
expressões regulares conforme as comparações e alinhamentos vão ocorrendo. Ao
final do percorrimento da árvore, os nós que foram alinhados são extraídos para
tabelas. A principal contribuição desta pesquisa, segundo os autores do
trabalho, é que este algoritmo é capaz de extrair registros aninhados e listas,
algo que as técnicas disponíveis na época não realizavam de maneira
satisfatória. Na publicação o algoritmo NET é comparado com DEPTA\cite{depta05},
técnica desenvolvida pelos mesmos autores. Os resultados obtidos são 98.99\% de
\textit{recall} e 98.92\% de precisão para registros não aninhados e 99.63\% de
\textit{recall} e 100\% de precisão para registros aninhados.

Em \cite{TPC09} é proposta uma técnica para extração de registros a partir de
uma única página \textit{web} utilizando um algoritmo de \textit{clustering}
espectral. O documento HTML é convertido para uma representação alternativa:
uma sequência de \textit{tag paths}. A sequência de \textit{tag paths} é
construída percorrendo-se a árvore DOM em profundidade, da raiz para as folhas,
e para cada novo caminho encontrado um código é atribuído e esses códigos são
concatenados para formar a sequência. Após a construção da sequência de
\textit{tag paths}, os códigos são agrupados (\textit{clustering}) de acordo com
suas frequências.
Cada \textit{cluster} resultante representa uma região de dados da página e,
portanto, os \textit{tag paths} de cada \textit{cluster} precisam ser combinados
em registros para possibilitar a extração. Para identificar e separar os
registros contidos em cada \textit{cluster} são utilizadas relações de
ancestralidade e descendência entre os \textit{tag paths} e algumas regras
heurísticas para identificar a divisão entre os registros. A pesquisa não se
preocupou com a questão de alinhamento dos campos, apenas indicou que a
abordagem proposta pode ser combinada com outras técnicas de alinhamento de
campos e registros. Os resultados obtidos foram \textit{recall} 98.1\% e
precisão 98.9\% para registros aninhados e \textit{recall} 99.9\% e precisão
99.4\% para registros não aninhados.

Em \cite{SuffixTree12} é proposta uma técnica para extração de registros chamada
STEM (Suffix Tree-based data record Extracting Method). O algoritmo proposto
processa páginas HTML individuais. A árvore DOM é convertida para a mesma
representação utilizada em \cite{TPC09} (sequência de \textit{tag paths}). A
partir da sequência é construída sua árvore de sufixos \cite{ukkonen1995} que
trata-se de uma estrutura de dados que permite realizar uma série de
processamentos em cima de uma \textit{strings} de forma eficiente, entre eles
encontrar o número de repetições de uma \textit{substring} e localizar as
posições em que ocorrem na sequência. Desta forma o algoritmo encontra
\textit{substrings} da sequência de \textit{tag paths} que ocorrem várias vezes
e considera que essas \textit{substrings} são registros. São definidas regras
heurísticas para evitar a extração de \textit{substrings} que não representam
registros, e.g., \textit{substrings} de tamanho unitário, ou muito longas, são
excluídas e; a quantidade de regiões que uma página pode conter é arbitrada.
Embora a técnica proposta permita, aparentemente, identificar diretamente os
registros dentro das regiões, o trabalho não aborda esta questão nem o
alinhamento de registros. A resistência a ruído também deveria ser melhor
investigada, pois a árvore de sufixo, ao menos como apresentada nesta
publicação, não tem mecanismos para \textit{matching} aproximado.
Os resultados obtidos foram 0.96 de \textit{recall} e 0.9698 de precisão. Os
resultados são comparados com \cite{TPC09,MDR03}, apresentando maior eficácia
que ambos. Foi utilizado o mesmo \textit{dataset} de \cite{TPC09} para
realização dos testes.

Em \cite{vips03}, pesquisa desenvolvida no centro de pesquisas da Microsoft, é
proposta uma das primeiras abordagens de segmentação de páginas e identificação
de conteúdo utilizando informações visuais obtidas a partir da ``renderização''
do documento HTML. O algoritmo proposto se chama VIPS (\textit{VIsion-based Page
Segmentation}). A abordagem se limita a apenas a segmentar a página em blocos
visuais semanticamente coerentes, não há preocupação quanto a identificação e
alinhamento de registros. O algoritmo utiliza as informações de formatação (cor,
texto, tamanho e \textit{tag}) dos nós da árvore DOM para calcular o
\textit{score} de coerência do nó e para decidir se o mesmo é um bloco visual ou
uma combinação de blocos que deve, portanto, ser subdividido. Numa etapa
subsequente, os blocos encontrados são analisados com relação aos separadores
visuais (linhas verticais e horizontais que se cruzam no \textit{canvas}), para
determinar se algum ajuste deve ser realizado nos blocos visuais e/ou
separadores. Finalmente, a partir dos separadores, é construída uma
representação visual e hierárquica do documento, contendo os blocos visuais
detectados. Várias regras heurísticas são utilizadas para calcular o
\textit{score} de coerência e realizar a análise dos separadores visuais e
muitas delas dependem diretamente de \textit{tags} HTML específicas (e.g.,
\texttt{<table>, <tr>, <td>, <hr>, <p>}) e da atual ``semântica'' com a qual a
\textit{tag} é empregada (e.g., a \textit{tag} \texttt{<table>} é utilizada
tanto para estruturação do documento como para tabulação de dados). A principal
desvantagem do uso desse tipo de regras é que elas são dependentes da (1)
sintaxe HTML específica, algo que muda com o tempo (surgem novas especificações
da linguagem HTML) e; (2) das práticas de desenvolvimento (\textit{e.g.}, hoje
em dia estruturar a página com a \textit{tag} \texttt{<table>} caiu em desuso),
ambas dependências efêmeras, o que compromete a validade da proposta no longo
prazo.
Outra questão controversa é o custo computacional envolvido na ``renderização''
da página quando se considera a aplicação em larga escala. Quanto aos resultados
apresentados na publicação, o processamento de 140 páginas, pelo algoritmo VIPS,
foi submetido a julgamento humano com a seguinte conclusão: 86 páginas
segmentadas perfeitamente, 50 satisfatoriamente e 4 falhas.

Em \cite{viper05} é proposta uma técnica chamada ViPER para extração de
páginas com múltiplos registros. O algoritmo é uma extensão do MDR \cite{MDR03}.
Vários melhoramentos foram introduzidos na identificação das regiões que contêm
estruturas repetitivas e na divisão das regiões em registros como o tratamento
de \textit{tandem repeats} e informação opcional, resultando em um aumento
considerável da eficácia. Outra melhoria, em relação ao MDR, é a identificação
de quais regiões contêm, de fato, conteúdo de interesse. As regiões
identificadas são pontuadas de acordo com sua posição em relação ao centro da
página ``renderizada'' e em relação à área total ocupada pela região. Para
calcular a pontuação é utilizada informação visual da página ``renderizada''
(coordenadas \texttt{(x,y)}, altura e largura). Após a identificação dos
registros estes são alinhados utilizando um algoritmo de alinhamento global de
múltiplas \textit{strings} que, segundo os autores, apresentou resultados
superiores ao método \textit{center star} \cite{centerstar1993}. O algoritmo foi
comparado ao MDR e ao ViNTs \cite{vints2005}. O algoritmo ViPER apresentou
eficácia consideravelmente superior ao MDR (52.8\% vs 98\% de \textit{recall} e
87.7\% vs 98.6\% de precisão). Com relação ao algoritmo ViNTs, utilizando os
\textit{datasets} de \cite{vints2005}, os resultados foram praticamente iguais e
utilizando utilizando o \textit{dataset} dos autores de ViPER, o algoritmo
obteve resultados ligeiramente superiores ao ViNTs (89.2\% vs 97.6\% de
\textit{recall} e 93.5\% vs 98.5\% de precisão).

Em \cite{vide10} é proposta uma técnica chamada ViDE para extração de registros
da \textit{deep web}. A técnica proposta utiliza informação visual
(\textit{i.e.}, ``renderiza'' o documento HTML, como em
\cite{depta05,vips03,viper05}) para identificar os registros a serem extraídos.
Para obter as informações visuais o algoritmo utiliza, na verdade, a abordagem
de segmentação de página proposta em \cite{vips03} (\textit{i.e.}, a saída do
algoritmo VIPS é a entrada do algoritmo ViDE). O algoritmo primeiro localiza a
região do documento onde os registros estão localizados de acordo com a posição
da região.
Segundo a pesquisa, a região principal fica localizada no centro da página e
possui um tamanho grande, em relação ao documento todo. Após a localização da
região principal do documento é realizada a divisão em registros. O processo de
identificação dos registros tem três etapas: (1) remoção de blocos que contêm
apenas ruído; (2) \textit{clustering} dos blocos remanescentes de acordo com sua
similaridade visual e; (3) reagrupamento de blocos para formar os registros de
acordo com sua similaridade de conteúdo. Para o alinhamento dos campos é
proposto um algoritmo iterativo que agrupa em colunas os campos de acordo com
suas posições e características visuais. Os autores reconhecem que a
complexidade computacional da proposta é alta demais para possibilitar sua
utilização em ``tempo real'' e, para superar este problema, sugerem a geração de
\textit{wrappers} visuais. Os testes foram realizados em dois \textit{datasets}
diferentes e comparados com \cite{MDR03,depta05}. A pesquisa propõe uma nova
métrica, chamada \textit{revision}, para medir a eficácia das propostas, além de
\textit{recall} e precisão. A nova métrica é calculada a partir da quantidade de
resultados de extração que não obtiveram 100\% de \textit{recall} e precisão e,
portanto, necessitam de revisão. Os resultados obtidos foram 97.2\% de
\textit{recall}, 96.3\% de precisão e 14.1\% de  \textit{revision} no primeiro
\textit{dataset} e 98.4\% de \textit{recall}, 95.6\% de precisão e 11.6\% de
\textit{revision} no segundo \textit{dataset}, ambos resultados
significativamente superiores aos resultados das técnicas comparadas.

Em \cite{clustVX2014} é proposta uma técnica chamada ClustVX (\textit{Clustering
Visually similar XPaths}), que ``renderiza'' a página \textit{web} utilizando o
navegador \textit{Mozilla Firefox} para obter as informações visuais de cada nó
da árvore DOM. A partir da árvore DOM são construídas as \textit{strings} dos
\textit{tag paths} de cada nó da árvore e essas \textit{strings} são
``clusterizadas'' de acordo com sua semelhança (incluindo as informações
visuais).
Para cada \textit{cluster} resultante é calculado o LCP (\textit{longest common
prefix}) dos \textit{tag paths}, este prefixo é considerado como o caminho da
raiz até a região dos registros. Para encontrar os campos dos registros é
calculado o LCS (\textit{longest common suffix}) dos \textit{tag paths} e o nós
folha no final destes sufixos são considerados os campos dos registros. A
\textit{substring} que fica entre o LCP e o LCS dos \textit{tag paths}
(\textit{i.e.,} a \textit{substring} entre o prefixo e o sufixo dos \textit{tag
paths}) é considerada como a divisão entre os registros.
O algoritmo foi comparado ao MDR \cite{MDR03}, ViNTs \cite{vints2005} e FiVaTech
\cite{fivatech2010} com resultados superiores de 100\% de precisão e 99.5\% de
\textit{recall}.


Em \cite{datapath2015} é proposta uma técnica de extração de registros
utilizando \textit{clustering} de subárvores do DOM para identificar as regiões
de registros da página. As subárvores são ``clusterizadas'' de acordo com a
similaridade do conjunto de \textit{tag paths} que cada uma contém. Os
\textit{clusters} resultantes são combinados ou eliminados de acordo com as
informações visuais (posição e área ``renderizada'') das subárvores contidas em
cada um. Os \textit{clusters} remanescentes são as regiões de registros. Os
registros são, então, convertidos para sequências de \textit{tag paths} e
alinhados. O algoritmo de alinhamento seleciona o maior registro do conjunto
como ``\textit{master}'' contra o qual os demais registros são alinhados um a
um. No alinhamento são considerados apenas \textit{tags} \texttt{<img>} e
\texttt{\#text}. Os algoritmo foi comparado ao DEPTA \cite{depta05},
apresentando resultados superiores de 94.31\% de precisão e 97.51\% de
\textit{recall} utilizando \textit{dataset} dos próprios autores contendo
páginas de 50 \textit{sites} reais.


Em \cite{autorm2015} é proposta uma técnica chamada AutoRM (Automatic data
Record Mining). O algoritmo proposto é baseado no DEPTA \cite{depta05}, que
utiliza a \textit{tree edit distance} para encontrar as regiões e os registros.
A diferença é que aqui o alinhamento das árvores foi modificado para priorizar o
alinhamento dos nós folha da árvore DOM.
Para eliminar eventual ``ruído'' contido em uma região e melhorar a
identificação dos registros, o algoritmo utiliza o conceito de ``nós
separadores'', que são os nós que não possuem nenhum conteúdo, independente da
\textit{tag} (\textit{e.g.}, \texttt{<br>}).
Para identificar os registros dentro de uma região o algoritmo utiliza a mesma
medida de semelhança entre árvores e ``clusteriza'' (com \textit{clustering}
aglomerativo \cite{cluster2009}) as subárvores da região de acordo com a
semelhança entre seus nós folha. O processo de identificação dos registros
necessita encontrar qual o nó (ou nós, pois os registros podem ser formados por
mais de uma subárvore) é a raiz de cada registro. Este processo, da forma como
foi colocado na publicação, aparenta ser realizado por busca exaustiva,
avaliando cada conjunto possível de nós raiz, selecionando o melhor candidato de
acordo com a similaridade entre os possíveis registros e a área total da página
ocupada por eles.
Da maneira como foi colocado, a identificação dos registros aparenta ter
complexidade exponencial. A proposta foi comparado ao DEPTA \cite{depta05}, ViDE
\cite{vide10}, G-STM \cite{gstm2010} e CTVS \cite{cvts2012}, apresentando
resultado ligeiramente superior aos demais: 99.1\% de \textit{recall} e 99.8\%
de precisão.

\section{Extração a partir de múltiplas páginas}\label{sec:site}
As abordagens de extração a partir de múltiplas páginas utilizam uma amostra de
um determinado \textit{site} para deduzir o seu \textit{template} e, a partir
daí, identificar o conteúdo de interesse. Em geral este problema é formulado
considerando que o \textit{template} da página é uma ``codificação'' dos dados
originais e as abordagens buscam ``decodificar'' esses dados.

Em \cite{RRunner01} é apresentado um algoritmo para extração de registros, a
partir de múltiplas páginas, chamado RoadRunner. Esta técnica considera
que o documento HTML ``codifica'' os dados originais em um \textit{template} e,
consequentemente, a extração é vista como um processo de ``decodificação''. O
algoritmo recebe, como entrada, uma coleção de documentos criados a partir do
mesmo \textit{template} e, a partir destes documentos, deduz uma expressão
regular para o \textit{template} do \textit{site}. Este tipo de técnica é
conhecida, de forma geral, como ``\textit{wrapper induction}''. Uma vez
encontrada a expressão regular esta é aplicada às demais páginas, criadas a
partir do mesmo \textit{template}, para extração dos dados. Esta pesquisa,
apresar de não ser recente, ainda é relevante e ainda é utilizada como
\textit{baseline} em novos trabalhos (\textit{e.g.}, \cite{wadar2015}) devido
sua implementação estar publicamente disponível. A publicação não apresenta
resultados em termos de \textit{recall} e precisão, apenas demonstra que a
técnica funcionou em 8 de 10 \textit{sites} contidos no \textit{dataset} de
testes.

Em \cite{exalg2003} é proposta uma abordagem chamada ExAlg. A técnica utiliza
uma formalização do problema semelhante à utilizada em \cite{RRunner01}, onde, a
partir de uma conjunto de páginas com o mesmo \textit{template}, o algoritmo
busca deduzir o \textit{template} do \textit{site} e extrair os dados.
O \textit{template} do \textit{site} é visto como uma ``codificação'' dos dados
e o objetivo do processo de extração é ``decodificar'' esses dados.
Embora a formulação do problema seja semelhante a \cite{RRunner01} a abordagem é
diferente. O algoritmo agrupa os \textit{tokens} das páginas de entrada com
relação à suas frequências (quantidade de ocorrências em cada página). Os grupos
de \textit{tokens} são chamados de classes de equivalência e a classe raiz é a
que contém os termos que ocorrem exatamente uma única vez em cada página e é a
partir da classe raiz que o \textit{template} é deduzido. O algoritmo foi
utilizado no mesmo \textit{dataset} de \cite{RRunner01}, extraindo corretamente
todo o conteúdo dessas páginas. Os autores também utilizaram um \textit{dataset}
próprio contendo páginas com \textit{templates} mais complexos, segundo eles,
onde o algoritmo extraiu corretamente 74 registros corretos e 12 registros
parcialmente corretos de um total de 86 registros.

Em \cite{fivatech2010} é proposta uma técnica chamada FiVaTech para extração de
registros a partir do processamento de múltiplas páginas com o mesmo
\textit{template}, de forma semelhante à \cite{RRunner01, exalg2003}. O
algoritmo proposto pode ser utilizado para extração em páginas que contenham
múltiplos registros ou que contenham um único registro de detalhe, segundo os
autores. O algoritmo realiza o alinhamento de várias árvores DOM utilizando uma
versão modificada do algoritmo de alinhamento de árvores proposto em
\cite{syntatic1991}. As árvores são processadas uma por vez e em cada nível os
nós são alinhados em uma matriz. A cada nova árvore adicionada, as matrizes de
alinhamento, em cada nível, precisam ser novamente realinhadas para acomodar os
nós da nova árvore. Após o alinhamento são identificados os nós ``variáveis'',
\textit{i.e.}, que contêm os dados (nó folha com conteúdo textual diferente dos
demais nós com os quais foi alinhado), e os demais são considerados
\textit{template} da página. Com esta informação, as páginas a serem extraídas
são podadas, para remoção do \textit{template}, e os nós remanescentes
representam os dados. Para extração de páginas com múltiplos registros, a
publicação não apresenta detalhes de como os registros são separados e
alinhados, apenas indica como a região de registros é identificada. O algoritmo
foi comparado com ExAlg e DEPTA\cite{exalg2003,depta05} apresentando resultados
consideravelmente melhores e com ViPER e MSE\cite{viper05,MSE2006} apresentando
resultados apenas ligeiramente superiores. Para medir os resultados foram
utilizadas as métricas padrão de \textit{recall} e precisão. Com o
\textit{dataset} utilizado para comparação com ExAlg o algoritmo apresentou
90.9\% de \textit{recall} e 95.1\% de precisão. Os demais \textit{datasets},
utilizados nas comparações com DEPTA, ViPER e MSE, são de páginas com múltiplos
registros e, como a publicação não detalha esse tipo de extração, esses
resultados são questionáveis.
 
\section{Quadro comparativo}
 
A seguir é apresentado, na Tabela \ref{table:compare}, um quadro comparativo
entre as abordagens analisadas.

As colunas \texttt{Tipo}, \texttt{Entrada}, \texttt{Alin.}, \texttt{Rend.},
\texttt{Heur.}, \texttt{KB}, \texttt{Impl.} indicam, respectivamente: o
\textbf{tipo} da abordagem, se é extração de listas, tabelas, registros ou regiões; a
\textbf{entrada}, se é apenas uma única página ou múltiplas páginas; o
\textbf{alinhamento} dos registros, se é realizado ou não; a
\textbf{renderização} da página HTML, se é necessária ou não; regras
\textbf{heurísticas}, se são empregadas ou não; \textbf{bases de conhecimento},
se são utilizadas ou não e; se existe \textbf{implementação} publicamente
disponível.

Idealmente, o que se procura é uma abordagem computacionalmente eficiente
(\textit{i.e.}, que não necessite de treinamento ou ``renderização'' a página e
tenha complexidade de tempo polinomial), que seja baseada em observações
gerais e não em regras heurísticas ou bases de conhecimento que
comprometam sua generalidade e sua validade no longo prazo. Além desses
atributos, para permitir a replicação e facilitar a compração e a análise das
propostas, estas devem ser disponibilizadas publicamente (bem como os
\textit{datasets}). Em muitos casos não é possível sequer implementar uma
abordagem, de forma independente, a partir da sua publicação, pois faltam
informações e detalhes indispensáveis para tanto.
 

\setcounter{magicrownumbers}{0}
\begin{table}[h]
\centering
\begin{tiny}
\caption{Comparativo entre abordagens de extração de dados estruturados.}
\label{table:compare}
%\rotatebox{-90}{
\begin{tabular}
{p{0.05cm}|p{4cm} p{0.8cm} p{0.5cm} p{0.3cm} p{0.4cm} p{0.4cm} p{0.2cm}
p{0.4cm}} \hline\hline
\#
&Abordagem
&Tipo
&Entrada
& Alin.
& Rend.
& Heur.
& KB
& Impl.
\\
\hline
\rownumber & \citeonline{webtables2008} & tabela & única & sim & não & sim & sim & não\\
\rownumber & \citeonline{tablesMS2012} & tabela & única & sim & não & sim & sim & não\\
\rownumber & \citeonline{listExtract2009} & lista & única & sim & não & sim & sim & não\\
\rownumber & \citeonline{tegra2015} & lista & única & sim & não & sim & sim & não\\
\rownumber & \citeonline{topklists2013} & lista & única & sim & não & sim & sim & não\\
\rownumber & \citeonline{MDR03} & registros & única & sim & não & não & não & sim\\
\rownumber & \citeonline{depta05} & registros & única & sim & sim & não & não & sim\\
\rownumber & \citeonline{NET05} & registros & única & sim & não & não & não & não\\
\rownumber & \citeonline{TPC09} & registros & única & não & não & não & não & não\\
\rownumber & \citeonline{SuffixTree12} & registros & única & não & não & sim & não & não\\
\rownumber & \citeonline{vips03} & regiões & única & não & sim & sim & não & sim\\
\rownumber & \citeonline{viper05} & registros & única & sim & sim & não & não & não\\
\rownumber & \citeonline{vide10} & registros & única & sim & sim & sim & não & não\\
\rownumber & \citeonline{clustVX2014} & registros & única & sim & sim & não & não & não\\
\rownumber & \citeonline{datapath2015} & registros & única & sim & sim & sim & não & não\\
\rownumber & \citeonline{autorm2015} & registros & única & não & sim & não & não & não\\
\rownumber & \citeonline{RRunner01} & registros & múlt. & sim & não & não & não & sim\\
\rownumber & \citeonline{exalg2003} & registros & múlt. & sim & não & não & não & não\\
\rownumber & \citeonline{fivatech2010} & registros & múlt. & sim & não & não & não & não\\
\end{tabular}
%}
\end{tiny}
\end{table}

\chapter{Proposta}\label{ch:prop}

A seguir são apresentados os desafios e perspectivas nesta área de pesquisa, na
Seção \ref{sec:persp}, e em seguida, uma proposta para abordagem do problema de
extração estruturada, composta das etapas de: limpeza do código HTML e
construção da árvore DOM (Seção \ref{sec:dom}); construção da sequência de
\textit{tag paths} (Seção \ref{sec:tps}); contorno da sequência, derivada e
segmentação (Seção \ref{sec:contour}); filtragem das regiões (Seção
\ref{sec:filter}); identificação e alinhamento dos registros (Seção
\ref{sec:align}).

A proposta tem como requisitos ser geral, eficiente e não
supervisionada para que possa ser aplicada em larga em escala. Para garantir a
generalidade da abordagem evitou-se a utilização de regras heurísticas
vinculadas ao HTML e qualquer suposição mais específica a respeito do
conteúdo e do \textit{template} dos documentos.

As premissas em que a abordagem se baseia são as seguintes:
\begin{description}
\item[Segmentação:] diferentes
regiões de uma página HTML são formatadas de maneiras diferentes, justamente
para que haja distinção entre elas, portanto serão formadas por conjuntos de \textit{tag
paths} diferentes, caso contrário não haveria diferença visual
significativa entre as regiões de uma página.
\item[Registros:] regiões com conteúdo semiestruturado são compostas por
conjuntos de registros semelhantes e contíguos, descritos por sequências de
\textit{tag paths} semelhantes sendo, portanto, regiões cíclicas.
\end{description} 

\section{Desafios e Perspectivas}\label{sec:persp}

A extração estruturada é composta por diversas etapas: extração, identificação
do \textit{schema}, integração e consulta. Nesta pesquisa o foco foi dirigido
para fase de extração, que foi decomposta em: identificação das regiões de
registros, identificação dos registros e alinhamento dos campos.

Diante do levantamento realizado, foi constatado que existem, ainda, muitos
problemas em aberto nesta área de pesquisa e, até o presente momento, não existe
nenhum consenso, considerando o problema como resolvido, com relação a nenhuma
das etapas do processo de extração estruturada.

Os resultados apresentados pelas abordagens existentes são satisfatórios,
geralmente, apenas nos \textit{datasets} elaborados pelos próprios autores, o
que indica que esses algoritmos foram, provavelmente, especializados para esses
conjuntos de dados e que, portanto, não generalizam bem, pois em comparações
subsequentes apresentaram resultados muito aquém dos originalmente publicados.
Sendo assim, existe bastante espaço para pesquisa de novas técnicas e algoritmos
mais gerais para o problema de estruturação da informação.

Com relação ao alinhamento de registros, muitas das técnicas sequer abordam esta
etapa. Apesar de ser uma área muito estudada na bioinformática (\textit{e.g.},
alinhamento de sequências de DNA), poucas soluções foram propostas
especificamente para área de \textit{data mining web}. Não é raro que
particularidades de um problema específico acabem levando a soluções melhores,
portanto também existe espaço para propostas de novas abordagem para o problema
de alinhamento de registros.

Além de existir espaço para proposta de novas abordagens para os problemas em
si, existem espaço, também, para propostas de padronização para esta área de
pesquisa, pois conforme apontado em levantamentos recentes
\cite{survey2013,survey2014} existe carência de \textit{datasets} padronizados
para \textit{baseline} e de métricas especializadas para o problema. 

Outra questão apontada nesses levantamentos, e em diversas outras publicações,
foi a disponibilização pública das abordagens para que os experimentos possam
ser replicados de forma independente. Neste quesito um \textit{framework} padrão
de implementação, que utilizasse métricas e \textit{datasets} apropriados,
facilitaria muito o trabalho de disponibilização e comparação das diversas
abordagens.
 
\section{Limpeza do código HTML e construção da árvore DOM}\label{sec:dom}
Para realizar a limpeza e correção de HTML malformado, foi utilizada a
ferramenta \textit{tidy} \cite{libtidy}. A biblioteca \textit{libtidy} foi
integrada ao sistema de extração. O HTML é corrigido e convertido para XHTML e a
árvore DOM é construída a partir daí.
A ferramenta \textit{tidy} é utilizada em praticamente todos os trabalhos que
necessitam manipular documentos HTML e é endossada pelo W3C \cite{tidyw3c}.

\section{Construção da sequência de \textit{tag paths} (TPS)}\label{sec:tps}
A sequência de \textit{tag paths} é gerada a partir da árvore DOM construída na
etapa anterior. A árvore DOM é percorrida em profundidade e para cada caminho
encontrado da raiz da árvore até cada um de seus nós é atribuído um código,
conforme ilustrado na Figura \ref{fig:ex1}. A sequência tem as seguintes
propriedades: o tamanho da sequência é igual ao número de nós na árvore DOM;
caminhos iguais recebem códigos iguais e; os códigos são atribuídos
incrementalmente.
O Algoritmo \ref{alg:tree2seq} detalha a construção da sequência.

\begin{algorithm}[H]
\caption{Converte árvore DOM para de sequência de \textit{tag paths}}
\label{alg:tree2seq}
\textbf{Input:} $node$ - um nó da árvore DOM, inicialmente a raiz da árvore \\
\textbf{Input:} $tp$ - o caminho de \textit{tags} atual \\
\textbf{Input:} $tps$ - a $TPS$ de uma árvore DOM, inicialmente vazia \\
\textbf{Output:} a $TPS$ de uma árvore DOM armazenada em $tps$
\begin{algorithmic}[1]
\Procedure{convertToSeq}{node,tp,tps by ref.}
\State $tp \leftarrow concat(tp,$``/''$,node.$tag$,node.style)$
\If {$tp \ni tagPathMap$}
\State $tagPathMap \leftarrow tagPathMap \cup \{tp\}$ 
\State $tagPathMap[tp].code \leftarrow tagPathMap.size$ 
\EndIf 
\State $tps \leftarrow concat(tps,tagPathMap[tp].code);$
\For {each $child$ of $node$}
\State $convertToSeq(child,tp,tps)$ 
\EndFor 
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{figure}[h]
  \centering
     \includegraphics[width=\textwidth]{img/example1-pt.jpg}
  \caption{Exemplo de uma sequência de \textit{tag paths} construída a partir do código HTML.}
  \label{fig:ex1}
\end{figure}
\vspace{3cm}

\section{Contorno, derivada e segmentação}\label{sec:contour}
Após a construção da sequência de \textit{tag paths} é realizado o cálculo do
contorno da sequência, conforme Algoritmo \ref{alg:contour}. Como os códigos dos
\textit{tag paths} (os símbolos) são \textbf{incrementais} a sequência sempre
avança para um novo patamar cada vez que um novo caminho é encontrado e nas
regiões com registros a sequência oscila, pois os registros possuem estruturas
semelhantes e, portanto, compartilham os mesmos símbolos, ou seja, os símbolos
se repetem nas regiões estruturadas do documento, conforme ilustra a Figura
\ref{fig:contour}.

O motivo pelo qual o contorno é calculado é que nas regiões estruturadas ele é
plano, pois essas regiões oscilam em torno de um mesmo patamar, ao passo que em
regiões não estruturadas o contorno é inclinado.
Portanto, onde o contorno for plano sua derivada deve ser igual a zero. Então,
calculando a derivada do contorno, encontramos as regiões que supostamente
contêm conteúdo estruturado, conforme ilustrado na Figura \ref{fig:deriv}.
Um vez identificadas, as regiões são analisadas com relação ao seus conjuntos de
códigos (ou símbolos do alfabeto). Caso as regiões compartilhem parte do
alfabeto com regiões adjacentes, estas são combinadas para formar uma única
região, conforme Algoritmo \ref{alg:merge}.

\begin{algorithm}[h]
\caption{Calcula contorno superior de uma sequência de \textit{tag paths}}
\label{alg:contour}
\textbf{Input:} $tps$ - a $TPS$ de uma árvore DOM \\
\textbf{Output:} o contorno da $tps$
\begin{algorithmic}[1]
\Function{contour}{tps}
\State $maxHeight \leftarrow 0$ 
\State $n \leftarrow length(tps)$
\For {$i \leftarrow 1..n$}
\If {$tps[i]>maxHeight$}
\State $maxHeight \leftarrow tps[i]$ 
\EndIf 
\State $contour[i] \leftarrow maxHeight$ 
\EndFor 
\State \Return $contour$ 
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{figure}[H]
  \centering
     \includegraphics[width=\textwidth]{img/contour-pt.jpg}
  \caption{Contorno da TPS com as principais regiões identificadas.}
  \label{fig:contour}
\end{figure}

\begin{figure}[h]
  \centering
     \includegraphics[width=\textwidth]{img/derivada-pt.jpg}
  \caption{Derivada do contorno da TPS.}
  \label{fig:deriv}
\end{figure}

\begin{algorithm}[H]
\caption{Combina regiões com alfabetos em comum}
\label{alg:merge}
\textbf{Input:} $regs$ - regiões em ordem ascendente de posição na TPS\\
\textbf{Output:} $mergedRegs$ regiões combinadas
\begin{algorithmic}[1]
\Function{merge}{regs}
\State $mergedRegs[1] \leftarrow regs[1]$ 
\State $j \leftarrow 1$ 
\State $n \leftarrow length(regs)$
\For {$i \leftarrow 2..n$}
\State $\Sigma_{previous} \leftarrow alphabet(mergedRegs[j])$ 
\State $\Sigma_{current} \leftarrow alphabet(regs[i])$
\If {$\Sigma_prev\cap\Sigma_curr\neq\emptyset$}
\State $mergedRegs[j] \leftarrow concat(mergedRegs[j], regs[i])$
\Else 
\State $j \leftarrow j + 1$ 
\State $mergedRegs[j] \leftarrow regs[i]$ 
\EndIf 
\EndFor 
\State 
\Return $mergedRegs$ 
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Filtragem das regiões}\label{sec:filter}
Uma vez que a sequência foi segmentada em regiões, este conjunto de regiões é
submetido a dois filtros.

No primeiro filtro (Algoritmo \ref{alg:struct}) é calculada a regressão linear
de cada região e analisado seu coeficiente angular. Se a inclinação for maior
que um determinado limiar a região é descartada por não possuir conteúdo
estruturado. Este procedimento é adotado, pois mesmo após a segmentação da
sequência utilizando a derivada do contorno, ainda é possível que algumas
regiões identificadas não possuam conteúdo estruturado. O coeficiente angular da
regressão linear é uma boa medida para identificar conteúdo estruturado, pois
essas regiões são cíclicas (ou estáveis na terminologia utilizada em
processamento de sinais) e, portanto, sua regressão linear será uma reta com
coeficiente angular próximo de zero como exemplificado na Figura \ref{fig:fft}a.

O segundo filtro (Algoritmo \ref{alg:noise}) serve para identificar se a região,
apesar de ter conteúdo estruturado, não se trata de ``ruído'' (menus,
propagandas, rodapé, etc). Para identificar estas regiões é calculado um
\textit{score} para cada uma delas a partir da sua posição na sequência (que
supõe-se ser correlacionada com sua posição na tela) e do seu tamanho em relação
ao documento como um todo. As regiões são ``clusterizadas'' em dois
\textit{clusters} utilizando o algoritmo proposto em \cite{1dkmeans2011} que é
uma versão do algoritmo \textit{kmeans} para uma dimensão (1D) que garante
solução ótima, utilizando programação dinâmica. O \textit{cluster} com maior
centro é considerado como conteúdo e as regiões do \textit{cluster} com menor
centro são descartadas. Supõe-se que exista ao menos uma região de dados.

\begin{algorithm}[H]
\caption{Identifica regiões estruturadas}
\label{alg:struct}
\textbf{Input:} $regs$ - conjunto de regiões\\
\textbf{Output:} $structuredRegs$ conjunto de regiões estruturadas
\begin{algorithmic}[1]
\Function{detectStructure}{regs}
\State $n \leftarrow length(regs)$
\State $structuredRegs \leftarrow \emptyset$
\For {$i \leftarrow 1..n$}
\State $angularCoef \leftarrow linearRegression(regs[i])$ 
\If {$|angularCoef|<10^\circ$}
\State $structuredRegs \leftarrow structuredRegs \cup regs[i])$
\EndIf 
\EndFor 
\State \Return $structuredRegs$ 
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Identifica regiões com ruído}
\label{alg:noise}
\textbf{Input:} $regs$ - conjunto de regiões\\
\textbf{Input:} $tps$ - sequência completa\\
\textbf{Output:} $contentRegs$ regiões com conteúdo de interesse
\begin{algorithmic}[1]
\Function{detectNoise}{regs, tps}
\State $tpsCenter \leftarrow \frac{tps.start + tps.end}{2}$
\State $tpsSize \leftarrow length(tps)$
\State $maxDistance \leftarrow \frac{tpsSize}{2}$
\State $contentRegs \leftarrow \emptyset$
\State $n \leftarrow length(regs)$
\For {$i \leftarrow 1..n$}
\State $regionCenter \leftarrow \frac{regs[i].start + regs[i].end}{2}$
\State $positionScore \leftarrow 1-\frac{|tpsCenter - regionCenter|}{maxDistance}$ 
\State $sizeScore \leftarrow \frac{length(regs[i])}{tpsSize}$ 
\State $score[i] \leftarrow \frac{positionScore+sizeScore}{2}$
\EndFor
\State $clusters \leftarrow kmeans1D(regs, score, 2)$ \Comment{força dois clusters}
\For {$i \leftarrow 1..n$}
\If {$regs[i] \in clusters[1]$} \Comment {descarta cluster zero}
\State $contentRegs \leftarrow contentRegs \cup regs[i]$
\EndIf
\EndFor
\State \Return $contentRegs$
\EndFunction
\end{algorithmic}
\end{algorithm}

\section{Identificação e alinhamento dos registros}\label{sec:align}
Após a filtragem das regiões é calculado o espectro de potência e a
autocorrelação para cada uma delas. Ambos os cálculos são realizados de forma
eficiente em tempo $O(nlogn)$. Essas informações são utilizadas para encontrar
uma estimativa para o período de cada região.

Como as regiões estruturadas são cíclicas, o período de cada uma dessas regiões
corresponde ao tamanho dos registros que elas contêm, conforme ilustrado na
Figura \ref{fig:fft}. O Algoritmo \ref{alg:period} apresenta o cálculo da
estimativa de tamanho e quantidade de registros, conforme descrito a seguir.

Cada ponto da curva de autocorrelação corresponde a um deslocamento da sequência
em relação à ela mesma. Os pontos com maior correlação são possíveis períodos,
pois significam que a sequência tem uma correlação maior com ela mesma deslocada
até aquele ponto, \textit{e.g.}, se existe uma autocorrelação siginificativa com
deslocamento igual $15$, pode siginificar que os registros tem um tamanho médio
de $15$ nós e uma quantidade (ou frequência) aproximada de $\frac{n}{15}$, onde
$n$ é o tamanho total da região.
Desta forma, os picos da autocorrelação são ordenados do maior para o menor
(Figura \ref{fig:fft}b) e seus períodos são convertidos para frequência (Figura
\ref{fig:fft}c) para verificação contra o espectro de frequência da região. O
período da autocorrelação que possuir correspondência com o maior pico no
espectro de frequência é adotado como sendo o valor provável do tamanho e
quantidade dos registros.

Após a apuração dos valores mais prováveis de tamanho e quantidade dos
registros, os símbolos da região são analisados para encontrar qual deles melhor
divide a sequência em registros (Algoritmo \ref{alg:recs}). Os símbolos são
analisados do menor para o maior (ordem ascendente), pois os códigos são
atribuídos incrementalmente, o que significa que quanto menor o código de um
símbolo, antes ele surgiu na sequência (e, portanto, está mais próximo da raiz),
por esse motivo os símbolos devem ser analisados nesta mesma ordem.
O \textit{score} de cada símbolo é calculado a partir da estimativa de tamanho e
quantidade de registros e, também, do percentual total da região coberta por
aquele símbolo, como demostrado nas Equações
\ref{eq:scoreSize}, \ref{eq:scoreCount} e \ref{eq:scoreCoverage} e
\ref{eq:scoreTotal} e na Figura \ref{fig:fft}a.

%\begin{small}
\begin{equation}\label{eq:scoreSize}
score_{tamanho} = \frac{min(tamanhoMedio, periodoEstimado)}{max(tamanhoMedio,
periodoEstimado)}
\end{equation}
\begin{equation}\label{eq:scoreCount}
score_{quantidade} = \frac{min(quantidade, frequenciaEstimada)}{max(quantidade,
frequenciaEstimada)}
\end{equation}
\begin{equation}\label{eq:scoreCoverage}
score_{cobertura}=\frac{posicaoRegistro_n - posicaoRegistro_1}{tamanhoTotalRegiao}
\end{equation}
\begin{equation}\label{eq:scoreTotal}
score_{total}=\frac{score_{tamanho}+score_{quantidade}+score_{cobertura}}{3}
\end{equation}
%\end{small}

Após a identificação dos registros, a sequência é dividida e as subsequências
são alinhadas com um algoritmo de alinhamento de múltiplas sequências (MSA), o
\textit{center star} \cite{centerstar1993,centerstar2011book}. O problema de
alinhamento de múltiplas sequências foi demonstrado NP-hard
\cite{msanphard2006}, portanto soluções aproximadas com complexidade polinomial
precisam ser utilizadas. Este é o caso do \textit{center star} que além de
fornecer uma solução aproximada em tempo polinomial, também tem garantias de
erro máximo.

\begin{figure}[h]
  \centering
     \includegraphics[width=\textwidth]{img/fftxcorr-pt.jpg}
  \caption{a) TPS e regressão linear; b) autocorrelação; c) FFT e estimativas
  de quantidade e tamanho dos registros.}
  \label{fig:fft}
\end{figure}

\begin{algorithm}[h]
\caption{Calcula estimativa do período de uma região}
\label{alg:period}
\textbf{Input:} $region$ - região\\
\textbf{Output:} $period$ período estimado para a região
\begin{algorithmic}[1]
\Function{estimatePeriod}{region}
\State $region \leftarrow region - mean(region)$
\State $fftRegion \leftarrow FFT(region)$
\State $powerSpectrum \leftarrow |fftRegion|$
\State $xCorr \leftarrow InverseFFT(fftRegion \cdot fftRegion^*)$
\State $xCorrPeaks \leftarrow sort(xCorr,descending)$
\State $n \leftarrow length(region)$
\State $maxPeak \leftarrow -\infty$
\For {$i \leftarrow 1..15$} \Comment {apenas 15 maiores picos}
\State $peak \leftarrow powerSpectrum[\frac{n}{xCorrPeaks[i]_{position}}]$
\If {$ peak>maxPeak$}
\State $maxPeak \leftarrow peak$
\State $period = \frac{n}{maxPeak}$
\EndIf
\EndFor
\State \Return $period$ 
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Localiza registros}
\label{alg:recs}
\textbf{Input:} $region$ - região\\
\textbf{Output:} $recs$ posição de início de cada registro
\begin{algorithmic}[1]
\Function{locateRecords}{region}
\State $\Sigma \leftarrow alphabet(region)$
\State $n \leftarrow length(region)$
\State $estimatedPeriod \leftarrow estimatePeriod(reg)$ \Comment{Algoritmo
\ref{alg:period}} 
\State $estimatedCount \leftarrow \frac{n}{estimatedPeriod}$
\State $candidates \leftarrow sort(\Sigma, ascending)$
\State $score \leftarrow -\infty$
\While {$candidates \ne \emptyset$}
\State $symbol \leftarrow candidates.first$
\State $candidates \leftarrow candidates - symbol$
\State $cRecs \leftarrow findAll(symbol,region)$
\State $count \leftarrow length(cRecs)$
\State $size \leftarrow
\frac{\sum_{i=2}^{count}{cRecs[i]-cRecs[i-1]}}{count-1}$ \State
$score_{size} = \frac{min(estimatedPeriod,size)}{max(estimatedPeriod,size)}$
\State $score_{count} =
\frac{min(estimatedCount,count)}{max(estimatedCount,count)}$
\State $score_{coverage} = \frac{cRecs[count]-cRecs[1]}{n}$ 
\State $score_{total} =
\frac{score_{size}+score_{count}+score_{coverage}}{3}$
\If {$score_{total} > score$}
\State $score \leftarrow score_{total}$
\State $recs \leftarrow cRecs$
\If {$score_{size,count,coverage}>0.75$} 
\State $break$ \Comment{termina se score bom o suficiente}
\EndIf
\EndIf
\EndWhile
\State \Return $recs$ 
\EndFunction
\end{algorithmic}
\end{algorithm}

% (identificar tamanho aproximado dos registros, investigar outras técnicas para
% detecção de periodicidade em sinais, inclusive lempel-ziv)

% (localizar qual TPC melhor divide a região em registros, fazer testes com
% lempel-ziv só no segmento, pesquisar tecnicas para identificacao de tandem
% repeats) (center star, investigar alinhamento global utilizado em ViPER e
% ViDE) schema inference (Probase e/ou ACSDb, não tem como fugir das bases de
% conhecimento) indexação de tabelas (webtables, analisar a proposta e procurar
% por melhorias)

\bibliographystyle{ufscThesis/ufsc-alf}
\bibliography{refs}
\end{document}
