\documentclass{ufscThesis}
\usepackage{graphicx}
\usepackage[labelsep=endash]{caption} % O separador de legenda é um -
\usepackage{algorithmicx}
\usepackage[Algoritmo,ruled]{algorithm}
\usepackage{algpseudocode}

\newtheorem{definition}{Definição}

\titulo{Segmentação e Remoção Automática de Ruído de Páginas Web Utilizando
$Tag$ $Paths$}
% Titulo do trabalho \subtitulo{Estilo \LaTeX~ padrão}                % Subtitulo do trabalho (opcional)
\autor{Roberto Panerai Velloso}
\data{22}{maio}{2013}                           % Data da publicação do
\orientador{Profa. Dra. Carina F. Dorneles}                    % Nome do
\begin{document}
\capa
\folhaderosto[comficha] % Se nao quiser imprimir a ficha, é só não usar o
% parâmetro
%\folhaaprovacao
%\paginadedicatoria
%\paginaagradecimento
%\paginaepigrafe
%\listadefiguras
%\listadetabelas 
%\listadeabreviaturas
%\listadesimbolos
%\sumario

\textoResumo{Segmentação e remoção de ruído de páginas web são etapas essenciais 
no processo de extração de dados estruturados. Identificar a região principal da 
página, eliminando o que não é importante (menus, anúncios, etc.), pode melhorar 
significativamente o desempenho do processo de extração. Para essa tarefa é proposto 
um novo algoritmo, totalmente automático, que utiliza uma sequência tag paths (TPS) 
como representação da página web. A TPS é composta por uma sequência de símbolos 
(string), cada um representando um tag path diferente. A técnica proposta procura 
por posições na TPS onde é possível dividi-la em duas regiões de tal forma que seus 
alfabetos não se intersectem, o que significa que as regiões têm conjuntos de tag 
paths completamente distintos e, portanto, são regiões diferentes da página. Os 
resultados mostram que o algoritmo é muito efetivo em identificar o conteúdo principal 
de vários sites, e melhora a precisão da extração, removendo resultados irrelevantes.}

\textAbstract{Web page segmentation and data cleaning are essential steps in structured web
data extraction. Identifying a web page main content region, removing what is
not important (menus, ads, etc.), can greatly improve the performance of the extraction
process. We propose, for this task, a novel and fully automatic algorithm that
uses a tag path sequence (TPS) representation of the web page. The TPS consists
of a sequence of symbols (string), each one representing a different tag path.
The proposed technique searches for positions in the TPS where it is possible to
split it in two regions where each region's alphabet do not intersect,
which means that they have completely different sets of tag paths and, thus, are
different regions. The results show that the algorithm is very effective in
identifying the main content block of several major websites, and improves the
precision of the extraction step by removing irrelevant results.
}

\paginaresumo
\paginaabstract

\chapter{Contextualização do problema}

Um passo fundamental da mineração de dados na web, inclusive em extração
estruturada, é a fase de limpeza que deve ocorrer antes da extração da
informação. Não se pode esperar bons resultados na fase de extração sem efetuar
a limpeza da página, removendo o ruído indesejado. Em \cite{Noisy03}, é
mencionado que apesar da importância desta tarefa, existem relativamente poucos
trabalhos nesta área e, de acordo com \cite{Editorial04}, o ruído existe nas
páginas pode prejudicar consideravelmente a mineração de dados na web.

Na extração estruturada, a maioria das técnicas existentes utilizam algum tipo
de reconhecimento de padrão para identificar os registros (como definido em
\cite{MDR03}) presentes nas páginas. O problema é que, em geral, apenas os
registros da região principal da página interessam, porém outras regiões da
página (menus, anúncios, etc.) freqüentemente contém padrões que também são
extraídos. Portanto, é importante efetuar a limpeza das páginas web antes de
extrair seu conteúdo estruturado.

Atualmente, alguns trabalhos na área de remoção de ruído e segmentação são
direcionados para indexação e $clustering$ de páginas ($i.e.$ é assumido que a
região principal é textual/não estruturada) como em
\cite{SiteOriented11,Noisy03} e, devido às diferenças intrínsecas entre
conteúdo estruturado e não estruturado, estas técnicas não podem ser
utilizadas para extração estruturada. As técnicas existem que podem ser
utilizadas para este fim requerem definições \textit{a priori} (\cite{vips03}),
ou treinamento prévio (\cite{Graph08}), ou então dependem de aspectos
específicos da linguagem HTML para funcionar, como em \cite{Entropy09}.

Este trabalho propõe e descreve um algoritmo para segmentação
e remoção de ruído de páginas web que utiliza a representação de seqüência de  
$tag$ $paths$ de uma página e que é simples e computacionalmente eficiente.
Trata-se de uma técnica geral de segmentação de páginas, utilizada aqui no
contexto de extração estruturada, levando em consideração o estilo e a estrutura
da página. As principais contribuições são as seguintes: 

\begin{itemize}
\item{Totalmente automática}: não necessita de treinamento nem intervenção
humana;
\item{Independente de domínio}: o único requisito é que a página possua
conteúdo estruturado, não importando a que domínio pertença;
\item{Independente da sintaxe HTML}: não existem regras associadas a $tags$
HTML específicas;
\item{Funciona em uma página}: é necessária apenas uma página como entrada, o
que constitui uma grande vantagem desta proposta, como discutido em \cite{Editorial04}.
\end{itemize}

Para avaliar o quão efetiva é a abordagem proposta neste trabalho, foi comparado o
ruído de saída do MDR \cite{MDR03}, uma conhecida técnica de extração
estruturada, contra o ruído de saída do MDR combinado com a técnica proposta, como 
ilustrado na Figura \ref{fig:method}, obtendo-se uma média de 77,03\% de ruído removido 
das páginas do conjunto de teste.

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.34]{method.jpg}
  \caption{Evaluation method adopted.}
  \label{fig:method}
\end{figure}


Este documento está organizado da seguinte maneira: no Capítulo \ref{ch:obj} são
apresentados os objetivos da pesquisa; no Capítulo \ref{ch:contr}
são enunciadas as principais contribuições do trabalho; no Capítulo \ref{ch:rev} é feita uma
revisão dos trabalhos relacionados, observando as limitações das propostas
existentes para solução do problema; no Capítulo \ref{ch:def} são enunciadas
as definições necessárias ao entendimento da proposta; no Capítulo \ref{ch:probform}
é descrita a proposta deste trabalho e as hipóteses nas quais a mesma se baseia;
no Capítulo \ref{ch:alg} são detalhados e ilustrados os algoritmos desenvolvidos
a partir da formulação do problema e das hipóteses; no Capítulo \ref{ch:met}
o método de avaliação dos resultados é detalhado; no Capítulo \ref{ch:res}
são apresentados os resultados obtidos e; no Capítulo \ref{ch:conclusion} uma conclusão final.

\chapter{Objetivos}\label{ch:obj}

O objetivo desta pesquisa é contribuir para área de extração estruturada de
dados da web, propondo uma nova técnica para segmentação e remoção de ruído de
páginas que possibilite incrementar a precisão dos métodos atuais, com o
mínimo de intervenção manual e o máximo de independência com relação a aspectos
da linguagem HTML e ao domínio da informação.

\chapter{Contribuições}\label{ch:contr}

A contribuição desta pesquisa é uma nova técnica de segmentação e remoção de
ruído de páginas web com as seguintes características:
\begin{itemize}
\item{Técnica completamente automática}: não é necessário treinamento nem
intervenção humana;
\item{Independente de domínio}: é necessário apenas que a página contenha
conteúdo estruturado, não importando a que domínio a informação pertence;
\item{Independente da sintaxe HTML}: não são utilizadas regras associadas a
$tags$ HTML;
\item{Necessita de apenas uma página}: não requer várias páginas do mesmo $site$
para funcionar, o que é um grande vantagem, como discutido em \cite{Editorial04}.
\end{itemize}

\chapter{Revisão bibliográfica}\label{ch:rev}

Existem muitos trabalhos propondo formas de segmentar páginas web e identificar
o que é conteúdo e o que é ruído nelas. Elas podem ser agrupadas em três
categorias: as que se baseiam no texto da página, as que se baseiam na árvore
DOM e as que utilizam informação visual.

\textbf{Técnicas baseadas no texto}. Em
\cite{Densiometric08,Boilerplate10,CETR10,BlockImp07,NonTemplate13} a
segmentação é realizada utilizando o conteúdo textual da página web. O foco
destes trabalhos, entretanto, não é em extração estruturada, mas em indexação e
$clustering$ de páginas. A maioria dos trabalhos existentes sobre segmentação e
remoção de ruído são direcionados para este tipo de aplicação. 

\textbf{Técnicas baseadas na árvore DOM}. Em
\cite{Entropy09,SiteOriented11,Graph08,Noisy03,Entropy12} a segmentação é
realizada utilizando a árvore DOM da página, portanto, estas técnicas levam em
consideração a estrutura da página web. Entretanto, \cite{SiteOriented11} e
\cite{Noisy03} necessitam de várias páginas de um mesmo $site$, \cite{Graph08}
propõe um $framework$ de treinamento que necessita de dados catalogados
manualmente para funcionar e \cite{Entropy12} necessita de uma base de dados de
termos associados a ``papéis semânticos'' para poder detectar regiões de dados.

\textbf{Técnicas baseadas em informação visual}.
Além das técnicas baseadas no texto e na árvore DOM da página, existem também
aquelas que utilizam informação visual, como \cite{vips03,vide09,viper05}. Todas
dependem de um renderizador de um navegador web para obter as informações
visuais das páginas, o que pode ser computacionalmente dispendioso, e, além
disso, \cite{vips03} é baseado em um conjunto de regras heurísticas, cada uma
aplicada a $tags$ HTML específicas. Técnicas que dependem fortemente de aspectos
específicos da linguagem HTML tem a séria desvantagem de serem afetadas por
mudanças nas práticas de desenvolvimento assim como mudanças na sintaxe da
linguagem.

A representação da página web utilizada nesta pesquisa (seqüência de
$tag$ $paths$) foi anteriormente empregada em \cite{TPC09} e
\cite{SuffixTree12}, embora em ambos os casos para a extração e não para segmentação e remoção de
ruído. Estes dois trabalhos são citados aqui apenas para ilustrar que, de acordo
com seus resultados, esta representação, assim como a árvore DOM, também é capaz
de expor a estrutura da página web e, portanto, serve para os propósitos desta pesquisa.
A Figura \ref{fig:ex1} ilustra como o código HTML de uma página web é convertido para
representação de $tag$ $paths$.

\begin{figure}[h]
  \caption{Exemplo de uma seqüência de \textit{tag paths} contruída a
  partir do código HTML.}
  \label{fig:ex1}
  \centering
    \includegraphics[scale=0.39]{example1.jpg}
\end{figure}

\chapter{Definições}\label{ch:def}

Neste capítulo são apresentados os conceitos e definições necessários para o
entendimento do problema e do algoritmo proposto. As definições são
exemplificadas com o auxílio da Figura \ref{fig:ex1}.
\begin{definition}[\textbf{Árvore DOM}]\label{def:domtree}
A árvore DOM é uma estrutura hierárquica, derivada do código HTML, que
representa a página web.
\end{definition}
Na Figura \ref{fig:ex1} um pequeno trecho de código é utilizado para ilustrar a
árvore DOM e as próximas definições.

\begin{definition}[\textbf{$Tag$ $paths$}]\label{def:tp} Um tag path (TP) é uma
string descrevendo o caminho absoluto, a partir da raiz da árvore DOM, até um
dado nó da árvore. Seja ``i'' a posição em profundidade do nó $``n_i"$ na
árvore, então é dito que $TP_i$ é a string descrevendo o caminho, a partir da
raiz da árvore DOM, até o nó $``n_i"$.
\end{definition}
Na Figura \ref{fig:ex1}, o caminho absoluto $TP_4$ do nó $body$ até a
célula da tabela $td_4$ é $TP_4=``body/table/tr/td"$. 

\begin{definition}[\textbf{Seqüência de \textit{tag paths}}]\label{def:tps}
Uma seqüência de tag paths (TPS) de uma árvore DOM com ``n'' nós é definida como
sendo a seqüência ordenada $TPS[1..n] = (TP_1,TP_2,TP_3, ...,TP_{n-1},TP_n)$
onde dois tag paths $TP_i$ e $TP_j$, com $i\neq{}j$, são considerados iguais
apenas se seus caminhos e suas definições de estilo são iguais, caso
contrário eles são considerados diferentes.
\end{definition}
Esta é a mesma definição utilizada em \cite{SuffixTree12}, onde cada $tag$
$path$ é representado na seqüência por um símbolo, exceto que neste trabalho
foram incorporadas as definições de estilo das $tags$. Na Figura \ref{fig:ex1}
é mostrada a $TPS$ de um dado trecho de código HTML, onde cada $TP$ recebe um
código, produzindo $TPS = (1,2,3,4,4,3,4,4)$.

\begin{definition}[\textbf{Alfabeto da TPS}]\label{def:alpha} Seja $\Sigma_a$ o
conjunto contendo todos os símbolos de uma dada seqüência $TPS_a$ de tamanho
``n'', $\Sigma_a$ é dito o alfabeto de $TPS_a$ definido como $\Sigma_a = \{\alpha |
\exists{TPS_a[i]}=\alpha \wedge 1 \leq i \leq n\}$, onde $\alpha$ é um símbolo
do alfabeto.
\end{definition}
Informalmente, o alfabeto contém todos os símbolos distintos de uma determinada
seqüência. Na Figura \ref{fig:ex1}, a $TPS$ é formada apenas pelos símbolos
``1'',``2'', ``3'' e ``4'', então seu alfabeto é $\Sigma=\{1,2,3,4\}$.

\begin{definition}[\textbf{Conjunto de freqüências de tag paths}]\label{def:tpfs}
Seja $(s,f)$ um par ordenado onde ``s'' é um símbolo de um alfabeto de uma
determinada TPS e ``f'' é o número de vezes que ``s'' ocorre na TPS, então o conjunto de
freqüências de tag paths é definido como sendo o conjunto de todos os pares 
$(s,f)$ possíveis de uma TPS. Seja $FS =
\{(s_1,f_{s1}),(s_2,f_{s2}),(s_3,f_{s3})$
\\ $,\ldots,(s_{n-1},f_{sn-1}),(s_n,f_{sn})\}$, onde ``n" é o tamanho da TPS.
\end{definition}
Na Figura \ref{fig:ex1}, o símbolo ``1'' ocorre uma vez na seqüência, o símbolo
``2'' ocorre uma vez também, o símbolo ``3'' duas vezes e o símbolo ``4'' quatro
vezes, então para esta seqüência $FS = \{(1,1), (2,1), (3,2), (4,4)\}$. O
conjunto $FS$ é o mapeamento de todos os símbolos de uma seqüência para suas
respectivas freqüências.

\begin{definition}[\textbf{Limiares de freqüência}]\label{def:ft}
Dada uma $TPS_a$ com alfabeto $\Sigma_a$ e conjunto de freqüências $FS_a$, os
limiares de freqüência $FT_a$ são definidos como sendo o conjunto ordenado
contendo apenas as freqüências de $FS_a$. Seja $FT_a = \{f|\exists{(s,f)} \wedge
(s,f) \in FS_a \wedge s \in \Sigma_a \}$, onde ``f'' é uma freqüência e ``s'' é
o símbolo correspondente do alfabeto $\Sigma_a$.
\end{definition}
Na TPS exibida na Figura \ref{fig:ex1}, o conjunto de freqüências é $FS=\{(1,1),
\\ (2,1), (3,2), (4,4)\}$, neste caso o conjunto dos limiares de freqüência é
igual a $FT=\{1,2,4\}$, pois os símbolos ``1'' e ``2'' tem freqüência igual a
\textbf{1}, o símbolo ``3'' tem freqüência igual a \textit{2} e o símbolo ``4''
tem freqüência igual a \textit{4}. O conjunto $FT$ é necessário para filtrar os
símbolos da TPS. Se $FT=\{1,2,4\}$, então não faria sentido filtrar símbolos com
freqüência igual a \textit{3}, pois não existem símbolos na seqüência com esta
freqüência.

\begin{definition}[\textbf{Região}]\label{def:region} 
Seja uma TPS a concatenação de duas outras seqüências $TPS=TPS_a . TPS_b$, então
é dito que $TPS_a$ e $TPS_b$ são regiões de $TPS$, sse $\Sigma_a \cap \Sigma_b =
\emptyset$.
\end{definition}
Na Figura \ref{fig:ex1} a TPS pode ser dividida em duas subseqüências
$TPS_a=TPS[1..2]=(1,2)$ e $TPS_b=TPS[3..8]=(3,4,4,3,4,4)$, com alfabetos
$\Sigma_a=\{1,2\}$ e $\Sigma_b=\{3,4\}$, então $TPS_a$ a $TPS_b$
são regiões disintitas de $TPS$, pois $\Sigma_a \cap \Sigma_b=\emptyset$.

\chapter{Proposta}\label{ch:probform}

Nesta pesquisa é proposta uma nova técnica para segmentação das regiões de uma
página web e identificação da região principal. A técnica proposta é simples,
computacionalmente eficiente e leva em consideração a estrutura e os estilos de
formatação das páginas. A página web é representada na forma de $tag$ $paths$. É
um método de segmentação genérico, utilizado aqui no contexto da extração estruturada.

O algoritmo proposto é formulado a partir de duas hipóteses:
\begin{enumerate}
  \item\label{ass:1} diferentes regiões de uma página web são descritas por
  diferentes $tag$ $paths$;
  \item\label{ass:2} em \textit{sites} com conteúdo estruturado, a região
  principal é mais densa/maior que as demais regiões (menus, anúncios, text, etc.). 
\end{enumerate}

A formulação da Hipótese \ref{ass:1} vem da observação de que as várias
regiões de uma página $web$ são diferentes ramificações da árvore DOM e essas regiões
são descritas ou utilizando $tags$ distintas para cada região ou, se as $tags$
forem semelhantes, com diferentes estilos, para que cada região possa ser
facilmente distingüida pelo usuário/leitor da página. Se todas as regiões de uma
página fossem parecidas, seria mais difícil, para o usuário, distingüi-las.
Então, pela Definição \ref{def:tps}, pode-se notar que os símbolos utilizadas
em cada região de uma página web deveriam ser diferentes, e então deve ser possível
segmentar a página como na Definição \ref{def:region}.

A Hipótese \ref{ass:2} vem do contexto no qual a segmentação proposta neste
trabalho é aplicada (i.e. extração estruturada). Como se está considerando
apenas páginas com registros, e para descrever a estrutura desses registros são
necessários nós da árvore DOM, então são necessários mais nós para descrever
conteúdo estruturado do que é necessário para conteúdo não estruturado (e.g.
texto). Portanto, é razoável assumir que, para páginas que contém registros, a
região principal da página é a maior de todas (i.e. a que contém mais nós).

Utilizando as definições do Capítulo \ref{ch:def} e as hipóteses acima, podemos
formular o problema de segmentação de páginas e identificação da região
principal como: \textbf{``encontrar a maior \textit{região} da $TPS$ de uma
página $web$ que possua um $alfabeto$ que não se intersecte com o $alfabeto$ de
outras \textit{regiões} menores''}.

Um detalhe \textbf{crucial} que deve ser levado em consideração, é que podem
existir $tag$ $paths$ em uma página que representam divisões estruturais da
página (i.e. formatação visual da página). Estes $tag$ $paths$, se forem
divisões, irão ocorrer ao longo de toda a $TPS$, impedindo a identificação das
regiões, pois sempre haverá uma intersecção dos alfabetos. Para remover este
``ruído'' da seqüência, podemos filtrar os símbolos com freqüências mais baixas,
sem prejudicar o processo de segmentação, pois os símbolos com freqüências mais
altas continuam sendo considerados.

Apenas para ilustrar, é apresentado um exemplo de uma página $web$ que inicia e
termina com o mesmo $tag$ $path$ (``/body/br'') e tem três regiões
delimitadas, também, pelo mesmo $tag$ $path$ (``/body/div''). Assumindo que
diferentes $tag$ $paths$ são utilizados para descrever cada região, sem filtrar
os símbolos de baixa freqüência da $TPS$, não seria possível identificar essas
regiões.

\begin{itemize}
\item{código HTML}
\begin{small} 
\begin{verbatim}
<body>
<br>
<div>
  <span class='region1'>...</span>
  ...
  <span class='region1'>...</span>
</div> 
<div>
  <span class='region2'>...</span>
  ...
  <span class='region2'>...</span>
</div>
<div>
  <span class='region3'>...</span>
  ...
  <span class='region3'>...</span>
</div>
<br>
</body>
\end{verbatim}
\end{small}
\item{TPS}
\begin{verbatim}
TPS = (1,2,3,4,...,4,3,5,...,5,3,6,...,6,2)
\end{verbatim}

Os símbolos $2$ e $3$ ocorrem ao longo de toda a $TPS$.

\item{TPS filtrada}
\begin{verbatim}
TPS = ( , , ,4,...,4, ,5,...,5, ,6,...,6, )
\end{verbatim}

Apenas símbolos com freqüência maior que $3$ são considerados no processo de
segmentação. Agora é possível dividir a $TPS$ em regiões.

\end{itemize}

\chapter{Algoritmos}\label{ch:alg}

Neste capítulo são apresentados os algoritmos desenvolvidos para tratar o problema
definido no Capítulo \ref{ch:probform}. Quais sejam: 

\begin{itemize}
  \item {}\textit{\textbf{tagPathSequenceFilter()}}. É o algoritmo principal, que
   recebe um arquivo HTML como entrada e retorna uma árvore DOM, após a poda, com o conteúdo da região principal;
  \item {}\textit{\textbf{convertToSeq()}}. Converte a árvore DOM da página web em uma $TPS$;
  \item {}\textit{\textbf{searchRegion()}}. A busca, propriamente dita, pela região principal da $TPS$;
  \item {}\textit{\textbf{filter()}}. Filtra um alfabeto, removendo 
   símbolos de menor freqüência, tornando o algoritmo de busca mais robusto e resistente ao ruído;
  \item {}\textit{\textbf{pruneDOMTree()}}. Poda da árvore DOM original, deixando apenas o conteúdo
   da região principal encontrado com $searchRegion$, mantendo a estrutura original do documento.
\end{itemize}

\section{$tagPathSequenceFilter()$ Algorithm}

\begin{algorithm}[H]
\caption{Filters out noise from a web page}
\label{alg:tpsfilter}
\textbf{Input:} $inputFile$ - an HTML file \\
\textbf{Output:} pruned $inputFile$'s DOM tree
\begin{algorithmic}[1]
\Procedure{tagPathSequenceFilter}{$inputFile$}
\State $DOMTree \leftarrow parseHTML(inputFile)$
\State $convertToSeq(DOMTree.body,$`` ''$,TPS)$
\State $searchRegion(TPS)$
\State $pruneDOMTree(DOMTree.body,TPS)$
\State return $DOMTree$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $tagPathSequenceFilter()$ no Algoritmo \ref{alg:tpsfilter} retorna
a região principal de $inputFile$. O procedimento $parseHTML()$, na 
Linha $2$, converte o código HTML em uma árvore DOM;
$convertToSeq()$, na Linha $3$, converte a árvore DOM em uma TPS; o
procedimento $searchRegion()$, na Linha $4$, procura recursivamente pela maior 
região da TPS que possui um alfabeto único e, finalmente; $pruneDOMTree()$, na
Linha $5$, retira da árvore DOM (poda) todos os nós que não estão na TPS
resultante, preservando a estrutura do documento retornado na Linha $6$.

Aabixo são detalhados os algoritmos $convertToSeq()$, $searchRegion()$,
$filter()$ e $pruneDOMTree()$. O algoritmo $parseHTML()$ não faz parte do escopo 
deste trabalho, portanto não será discutido aqui.


\section{Algoritmo convertToSeq()}

\begin{algorithm}[H]
\caption{Converts a DOM tree to a tag path sequence representation}
\label{alg:tree2seq}
\textbf{Input:} $node$ - a DOM tree node, initially the root \\
\textbf{Input:} $tp$ - the previous tag path, initially empty \\
\textbf{Input:} $TPS$ - the $TPS$ built from the DOM tree,
initially empty \\
\textbf{Output:} the $TPS$ for the given DOM tree stored in $TPS$
\begin{algorithmic}[1]
\Procedure{convertToSeq}{node,tp,TPS by ref.}
\State $tp \leftarrow concat(tp,$``/''$,node.tag,node.style)$
\If {$tp \ni tagPathMap$}
\State $tagPathMap \leftarrow tagPathMap + \{tp\}$
\State $tagPathMap[tp].code \leftarrow tagPathMap.size$
\EndIf
\State $TPS \leftarrow concat(TPS,tagPathMap[tp].code);$
\For {each $child$ of $node$}
\State $convertToSeq(child,tp,TPS)$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $convertToSeq()$ no Algoritmo \ref{alg:tree2seq}
converte uma página web de sua representação em árvore DOM para uma representação em formato TPS,
percorrendo a árvore DOM em profundidade. Inicialmente é chamado no
Algoritmo \ref{alg:tpsfilter} com um $tagPath$ vazio como parametro, que 
representa a $string$ do tag path anterior (i.e. da chamada recursiva anterior).
Na Linha $2$, o $tag$ $path$ anterior é concatenado com a $tag$ atual e com
suas definições de estilo, com objetivo de distinguir caminhos repetidos com
estilos diferentes; na Linha $3$, é verificado se o $tag$ $path$ atual já foi
visto antes pelo algoritmo ou não ($tagPathMap$ is used for this purpose) e, se
não, na Linha $4$, ele é inserido no conjunto $tagPathMap$ e um novo código é
atribuído à ele na Linha $5$, como definido na Definição \ref{def:tps}; na Linha
$7$, o código do tag path é concatenado ao final da sequencia e, finalmente, o
procedimento e invocado recursivamente na Linha $9$ para cara filho de $node$.

\section{Algoritmo searchRegion()}

Este é o algoritmo central, já que é responsável por encontrar a região
principal, portanto incluímos uma ilustração, na Figura \ref{fig:alg}, para
facilitar seu entendimento. A Figura \ref{fig:alg} apresenta uma ilustração da
execução do algoritmo na qual foi omitido o procedimento de filtragem dos
símbolos com baixas freqüências para maior clareza, pois a intenção é passar a
idéia principal. Os detalhes e minúcias podem ser verificados na listagem do
algoritmo e na sua descrição.

\begin{figure}[h]
  \caption{Ilustração da execução do algoritmo.}
  \label{fig:alg}
  \centering
    \includegraphics[scale=0.48]{alg-pb.jpg}
\end{figure}


\begin{tiny}
\begin{algorithm}[H]
\caption{Busca regiões da TPS com alfabetos diferentes}
\label{alg:searchreg}
\textbf{Entrada:} $tagPathSequence$ - a $TPS$ de uma determinada página web \\
\textbf{Saída:} a região principal da $TPS$, armazenada em $tagPathSequence$
\end{algorithm}
\begin{algorithmic}[1]
\Procedure{searchRegion}{$tagPathSequence[1..n]$ by reference}
\State $alphabet \leftarrow \emptyset$
\State $t \leftarrow 0$
\For {$i \leftarrow 1..n$}
\State $symbol \leftarrow tagPathSequence[i]$
\If {$symbol \ni alphabet$}
\State $alphabet \leftarrow alphabet \cup \{symbol\}$
\State $symbolCount[symbol] \leftarrow 0$
\EndIf
\State $symbolCount[symbol] \leftarrow symbolCount[symbol]+1$
\EndFor
\State $thresholds \leftarrow Ordered Set Of Frequencies(symbolCount)$
\State $regionFound \leftarrow false$
\While {not $regionFound$}
\State $t \leftarrow t + 1$
\State $currentAlphabet \leftarrow filterAlphabet(alphabet,symbolCount,thresholds[t])$
\If {$currentAlphabet.size < 2$}
\State $break$
\EndIf
\State $currentSymbolCount \leftarrow symbolCount$
\State $regionAlphabet \leftarrow \emptyset$
\For {$i \leftarrow 1..n$}
\State $symbol \leftarrow tagPathSequence[i]$
\If {$symbol \in currentAlphabet$}
\State $regionAlphabet \leftarrow regionAlphabet \cup \{symbol\}$
\State $currentSymbolCount[symbol] \leftarrow currentSymbolCount[symbol]-1$
\If {$currentSymbolCount[symbol] = 0$}
\State $currentAlphabet \leftarrow currentAlphabet - \{symbol\}$
\If {$currentAlphabet \cap regionAlphabet = \emptyset$}
\If {$currentAlphabet \neq \emptyset$ and $(n-2*i)/n > 0.20$}
\State $regionFound \leftarrow true$
\EndIf
\State $break$
\EndIf
\EndIf
\EndIf
\EndFor
\EndWhile
\If {$regionFound$}
\If {$i < n/2$}
\State $tagPathSequence \leftarrow tagPathSequence[i+1..n]$
\Else
\State $tagPathSequence \leftarrow tagPathSequence[1..i]$
\EndIf
\State $searchRegion(tagPathSequence)$
\EndIf
\EndProcedure
\end{algorithmic}
%\end{algorithm}
\end{tiny}

O procedimeto $searchRegion()$ no Algoritmo \ref{alg:searchreg}
calcula o alfabeto da TPS e as freqüências correspondentes de cada símbolo nas
Linhas $4$ a $11$; na Linha $12$, os limiares de freqüência, da Definição
\ref{def:ft}, são calculados; nas Linhas $14$ a $38$ é realizada a busca por uma
posição na seqüência onde é possível dividí-la (i.e. onde existe uma região); na
Linha $15$ os limiares de freqüência são iterados; na Linha $16$ o alfabeto da
TPS, da Definição \ref{def:alpha}, é filtrado, como descrito no Capítulo
\ref{ch:probform}; na Linha $22$ a TPS é iterada; na Linha $25$ o alfabeto da região
é calculado e; nas Linhas $27$ a $35$ é verificado se existe uma intersecção
entre os alfabetos das duas partes da TPS (uma intersecção vazia indica que uma
possível região foi encontrada, como na Definição \ref{def:region}). A região
encontrada só é considerada válida se for ao menos 20\% maior que o restante da
seqüência, caso contrário o filtro de freqüência é iterado e a busca prossegue.
Este percentual é, na verdade, um parâmetro com o propósito de evitar relatar
uma região sob condições ambíguas (nos experimentos foi utilizado o valor de
20\%); finalmente nas Linhas $39$ a $46$ a TPS é dividida se uma região foi
encontrada, chamando o procedimento $searchRegion()$ recursivamente na Linha
$45$.

\section{Algoritmo filter()}

\begin{algorithm}[H]
\caption{Filters out symbols with lower frequencies from the alphabet}
\label{alg:filteralpha}
\textbf{Input:} $\Sigma$ - the alphabet ot be filtered \\
\textbf{Input:} $symCount$ - the tag path frequency set ($FS$) of $\Sigma$ \\ 
\textbf{Input:} $threshold$ - a frequency threshold \\
\textbf{Output:} a filtered alphabet
\begin{algorithmic}[1]
\Procedure{filter}{$\Sigma$, symCount, threshold}
\State $filtered\Sigma \leftarrow \emptyset$
\For {$i \leftarrow 1..n$}
\If {$symCount[\Sigma[i]] \geq threshold$}
\State $filtered\Sigma \leftarrow filtered\Sigma \cup \{\Sigma[i]\}$
\EndIf
\EndFor
\State return $filtered\Sigma$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $filter()$ no Algoritmo \ref{alg:filteralpha}
remove do alfabeto $\Sigma$, todo símbolo com frequencia inferior a $threshold$.
Nas Linhas $3$ to $7$ apenas os símbolos com frequencia maior ou igual a
$threshold$ são inseridos no conjunto resultante. O resultado de $filter()$ é 
utilizado no Algoritmo \ref{alg:searchreg}, Linha $24$, onde apenas os
símbolos em $filtered$ são considerados durante a busca pela região principal.

\section{Algoritmo pruneDOMTree()}

\begin{algorithm}[H]
\caption{Prune from the DOM tree the nodes that are not in sequence}
\label{alg:prune}
\textbf{Input:} $node$ - a DOM tree node, initially the root \\
\textbf{Input:} $seq$ - the TPS that has to remain in the DOM tree \\
\textbf{Output:} the DOM tree pointed by $node$ pruned
\begin{algorithmic}[1]
\Procedure{pruneDOMTree}{$node$ by ref.,$seq$}
\For {each child of node}
\If {$pruneDOMTree(child,seq) = true$}
\State remove child from $node$
\EndIf
\EndFor
\If {$node \ni seq$ and $node.childCount = 0$}
\State return $true$
\EndIf
\State return $false$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $pruneDOMTree()$ no Algoritmo \ref{alg:prune}, percorre a árvore
DOM, em produndidade, removendo os nós que não pertencem a $sequence$. Na
Linha $3$ a árvore DOM é percorrida; nas Linhas $7$ a $9$ é decidido se o nó
$node$ deve ser removido ou não.

Um nó é removido da árvore, apenas se ele não está em $sequence$ \textbf{e} não
tem nenhum filho. Desta forma é possível manter a estrutura da árvore
remanescente intacta, evitando afetar, negativamente, a etapa subsequente de
extração estruturada.

\section{Complexidade do Algoritmo}

Com relação à complexidade do algoritmo, observando as Linhas $14$ e $22$ do
procedimento $searchRegion()$, pode-se ver que o laço de repetição da Linha $14$
itera os limiares de frequencia até encontrar uma região e na Linha $22$ a TPS, 
filtrada em uma determina frequencia, é iterada, também, até encontrar uma região e,
se encontrada, a região é processada recursivamente.

No pior caso, quando a intersecção dos alfabetos só é vazia na última posição da TPS, 
a complexidade é, no máximo, $O(n^2 f)$, onde $n$ é o tamanho da TPS e $f$ é o tamanho 
do conjunto $thresholds$. Na prática, o tamanho do conjunto $thresholds$ e bem menor 
do que o tamanho da TPS, então é possível afirmar que a complexidade de tempo se aproxima
de $O(n^2)$ como mostrado na Equação \ref{eq:w}.
  \begin{equation}\label{eq:w}
  T(n) = T(n - 1) + \Theta(n) \Longrightarrow \sum_{i=1}^{n}
  i = \frac{n(n+1)}{2} = O(n^2)
  \end{equation} 

Na média, se a TPS é dividida na metade, a complexidade é $O(n)$, como mostrado na Equação \ref{eq:a}. 
  \begin{equation}\label{eq:a}
  T(n) = T(n/2) + \Theta(n) \Longrightarrow \sum_{i=1}^{log_{2}^{n}}
  \frac{n}{2^i} = n - 1 = O(n)
  \end{equation} 

No melhor caso, a TPS é dividida na primeira posição, resultando em uma complexidade de $O(n)$ como
mostrado na Equação \ref{eq:b}.
  \begin{equation}\label{eq:b}
  T(n) = T(n - 1) + \Theta(1) \Longrightarrow \sum_{i=1}^{n} 1 = n = O(n)
  \end{equation} 


Em situações reais, como as que ocorreram na avaliação empírica dos resultados, as sequencias são divididas 
aproximadamente quatro ou cinco vezes até que não seja mais possível dividí-las. Portanto, para situações reais, 
é possível afirmar que o algoritmo possui complexidade de tempo $O(in)$, onde $n$ é o tamanho da TPS e $i$ é 
o número de vezes que a sequencia é divdida, o qual podemos considerar como sendo uma constante de valor irrisório 
e, sendo assim, afirmar que a complexidade de tempo é $O(n)$.

\chapter{Método}\label{ch:met}

Para avaliar os resultados da técnica proposta, será utilizado o algoritmo MDR
(\cite{MDR03}), uma conhecida técnica de extração estruturada. O resultado da
extração pura (apenas MDR) será considerado como $baseline$ para comparação
com o resultado da extração combinada com a técnica proposta de remoção
de ruído.

A avaliação dos resultados é constituída das seguintes etapas:
\begin{itemize}
  \item escolha de uma técnica de extração automática de dados estruturados;
  \item definição de um conjunto de páginas para teste;
  \item aplicação da técnica de extração no conjunto de teste, documentando os
  resultados obtidos como \textit{target} (registro de interesse) ou
  \textit{noise} (ruído);
  \item nova aplicação da técnica de extração no mesmo conjunto de teste,
  mas desta vez filtrando-o antes, utilizando a técnica proposta neste trabalho,
  documentando os resultados da mesma maneira;
  \item comparação de ambos os resultados e medição do aumento/redução da
  precisão (remoção de ruído).
\end{itemize}

Para primeira etapa foi escolhido o MDR para realizar a extração, pois esta
técnica possui diversas implementações disponíveis, vários trabalhos publicados
a respeito, funciona em uma única página e os resultados da extração são
razoáveis para avaliar a técnica proposta neste trabalho. Outras técnicas
poderiam ser utilizadas, se estivessem disponíveis, desde que não necessitassem
de várias páginas HTML como entrada (e.g. $RoadRunner$ \cite{RRunner01}).


\chapter{Resultados Experimentais}\label{ch:results}

Neste capítulo são descritos e discutidos os resultados dos experimentos e sua realização.
Para obter os resultados apresentados na Seção \ref{sec:res}, o algoritmo proposto foi implementado
e testado em vários $web$ $sites$ comerciais e institucionais. Na Seção \ref{sec:clar} um destes resultados 
é detalhado, para servir de exemplo, para esclarecer como eles foram compilados na Tabela \ref{table:results}.

\section{Detalhamento dos Experimentos}\label{sec:clar}

Os resultados da extração obtidos utilizando apenas o MDR, foram considerados como $baseline$ a ser comparado com os resultados
obtidos através do uso combinado da técnica proposta com o MDR, como ilustrado na Figura \ref{fig:method}.
   
Quando ambas abordagens (MDR e filtragem TPS + MDR) são aplicadas a uma página de resultados do $web$ $site$ ``YouTube'' (\#38), os seguintes resultados são obtidos:

\begin{itemize}
\item{}página $web$ 'crua' (i.e. página original, sem filtragem da TPS)
\begin{itemize}
\item{}árvore DOM processada: $1424$ nós;
\item{}resultados do MDR: $82$ registros no total ( $62$ ruído / $20$ alvo );
\end{itemize}

\item{}página $web$ filtrada (i.e. página $web$ após aplicação do filtro da TPS)
\begin{itemize}
\item{}árvore DOM processada: $674$ nós, tamanho $47,33\%$ da página original, redução de $(-52,67\%)$
\item{}resultados do MDR: $20$ registros no total ( $0$ ruído / $20$ alvo ), ruído removido $100\%$
\end{itemize}
\end{itemize}

Neste resultado observa-se uma melhora na extração dos registros, assim como uma redução considerável 
do tamanho da árvore DOM que precisou ser processada. Um percentual de $52.67\%$ da árvore DOM foi podado 
sem que ocorresse a perda dos registros de interesse (alvo) durante o processo. Tudo o que foi podado da 
árvore, neste caso, era ruído. A Figura \ref{fig:ex2} ilustra a região principal da página e a 
Figura \ref{fig:tps} mostra a respectiva TPS e a regição principal detectada pelo algoritmo proposto.

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.26]{example2-gs.jpg}
  \caption{A page from the YouTube web site and the main content region delimited.}
  \label{fig:ex2}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.40]{tps.jpg}
  \caption{The TPS from a YouTube web page and the detected main content region.}
  \label{fig:tps}
\end{figure}


Sem aplicar o filtro na TPS, são obtidos $82$ registros no total e, como sabe-se de antemão que 
existem $20$ registros de interesse nesta página, considera-se que a quantidade de $62$ registros 
representam $100\%$ do ruído presente nesta página e que deve ser removido. Quando o filtro é utilizado, 
desta vez são obtidos apenas os $20$ registros de interesse na fase de extração, resultando em uma
precisão de $100\%$, o que significa que todo o ruído da página foi removido, neste caso. O percentual de 
ruído removido é calculado como
\begin{equation}\label{eq:noise}
NoiseRemoved=1-\frac{\#Rec_{totalTPS}-\#Rec_{targetTPS}}{\#Rec_{total}-\#Rec_{target}}
\end{equation}
Onde $\#Rec_{total}$ e $\#Rec_{target}$ são o total de registros e o total de registros de interesse, 
respectivamente, da página original, e $\#Rec_{totalTPS}$ e $\#Rec_{targetTPS}$ são o total de 
registros e o total de registro de interesse, respectivamente, da página $web$ filtrada. 

\section{Resultados}\label{sec:res}

Na Tabela \ref{table:results} são apresentados, nas primeiras três colunas, o
tamanho da árvore DOM processada pelo MDR (i.e. original) e a redução obtida
após remoção do ruído (i.e. podado). A coluna ``Conteúdo principal'' indica se o
processo de remoção de ruído preservou a região principal ou não. As próximas
quatro colunas são os resultados obtidos com o MDR sem e com remoção de ruído
respectivamente, exibindo o total geral de registros extraídos e o total de
registros de interesse ($target$) extraídos para ambas abordagens. A última
coluna mostra o percentual de ruído removido, calculado conforme a Equação
\ref{eq:noise}.

\begin{table}[h]
\caption{Resultados}
\label{table:results}
\centering
\begin{tiny}
\begin{tabular}{l r r r c r r r r r}

\hline\hline
\multicolumn{5}{c}{} & \multicolumn{4}{c}{MDR (\# registros)} &  (eq. \ref
{eq:noise})\\
& \multicolumn{3}{c}{Tam. DOM (\# nós)} & Cont. & \multicolumn{2}{c}{Orig.} &
\multicolumn{2}{c}{Podado} & Ruído\\
Site & Orig. & Podado & Red. & princ. & Tot & Tgt & Tot & Tgt & rem. \\
\hline
acm.org & 601 & 340 & -43.43\% & Yes & 61 & 10 & 16 & 10 & 88.24\% \\
amazon.com & 3309 & 1054 & -68.15\% & Yes & 368 & 15 & 27 & 15 & 96.60\% \\
americanas.com.br & 2660 & 710 & -73.31\% & Yes & 211 & 20 & 20 & 20 & 100.00\% \\
bestbuy.com & 3632 & 1425 & -60.77\% & Yes & 299 & 15 & 15 & 15 & 100.00\% \\
bondfaro.com.br & 3897 & 3069 & -21.25\% & Yes & 231 & 28 & 178 & 28 & 26.11\% \\
bradesco.com.br & 1913 & 1113 & -41.82\% & Yes & 164 & 10 & 93 & 10 & 46.10\% \\
buscape.com.br & 3608 & 3514 & -2.61\% & Yes & 279 & 24 & 266 & 24 & 5.10\% \\
ebay.com & 2623 & 1801 & -31.34\% & Yes & 162 & 50 & 50 & 50 & 100.00\% \\
elsevier.com & 906 & 160 & -82.34\% & Yes & 120 & 10 & 32 & 10 & 80.00\% \\
g1.com.br & 900 & 619 & -31.22\% & No & 225 & 10 & 202 & 0 & N/A \\
globo.com & 400 & 193 & -51.75\% & Yes & 80 & 10 & 20 & 10 & 85.71\% \\
google.com & 1421 & 981 & -30.96\% & Yes & 118 & 11 & 61 & 11 & 53.27\% \\
itau.com.br & 1111 & 410 & -63.10\% & No & 77 & 10 & 11 & 0 & N/A \\
magazineluiza.com.br & 3167 & 1115 & -64.79\% & Yes & 314 & 40 & 44 & 40 & 98.54\% \\
mercadolivre.com.br & 2401 & 1771 & -26.24\% & Yes & 136 & 50 & 52 & 50 & 97.67\% \\
reuters.com & 1202 & 480 & -60.07\% & Yes & 136 & 10 & 54 & 10 & 65.08\% \\
scopus.com & 4929 & 4688 & -4.89\% & Yes & 114 & 20 & 75 & 20 & 41.49\% \\
submarino.com.br & 2389 & 1268 & -46.92\% & Yes & 116 & 20 & 22 & 20 & 97.92\% \\
terra.com.br & 869 & 588 & -32.34\% & Yes & 122 & 50 & 76 & 50 & 63.89\% \\
valor.com.br & 514 & 126 & -75.49\% & No & 55 & 10 & 2 & 0 & N/A \\
webmotors.com.br & 2119 & 1361 & -35.77\% & Yes & 113 & 14 & 19 & 14 & 94.95\% \\
%wikipedia.com & 3224 & 1949 & -39.55\% & Yes & N/A & N/A & N/A & N/A & N/A \\
yahoo.com & 760 & 290 & -61.84\% & Yes & 67 & 10 & 10 & 10 & 100.00\% \\
youtube.com & 1424 & 674 & -52.67\% & Yes & 82 & 20 & 20 & 20 & 100.00\% \\
\hline
Média/Total &  &  & -46.22\% & 86.96\% &  &  &  &  & 77.03\% \\
\hline

\end{tabular}
\end{tiny}
\end{table}

Como se pode observar na Tabela \ref{table:results}, o total da coluna ``Conteúdo 
principal'' indica que o algoritmo funcionou em 84.21\% dos casos e removeu, com este
conjunto de teste, uma média de 89.02\% de todo o ruído presente nos dados,
como mostra a média da coluna ``Ruído rem.''. 

A média de 47.93\% de redução no tamanho da árvore DOM é um resultado interessante. Primeiro,
por que significa que quase metade da árvore DOM é ruído, na média. Segundo, por que
este valor ficou bem próximo do valor relatado, de forma independente, em \cite{Volume05} como
sendo o tamanho do ``template'' das páginas (em torno de 50\%).

Uma situação que chama a atenção na Tabela \ref{table:results} é o resultado obtido
para a página do site ``Build'' (\#8). Sem filtrar o ruído, o MDR retornou um total de $117$
registros, incluíndo os $12$ registros de interesse. Após aplicar o filtro de ruído, um total 
de $69$ registros são retornados, nenhum deles de interesse, todos ruído. Então, após a filtragem,
se a árvore retornada fosse, na realidade, o seu complemento, o resultado seria de $48$ registros
no total ($117-69=48$), incluindo os $12$ registros de interesse, o que seria um bom resultado
já que significa uma remoção de 65.71\% do ruído. O que se pode deduzir disto é que, a segmentação
da página ocorreu corretamente, apenas a identificação da região principal falhou, já que esta é 
relativamente pequena. 

\section{Discussão dos Resultados}

There are three main situations where the algorithm needs to be improved but, fortunately, 
only two of these can lead to loss of main content (content removal). In Table
\ref{table:results}, column ``Content Present'', these two situations account for 15.79\% 
of the cases, where the content region was removed in the filtering process.

\begin{enumerate}
  \item{\textbf{templates too homogeneous}}. These are pages with little difference between
  the regions. In this case, using this technique, there is
  not much to do. We simply do not have enough information to work with,
  since the entire page looks alike. We do not lose the target records, but the
  amount of noise removed is very low;
  \item{\textbf{templates too heterogeneous}}. These are pages where the main content is 
  subdivided in more than one region. In this case, the main region gets split
  over and over, and only the largest part passes through the filter (and it
  might be noise). We propose a way to work around this problem later in this Chapter;
  \item{\textbf{pages where the main content is smaller than the rest}}. That is a
  consequence of the second assumption we made in Chapter \ref{ch:probform}:
  ``the main region is denser/bigger than the rest''. In this case, noise will
  always be reported as content. The same proposal made for the former situation 
  can be used to deal with this one as well.
\end{enumerate}

In the case of heterogeneous templates, TPS filtering can still be used if we
make some slight modifications in the algorithm. One such case of heterogeneous
template are ``news sites'', where every record has a different structure, but
they are all records from the same domain (i.e. they belong to the same entity). 
In this specific situation, TPS segmentation could be used to split the page in 
several parts, and a semantic approach used to combine the regions, reporting the
main content as a set of regions instead of only one.

For situation described for the site ``Build'' (that happened for five other
sites we tested), when the content region is smaller than the rest, we
could apply a semantic technique to check whether or not the desired content is
present in the reported region, if not, report the complementary DOM tree  (i.e.
inverse the pruning) instead. The main algorithm would look like this:

\begin{algorithm}[H]
\caption{Filters out noise from a web page}
\label{alg:tpsfilter2}
\textbf{Input:} $inputFile$ - an HTML file \\
\textbf{Output:} pruned $inputFile$'s DOM tree
\begin{algorithmic}[1]
\Procedure{tagPathSequenceFilter}{$inputFile$}
\State $DOMTree \leftarrow parseHTML(inputFile)$
\State $convertToSeq(DOMTree.body,$`` ''$,TPS)$
\State $backupTPS \leftarrow TPS$
\State $searchRegion(TPS)$
\If {$TPS$ not $content$}
\State $TPS = backupTPS - TPS$   
\EndIf
\State $pruneDOMTree(DOMTree.body,TPS)$
\State return $DOMTree$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:tpsfilter2} is the same as Algorithm \ref{alg:tpsfilter}, except for Line $6$
where it checks if the main content is present in the reported region and, if not, we report
the complementary sequence instead (Line $7$), ensuring the presence of the main content. 

\chapter{Conclusão}\label{ch:conclusion}

As shown in the results, the method we have proposed for page segmentation and noise
removal is very effective for some commercial/institutional web sites.
In most cases, a very large amount of noise is removed without compromising the
main content region. Also, when applied in conjunction with MDR, we can see that
the extraction precision is greatly improved.

In the situations where our algorithm fails, other techniques have to/should/could
be combined depending on the targeted application. In extreme cases, where a
page has either too homogeneous structure (so we can not find a split anywhere
along the TPS) or too heterogeneous structure (then the main content itself gets
split in several parts), the main content block could be detected using,
perhaps, semantic approaches.


%techniques, like the one presented in \cite{Adaptive07}, that labels a sequence 
%of blocks as $content$ or $notContent$ could be used to implement 
%line $6$ of algorithm \ref{alg:tpsfilter2}.

The algorithm shows outstanding performance, as it works very well for the majority 
of large commercial web sites we have tested. It also outcomes the limitations 
(training requirements, HTML tag dependency, manual labeling, among others) of previous 
works in the area of data cleaning, page segmentation and noise removal as mentioned in 
Chapter \ref{sec:relatedwork}.

\bibliographystyle{ufscThesis/ufsc-alf}
\bibliography{refs}

\end{document}
