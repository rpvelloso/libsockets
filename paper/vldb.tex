%%%%% FALTA: abstract e passo a passo dos algoritmos, corrigir algumas figuras
%%%%% FALTA: abstract e passo a passo dos algoritmos, corrigir algumas figuras
%%%%% FALTA: abstract e passo a passo dos algoritmos, corrigir algumas figuras

\documentclass{vldb}
\usepackage[utf8]{inputenc}
\usepackage{graphicx, caption}
\usepackage{algorithmicx}
\usepackage[Algorithm,ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{url}
\usepackage{balance}

\title{Extracting Records from the Web Using a Signal Processing Approach}
\numberofauthors{2}
\author{
\alignauthor
Roberto Panerai Velloso\\
       \affaddr{Universidade Federal de Santa Catarina}\\
       \affaddr{Florianópolis (SC), Brazil}\\
       \email{rvelloso@gmail.com}
% 2nd. author
\alignauthor
Carina F. Dorneles\\
       \affaddr{Universidade Federal de Santa Catarina}\\
       \affaddr{Florianópolis (SC), Brazil}\\
       \email{dorneles@inf.ufsc.br}
}
\date{9 November 2016}

\begin{document}

\newdef{definition}{Definition}

\maketitle

\section{Introduction}

The extraction of structured information from the web is, undeniably, both
important and difficult. It is an important task due to the ever growing amount
of information published and readily available on the web and, also, because
structured information can be used to enrich, and allow, a number of
applications (e.g., price comparison, semantic keyword search, metasearch, etc).
It is a difficult task because the data is laid out for human consumption and
published in a variety of formats and templates, as observed in
\cite{structured2011}. Moreover, the Beckman report on database
research\cite{abadi2014beckman} mentions the diversity of data as a key
challenge in this research field.

Identifying the underlying content structure in a web document was attempted
before. The existing approaches range from solutions to specific instances of
the problem (e.g., extracting only data formatted with
\texttt{<table>}\cite{webtables2008}) to more broader attempts that search for
patterns in a web document\cite{MDR03}.

Here we outline a novel approach for the problem of structured extraction (or
record extraction) from web pages, using an alternative representation of the
DOM tree and some signal processing techniques. Our approach first converts the
DOM tree to a sequence representation; segments the sequence into regions;
identifies the regions with structured content; locates the records boundaries
within the structured regions and; aligns the records into a table.

The main contributions presented here are:
\begin{itemize}
    \item a novel insight on how the structure of a web document can be seen: as
    a periodic signal. To the best of our knowledge there is nothing alike in
    the literature and the results depict both its efficiency and effectiveness;
    \item a novel web page segmentation technique;
    \item a new and efficient way of checking if a given region of a web page is
    structured or not. Such a framework, that is independent of the extraction
    process (i.e., one doesn't needs to extract the entire document), is useful
    to avoid processing portions of the document that are unpromising be it for
    record extraction, indexing or any other end;
    \item a more efficient approach, when compared to the state of the art. That
    is due to the use of efficient signal processing algorithms. Most existing
    works rely mainly on dynamic programming (e.g., edit distance), clustering
    algorithms and/or full page rendering and hence are less efficient.
\end{itemize}

The rest of the paper is organized as follows. In Section \ref{sec:work} we
briefly review the work related to structured extraction; in Section
\ref{sec:defs} we presents some definitions required to better understand the
proposed approach for the problem; in Section \ref{sec:prop} we outline the
proposed approach; in Section \ref{sec:result} we discuss and compare the
achieved results and; in Section \ref{sec:con} we conclude and present some
ideas for future work.

\section{Related Work}\label{sec:work}

The task of extracting records from web pages has been approached from many
angles. Some authors proposed \textit{ad hoc} solutions while others tried more
broader approaches for the problem.

In \cite{webtables2008, listExtract2009, tablesMS2012, tegra2015, topklists2013}
the records (or tables, for that matter) are extracted from very specific
sources like data formatted with \texttt{<table>}, \texttt{<ul/ol/etc.>} or even
specific templates. Although these approaches may seem to be limited, at least
at a first glance, they actually yield sound results as documented in
\cite{relationalWeb2008, probase, probase2012, acsdb}. Such results are due to
the large amount and heterogeneity of data in the web. From that fact we get an
important premise when dealing with web scale data: recall can be traded off for
precision. And that is what those approaches are all about, they tackle very
specific situations (sacrificing recall) in a very specialized way (ensuring
high precision).

In the other hand, in \cite{RRunner01, exalg2003, vips03, viper05, MDR03,
depta05, NET05, TPC09, vide10, gstm2010, fivatech2010, cvts2012, SuffixTree12,
clustVX2014, datapath2015, autorm2015} more general approaches are proposed.
These proposals try to solve the problem in a broader sense, some using machine
learning techniques, others grounded on general observations about structured
content. Although each of these works represent advances in the research field,
they fail to provide readily usable results, unlike their \textit{ad hoc}
counterparts, but they serve as stepping stones for future works to catch on.

In \cite{TPC09, SuffixTree12, TPS2013} an alternative document representation is
used. Instead of the DOM tree the document is converted to a sequence of tag
paths. Such representation enables the use of algorithms targeted at strings and
sequences (e.g., suffix tree/array\cite{ukkonen1995, manber1993suffix},
FFT\cite{fft1965}, lempel-ziv compression\cite{ziv1977universal}, etc.), which
are vast and well studied \cite{gusfield1997algorithms}, unleashing new
possibilities in this research field.

Recent surveys\cite{survey2013, survey2014} about the subject of structured
extraction have reached the same conclusion about the state of the art: existing
works fail to definitively solve the problem, i.e., its still an open research
problem. Those surveys also agreed about the problem of comparing two different
approaches in this research area: there is no standard dataset/baseline and most
implementations are not publicly available.

\section{Background}\label{sec:defs}
In this section we define some concepts needed for the understanding of our approach.

\begin{definition}\textbf{(Tag Path)} is a string describing the path from the
root node of the DOM tree to every other node in the tree. For example:
``\texttt{html/body/table/tr/td/\#text}''. To better characterize each path we
include style definitions of every node in the path, like this:
``\texttt{html/body/table/tr class=tbrow/td class=tbcell/\#text bgcolor=red}''.
\end{definition}

\begin{definition}\textbf{(Tag Path Code - TPC)} is a numeric ascending code
assigned to every different tag path string encountered in the tree, in order of
appearance. If a given path has occurred in the past, it is assigned the same
code as before. The paths are built in depth first order. Figure
\ref{fig:tree2seq} shows an example of this definition.
\end{definition}

\begin{definition}\textbf{(Tag Path Sequence - TPS)} is a sequence of TPC in the
same order as they were built from the DOM tree. Figure \ref{fig:tree2seq} shows
the resulting TPS for an HTML snippet as  well as the set of TPC used for that
the sequence. Here, in this paper, we also refer to TPS as simply ``sequence'',
TPC as ``symbol'' and the set of TPC as ``alphabet''.
\end{definition}

\begin{definition}\textbf{(Coefficient of Variation - CV)} is a statistical
measure of data dispersion and is defined as the ratio between the standard
deviation and the mean as shown in Equation \ref{eq:cv}. We use this measure to
find out how well distributed a symbol is in a sequence. For example, consider
the sequence $tps(1..12)=[1,2,3,4,5,4,5,4,5,6,4,5]$ and the symbol $s=4$ of this
sequence; the symbol occurs at positions $[4,6,8,11]$ in the sequence and the
distances between adjacent occurrences are
$[6-4=\textbf{2},8-6=\textbf{2},11-8=\textbf{3}]$, so the CV for symbol ``$s$''
is equal to $\frac{\sigma([2, 2, 3])}{\mu([2, 2, 3])}=0.24744=24,744\%$. The more
regular the distance between adjacent occurrences, the lower the CV and vice
versa.

\begin{equation}\label{eq:cv}
    CV=\frac{\sigma}{\mu}
\end{equation}
\end{definition}

\begin{definition}\textbf{(Discrete Fourier Transform - DFT)} is a mathematical
transformation that decomposes a discrete sequence into its frequency components
(Equation \ref{eq:dft}). The result of the transformation is complex-valued, the
real part represents the amplitude and the imaginary part represents the phase.
To evaluate the contribution of a frequency component in the overall sequence we
use the power spectrum density (PSD), which is real-valued, as defined in
Equation \ref{eq:psd}.
Figure \ref{fig:fftreg} shows a discrete sequence (a) and its corresponding PSD
standardized to \textit{z score} (b).

\begin{equation}\label{eq:dft}
    X_k=\sum_{n=0}^{N-1}{x_n\cdot e^{-2\pi ikn/N}}    
\end{equation}

\begin{equation}\label{eq:psd}
    PSD=|X_k|^2
\end{equation}

\end{definition}

\begin{definition}\textbf{(Fast Fourier Transform - FFT)} is an efficient
implementation of the DFT that yields the same result. The naive implementation
of the DFT has $O(n^2)$ time complexity whereas the FFT has $O(nlogn)$ time
complexity. For many more details about DFT, FFT and discrete signal
processing in general we refer the reader to \cite{oppenheim1989discrete},
which covers the subject in great depth, since this is not the scope of this
paper.
\end{definition}

\begin{definition}\textbf{(Finite Difference)} is the difference between
adjacent values of a sequence as defined in Equation \ref{eq:diff}. We use the
finite difference to delimit cyclic regions of a sequence.

\begin{equation}\label{eq:diff}
    d[n] = x[n] - x[n - 1]
\end{equation}
\end{definition}

\section{Proposal}\label{sec:prop}
We present here, in this Section, the details of our proposal to tackle the
problem of extracting data records from web pages. The technique is subdivided
in four main steps, namely: sequence representation, region identification,
record identification and record alignment. Each step is detailed in Subsections
\ref{ss:seq}, \ref{ss:regi}, \ref{ss:reci} and \ref{ss:reca} respectively.

The whole process is based on the observation that the structured content of a
document is organized and formatted in a clear and consistent way. In other
words, the records are laid out so that it is obvious to the human reader that
they are related to each other and they refer to same subject (or entity). Such
an organization necessarily reflects on the structure of the DOM tree and its
style definition and, consequently, on its tag path sequence representation.

\subsection{Sequence Representation}\label{ss:seq}
Most of the works in this area uses the DOM tree to represent the documents
subjected to record extraction. We choose, instead, an alternative
representation: a sequence of tag paths. This representation allows us to see
the web page as a sequence instead of a tree, enabling us to make use of
algorithms targeted at sequences. Such representation was used before in
\cite{TPC09, SuffixTree12, TPS2013}.

The translation process from DOM tree representation to tag path sequence is
depicted in Figure \ref{fig:tree2seq} and detailed in Algorithm
\ref{alg:tree2seq}. It is basically a depth first search that assembles the path
from the root of the tree to every other node and assigns a code to each
distinct path found. To help distinguish different regions of the document,
during the region identification step, we've added style definitions to the tag
paths, similar to what was done in \cite{clustVX2014} with the difference that
here we use class and inline style attributes instead of rendering the entire
document, avoiding the computational complexity associated with such a task.

\begin{figure}[h]
  \centering
     \includegraphics[width=\columnwidth]{img/tree2seq.jpg}
  \caption{tag path sequence built from HTML code snippet.}
  \label{fig:tree2seq}
\end{figure}

\begin{algorithm}[H]
\caption{Builds a tag path sequence from a DOM tree}
\label{alg:tree2seq}
\begin{algorithmic}[1]
\Procedure{treeToSequence}{node,tp,tps by ref.}
\State $tp \leftarrow concat(tp,$``/''$,node.$tag$,node.style)$
\If {$tp \ni tagPathMap$}
\State $tagPathMap \leftarrow tagPathMap \cup \{tp\}$ 
\State $tagPathMap[tp].code \leftarrow tagPathMap.size$ 
\EndIf 
\State $tps \leftarrow concat(tps,tagPathMap[tp].code);$
\For {each $child$ of $node$}
\State $treeToSequence(child,tp,tps)$ 
\EndFor 
\EndProcedure
\end{algorithmic}
\end{algorithm}



\subsection{Region Identification}\label{ss:regi}

Before extracting the records itself we first isolate the regions that contain
structured data. It is simpler and easier to extract records from a delimited
region that is known to contain structured data than it is to do so from the
entire document. The reason for this is that structured regions are cyclic,
i.e., since the records have similar structure and are contiguous then its tag
path sequence will display a cyclic behaviour as illustrate in Figure
\ref{fig:stru}.

\begin{figure*}[h]
 \captionsetup{width=\textwidth}
  \centering
     \includegraphics[width=\textwidth]{img/main-reg.jpg}
  \caption{How structured content appears in the sequence (structured regions encircled).}
  \label{fig:stru}
\end{figure*}

We can see in Figure \ref{fig:stru} that whenever there is structured content in
the document, be it the main content, menus, or anything else, the corresponding
interval in the sequence displays a cyclic behaviour. In other words, the
sequence stabilizes during the structured portions. That happens because the
records look alike and so their paths and styles repeat themselves. We took
advantage of this observation to devise Algorithm \ref{alg:idreg} that isolates
the structured segments of the sequence.

\begin{algorithm}
\caption{Identifies structured regions in a document}
\label{alg:idreg}
\textbf{Input:} tag path sequence \\
\textbf{Output:} a set of structured regions

\begin{algorithmic}[1]

\Function{IdRegions}{tps} \Comment{Main Function}
\State $tpsContour \leftarrow \Call{contour}{tps}$
\State $regions \leftarrow \Call{segment}{tpsContour}$
\State $regions \leftarrow \Call{mergeRegions}{regions}$
\State $regions \leftarrow \Call{filterRegions}{regions}$
\State \Return $regions$
\EndFunction

%\\\hrulefill

\Function{contour}{tps} \Comment{Subroutine}
\State $maxHeight \leftarrow 0$ 
\For {$i \leftarrow 1..length(tps)$}
\If {$tps[i]>maxHeight$}
\State $maxHeight \leftarrow tps[i]$ 
\EndIf 
\State $contour[i] \leftarrow maxHeight$ 
\EndFor 
\State \Return $contour$ 
\EndFunction

%\\\hrulefill

\Function{segment}{contour} \Comment{Subroutine}
\State $diff \leftarrow firstDifference(contour)$
\State $diff \leftarrow concat(diff, 1)$
\State $regions \leftarrow \emptyset$
\State $start \leftarrow 1$
\State $end \leftarrow 1$
\For{$i \leftarrow 1..length(diff)$}
\If {$diff[i] \neq 0$}
\If {$start \neq end$}
\State $regions \leftarrow regions \cup [start, end)$
\EndIf
\State $start \leftarrow i + 1$
\State $end \leftarrow i + 1$
\Else
\State $end \leftarrow end + 1$
\EndIf
\EndFor
\State \Return $regions$
\EndFunction

%\\\hrulefill

\Function{mergeRegions}{regions} \Comment{Subroutine}
\State $merged[1] \leftarrow regions[1]$ 
\State $j \leftarrow 1$ 
\For {$i \leftarrow 2..regions.count()$}
\State $\Sigma_{prev} \leftarrow alphabet(merged[j])$ 
\State $\Sigma_{curr} \leftarrow alphabet(regions[i])$
\If {$\Sigma_{prev}\cap\Sigma_{curr} \neq \emptyset$}
\State $merged[j] \leftarrow concat(merged[j], regions[i])$
\Else 
\State $j \leftarrow j + 1$ 
\State $merged[j] \leftarrow regions[i]$ 
\EndIf 
\EndFor 
\State 
\Return $merged$ 
\EndFunction

%\\\hrulefill

\Function{filterRegions}{regions} \Comment{Subroutine}
\For{each region in regions}
\State $angCoef \leftarrow linearRegression(region)$
\If{$|angCoef| > threshold$}
\State $regions \leftarrow regions - \{region\}$
\EndIf
\EndFor
\State \Return $regions$
\EndFunction

\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:idreg} computes the contour of the input sequence and its
finite difference and then uses this to segment the document in several regions.
The idea behind this procedure is that the finite difference will be zero
whenever the sequence stabilizes, i.e., its contour remains constant as shown in
Figure \ref{fig:contour}.

\begin{figure}[h]
  \centering
     \includegraphics[trim={2.5cm 7.5cm 1cm 6.5cm}, width=\linewidth
     ]{img/contour.pdf}
  \caption{a) Contour of a TPS; b) Finite difference of contour.}
  \label{fig:contour}
\end{figure}

The resulting regions are then merged together according to their alphabets
(i.e., set of symbols that form the sequence). If two adjacent regions share a
common alphabet, they are merged in a same region, if their alphabet is disjoint
(i.e., they share no common symbols) they are kept apart.

At last, to ensure that the identified regions contain, in fact, structured
content, they are filtered out according to their angular coefficient. This
filtering step is necessary because when using the contour and its difference to
segment the sequence, it is possible that some of the identified regions are
spurious. If a region is indeed cyclic, its linear regression will yield a small
angular coefficient. If it is not cyclic, then it is either increasing or
decreasing and in both cases the linear regression will yield a much greater
angular coefficient.

The computational complexity of functions $contour$ and $segment$ are linear in
the size of the input sequence. For functions $mergeRegions$ and $filterRegions$
the complexity is linear in the number of regions detected, which in turn have a
worst case proportional to the size of the sequence, so the overall time
complexity for function $idRegions$, in the worst case, is $O(n)$ where $n$ is
the size of the sequence.

\subsection{Record Identification}\label{ss:reci}

For each region identified in the previous step we now try to extract records
out of it and rank those regions according to their content likelihood. To do
so, we analyse the tag paths that appear in the sequence against the region's
spectrum. The spectrum is calculated using the fast Fourier
transform\cite{fft1965} (FFT for short). The Fourier transform factors the
sequence into its frequency components, so cyclic sequences will exhibit higher
peaks in frequencies that correspond to the main period of the sequence as we
can see in Figure \ref{fig:fftreg} (the main content region of Figure
\ref{fig:stru}).

\begin{figure}[h]
  \centering
     \includegraphics[trim={2.5cm 7.5cm 1cm 6.5cm}, width=\linewidth
     ]{img/fftreg.pdf}
  \caption{a) The cyclic behaviour of a structured region; b) The PSD of a
  structured region.}
  \label{fig:fftreg}
\end{figure}

For every tag path code, in increasing order, we check how well distributed it
is along the sequence using the coefficient of variation (CV for short) of the
distance between it's occurrences, e.g., if a given tag path occurs at positions
\{10, 20, 30\}, the distances between consecutive occurrences are \{$20-10 =
\textbf{10}$, $30-20 = \textbf{10}$\} and the CV is equal to 0.0\% since all
distances are the same. In order to determine if a tag path is well distributed
or not a threshold must be set. Since the regions are cyclic, the number of
different tag paths that form the region's sequence is much smaller when
compared to the size of the sequence, due to inherent repetitions, and so it is
not prohibitive to check the CV of every tag path.

Cross validating TPC variation against the TPS spectrum is more robust than
doing the inverse (i.e., selecting the highest peak in the spectrum and then
searching for a corresponding TPC). That happens because the signal may be noisy
and the spectrum may contain noisy artifacts and errors due to windowing (e.g.,
spectral leakage) and frequency resolution limitation, and so it is not
guarantee that the highest peak will, in fact, correspond to the correct period.
Figure \ref{fig:fftleak} shows an example of this situation where the highest
peak in the spectrum is not the correct period, but nonetheless there is a
significant peak that corresponds to the correct period.

\begin{figure}[h]
  \centering
     \includegraphics[trim={2.5cm 7.5cm 1cm 6.5cm}, width=\linewidth
     ]{img/fftleak.pdf}
  \caption{An example when the maximum peak in the spectrum does not correspond to the correct period.}
  \label{fig:fftleak}
\end{figure}

For those tag paths that are well distributed along the sequence, we search the
corresponding range of the spectrum for a significant peak. If such a peak is
found we subdivide the region accordingly, extract the records and end the
process. To determine if a peak is significant we convert the spectrum to
\textit{z score} and set a threshold measured in number of standard deviations.

Algorithm \ref{alg:idrec} along with Figure \ref{fig:fftreg} shows in more
detail the process of identifying the records within a region.

\begin{algorithm}
\caption{Locates record boundaries in a region}
\label{alg:idrec}
\textbf{Input:} a region's TPS \\
\textbf{Output:} a set of record starting positions

\begin{algorithmic}[1]

\Function{IdRecords}{region}
\State $\Sigma \leftarrow sort(alphabet(region), ascending)$
\State $signal \leftarrow region-\mu(region)$ \Comment{remove DC}
\State $PSD \leftarrow abs(FFT(signal))^2$
\For{each $symbol$ in $\Sigma$}
\State $recordPositions \leftarrow find(region == symbol)$
\State $recSize \leftarrow diff(recordPositions)$
\State $recCount \leftarrow length(recordPositions)$
\State $CV \leftarrow \frac{\sigma(recSize)}{\mu(recSize)}$
\If{$CV < CVthreshold$}
\State $interval \leftarrow [recCount-2 .. recCount+2]$
\If{$PSD[interval] > numSD \cdot \sigma(PSD)$}
\State \Return $recordPositions$
\EndIf
\EndIf
\EndFor
\State \Return $\emptyset$
\EndFunction

\end{algorithmic}
\end{algorithm}

% colocar aqui algo e pict
% comentar linhas do algo e pict

\subsection{Record Alignment}\label{ss:reca}

Once the records are identified they need to be properly aligned so they can be
extracted in tabular form. The problem of optimal multiple sequence alignment is
known to be NP-Hard\cite{msanphard2006}, so we are forced to adopt approximate
solutions for this problem, one such solution, known as \textit{Center
Star}\cite{centerstar1993}, has a polynomial time complexity and guaranteed
error bound.

It is important to note that the previous step of record identification is
parametrized by the CV of records size and this parameter directly affects the
quality of the alignment in the following way: the lower the CV the better the
alignment at the expense of recall.

Another measure we take, in order to improve alignment quality, is to prune the
records before aligning them. Since we are not constrained here by the hierarchy
imposed by the DOM's tree structure, we can remove intermediate nodes that do
not contain information, easing the alignment process improving both it's
quality and running time (since the input size is diminished).

\section{Results}\label{sec:result}

In this section we discuss and compare the results of our research with other
approaches found in the literature that we considered to be more relevant. We
considered more relevant the proposals which are fully automatic, highly
effective and computationally efficient. 

The main difficulty we've encountered, when trying to compare our results with
the results of the others, was the implementations unavailability. For this
reason we were unable to make a deeper and independent comparative analysis.
Were implementations available one could compare specific situations where one
approach performs better than the other, otherwise we can only compare
summarized results. Although several works have criticized this aspect, few
provided implementations.

We have implemented our approach using a two tier software architecture where
the core is coded in C++ and the presentation layer (graph plotting, result
manipulation/display, logging, timing, etc.) in embedded Lua \cite{luahome}.
Our implementation is publicly available at \cite{} and we encourage and support
the scientific community to build on top of it, run independent experiments,
implement other techniques for the sake of comparison and so on.

We used two datasets for comparison. Dataset $\#1$ was proposed by
\cite{yamada2004testbed} and was used, among others, by \cite{TPC09,clustVX2014}
which are, in our opinion, the two more relevant works in the field and are
relatively recent. Dataset $\#2$ was assembled and used in \cite{clustVX2014}.

Table \ref{table:compare1} compares precision, recall and F-Measure of our
approach with three other state-of-the-art techniques, using dataset $\#1$:
MDR\cite{MDR03}, TPC\cite{TPC09} and ClustVX\cite{clustVX2014}. Our approach performs better than
MDR and TPC and slightly worst than ClustVX, but in the other hand our technique
doesn't need to render the entire page in order to extract the records. As a
matter of fact, we just parse the HTML and don't even process CSS definitions
(our approach just cares about \texttt{class} attributes).
We've considered MDR to be of relevance here because it is one of the few
approaches with available implementation and probably the first to achieve
acceptable results (both in computational complexity and effectiveness).

Table \ref{table:compare2} compares only against ClustVX, using dataset $\#2$.
Again we've performed slightly worst, but close to a perfect result. This
dataset is more up to date, than dataset $\#1$, with respect to document
template and web programming practices, but it contains only a small number of
documents so it is not an extensive, large scale, comparison.

\begin{table}[h]
\centering
\caption{Results comparison using the dataset $\#1$.}
\label{table:compare1}
\begin{tabular}
{|c| c| c| c|}\hline
	& Precision	& Recall	& F-Score\\ \hline
MDR\cite{MDR03} &	59,80\%	& 61,80\%	& 60,78\%\\ \hline
TPC\cite{TPC09}	& 90,40\%	& 93,10\%	& 91,73\%\\ \hline
ClustVX\cite{clustVX2014} &	99.81\% & 99.52\% & 99.66\%\\ \hline
Ours &	92,02\%	& 94,11\%	& 93,05\% \\ \hline
\end{tabular}
%}
\end{table}

\begin{table}[h]
\centering
\caption{Results comparison using dataset $\#2$.}
\label{table:compare2}
\begin{tabular}
{|c| c| c| c|}\hline
	& Precision	& Recall	& F-Score\\ \hline
ClustVX\cite{clustVX2014} &	100.00\% & 99.50\% & 99.80\%\\ \hline
Ours &	100.00\% & 95.45\% & 97.67\% \\ \hline
\end{tabular}
%}
\end{table}

Finally we present in Figure \ref{fig:runtime} the running time behaviour of our
implementation. We can see that the graph suggests a linear increase,
proportional to the document size, in the running time to process a document
end-to-end.

\begin{figure}[h]
  \centering
     \includegraphics[trim={2.5cm 7.5cm 1cm 6.5cm}, width=\linewidth
     ]{img/runtime.pdf}
  \caption{Document size vs running time.}
  \label{fig:runtime}
\end{figure}


\section{Conclusion and Future Work}\label{sec:con}

We have shown here a novel approach for the problem of extracting records from
semi-structured web pages, based on a new insight: seeing the structure of the
document as a cyclic signal. Our results are equivalent to the state-of-the-art,
but we claim we've achieved it with less processing since no document rendering
is required.

Although the high efficiency and effectiveness of our technique are stated by
the results in Section \ref{sec:result}, this is not a perfect approach. We've
assumed the records in a web document are contiguous, consistently
formatted/structured and with approximate size. Although these assumptions are
plausible, they are not universal and where they do not hold our approach fails.
Nonetheless, when considering web scale data, this subset of all structured data
is of considerable size and value.

In our opinion, the major set backs in this area of research are the
unavailability of implementations and the absence of a common framework.
Without the implementation of a particular technique we can't analyse individual
results to gain insight about why it did or did not work in a particular case.
This kind of analysis speeds up research because it helps refine research
assumptions faster, using previous works. On the other hand, a common framework
for the implementation of extractors reduces the variables involved in a
comparison by standardizing the basic software infrastructure (e.g., same HTML
parser used by everyone, same rendering engine, datasets, etc.)
 
Our guidelines for future work are the following:
\begin{itemize}
  \item research alignment techniques: here we have used a general and
  supralinear (but still polynomial) sequence alignment algorithm, but we
  believe it is possible to devise a solution specifically for the problem of
  record alignment that is more effective and has a lower complexity bound;
  \item standardize datasets: the ideal testbed should consist of several
  randomly taken samples from a large scale search engine index with annotated
  ground-truth.
  Such a dataset enables the researcher to draw consistent conclusions from statistics (e.g.,
  percentage of documents with structured content, ``real'' precision and
  recall). We haven't found such a dataset in the literature;
  \item use Goertezel's \cite{goertzel1958algorithm} in place of FFT:
  Goertezel's algorithm computes a single coefficient in linear time whereas the
  FFT computes all coefficients in loglinear time. Our technique searches for a
  candidate frequency in a small range of the spectrum, there is no need to
  compute, up front, all coefficients so that may be an alternative to reduce
  the time complexity a little more;
  \item investigate the use of lempel-ziv\cite{ziv1977universal} (LZ) in the context of
  pattern detection: LZ is a compression algorithm targeted specifically at sequences,
  it works by looking for redundancies in the sequence, the redundancy can be
  seen as a repeating pattern (much like contiguous records). We believe it is
  possible to exploit this algorithm in this context.
  %\item develop the extraction framework further: continue the development of
  %the extraction framework, refine and refactor code, implement other
  % extraction techniques, etc.
\end{itemize}
 
\balance 

\bibliographystyle{abbrv}
\bibliography{refs}

\end{document}
