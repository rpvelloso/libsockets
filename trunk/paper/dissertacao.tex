\documentclass{ufscThesis}
\usepackage{graphicx}
\usepackage[labelsep=endash]{caption}
\usepackage{algorithmicx}
\usepackage[Algoritmo,ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage{listings}
\usepackage[table]{xcolor}

\newtheorem{definition}{Definição}
\renewcommand{\lstlistingname}{Listagem}

\titulo{Algoritmo não Supervisionado para Segmentação e Remoção de
Ruído de Páginas $web$ Utilizando $tag$ $paths$}
\autor{Roberto Panerai Velloso}
\data{01}{março}{2014}
\orientador[Orientadora]{Profa. Dra. Carina F. Dorneles}
\coordenador{Prof. Dr. Ronaldo dos Santos Mello}


\departamento{Departamento de Informática e Estatística}
\curso{Programa de Pós-Graduação em Ciência da Computação}

\numerodemembrosnabanca{5}
\orientadornabanca{sim}
\bancaMembroA{Prof. Presidente da banca}
\bancaMembroB{Prof. segundo membro}
\bancaMembroC{Prof. terceiro membro}

%\bancaMembroD{Prof. quarto membro}
%\bancaMembroE{Prof. quinto membro}
%\bancaMembroF{Prof. sexto membro}
%\bancaMembroG{Prof. setimo membro}

\dedicatoria{À minha família.}

\agradecimento{Agradeço o apoio dos familiares e a atenção dispensada pelos
professores do curso, em especial pela orientação que recebi para realização e
publicação deste trabalho.}

\epigrafe{ Está instaurada a dúvida.\\
A metódica dúvida epistemológica.\\
Neste mundo a terra não está no centro\\
nenhum saber é saber completo.\\
Seja bem-vinda era da razão.\\
Não há que se temer a revisão.\\
Nada que se diga ou que foi dito\\
merece estatuto de dogma irrestrito.\\
Cuidado com a verdade\\
que se pretende\\
maior que a realidade,\\
pois, os fatos são os fatos\\
e fluem diante de nós\\
que estupefatos\\
assistimos ao espetáculo.}{Galileu Galilei}

\textoResumo{Segmentação e remoção de ruído de páginas $web$ são etapas
essenciais no processo de extração de dados estruturados. Identificar a região
principal da página, eliminando o que não é importante (menus, anúncios, etc.),
pode melhorar significativamente o desempenho do processo de extração. Para essa
tarefa é proposto um novo algoritmo, totalmente automático, que utiliza uma
seqüência de $tag$ $paths$ (TPS) como representação da página $web$.
A TPS é composta por uma seqüência de símbolos ($string$), cada um representando
um $tag$ $path$ diferente. O algoritmo proposto procura por posições na
TPS onde é possível dividi-la em duas regiões de tal forma que seus alfabetos não se
intersectem, o que significa que as regiões têm conjuntos de $tag$ $paths$
completamente distintos e, portanto, são regiões diferentes da página. Os
resultados mostram que o algoritmo é muito efetivo em identificar o conteúdo
principal de vários $sites$, e melhora a precisão da extração, removendo
resultados irrelevantes.}

\palavrasChave{remoção ruído. segmentação página. extração estruturada. mineração dados $web$.}

\textAbstract{Web page segmentation and data cleaning are essential steps in
structured web data extraction. Identifying a web page main content region,
removing what is not important (menus, ads, etc.), can greatly improve the
performance of the extraction process. We propose, for this task, a novel and
fully automatic algorithm that uses a tag path sequence (TPS) representation of
the web page. The TPS consists of a sequence of symbols (string), each one
representing a different tag path.
The proposed technique searches for positions in the TPS where it is possible to
split it in two regions where each region's alphabet do not intersect, which
means that they have completely different sets of tag paths and, thus, are
different regions. The results show that the algorithm is very effective in
identifying the main content block of several major web sites, and improves the
precision of the extraction step by removing irrelevant results.
}

\keywords{noise removal. page segmentation. structured extraction. web mining}

\begin{document}
\rowcolors{2}{gray!25}{white}


\capa  
\folhaderosto[comficha]
\folhaaprovacao
\paginadedicatoria
\paginaagradecimento
\paginaepigrafe
\paginaresumo
\paginaabstract

\listadefiguras
\listadetabelas 
\listadeabreviaturas
%\listadesimbolos
\sumario

\chapter{Introdução}
%\section{Contextualização do problema}

Um passo fundamental da mineração de dados na $web$, inclusive em extração
estruturada, é a fase de limpeza que deve ocorrer antes da extração da
informação. Não se pode esperar bons resultados na fase de extração sem efetuar
a limpeza da página, removendo o ruído indesejado. Em \citeonline{Noisy03}, é
mencionado que apesar da importância desta tarefa, existem relativamente poucos
trabalhos nesta área e, de acordo com \citeonline{Editorial04}, o ruído
existente nas páginas pode prejudicar consideravelmente a mineração de dados na
$web$.

A extração de dados estruturados de páginas $web$, na maioria das propostas
existentes, consiste na identificação e localização de padrões estruturais no
documento HTML, os quais podem representar registros.
Em \citeonline{MDR03}, é dada uma definição para o termo ``registro'', neste
contexto. O problema é que, em geral, apenas os registros da região principal da
página são de interesse, porém outras regiões do documento (como menus,
anúncios, etc.) freqüentemente contém padrões em sua estrutura, fazendo com que
sejam identificados como registros e, conseqüentemente, extraídos prejudicando a
precisão da extração dos dados. Daí surge a importância de se efetuar a limpeza
das páginas $web$ antes de se extrair seu conteúdo estruturado. A Figura
\ref{fig:exr} apresenta uma página típica de comércio eletrônico, onde os
registros de interesse são os produtos apresentados no corpo do documento e
não os menus (que também seguem um padrão estrutural) na barra lateral esquerda
e no cabeçalho da página.

 \begin{figure}[H]
  \centering
    \includegraphics[scale=0.22]{exemploruido.jpg}
  \caption{Exemplo de uma página de comércio eletrônico com os registros e
  ruído identificados.}
  \label{fig:exr}
\end{figure}


Atualmente, alguns trabalhos na área de remoção de ruído e segmentação são
direcionados para indexação e $clustering$ de páginas ($i.e.$ é assumido que a
região principal é textual/não estruturada) como em
\citeonline{SiteOriented11,Noisy03} e, devido às diferenças intrínsecas entre
conteúdo estruturado e não estruturado, estas propostas não podem ser utilizadas
para extração estruturada. As propostas existentes, que podem ser utilizadas
para este fim, na sua maioria ou dependem do domínio em que são aplicadas
\cite{reis2004automatic}, ou requerem definições \textit{a priori} de regras
estáticas e/ou bases de dados \cite{vips03}, ou são supervisionadas (não
automáticas) e necessitam de uma amostragem das páginas que serão objeto da
extração \cite{Graph08}, ou então dependem de aspectos específicos da linguagem
\abreviatura{HTML}{hypertext markup language}HTML para funcionar, como em
\citeonline{Entropy09}.

O presente trabalho descreve um algoritmo para segmentação e remoção de ruído de
páginas $web$ que utiliza, como representação do documento HTML, a seqüência de
$tag$ $paths$ da árvore DOM. Conforme a análise apresentada na Seção
\ref{sec:compl}, o algoritmo é computacionalmente eficiente. Trata-se, em sua
forma mais ampla, de um algoritmo de segmentação das regiões de um documento
HTML que é utilizado aqui no contexto de extração estruturada e que leva em
consideração o estilo (CSS) e a estrutura da página. As principais contribuições
são as seguintes:

\begin{itemize}
\item{Totalmente automática}: não necessita de treinamento nem intervenção
humana;
\item{Independente de domínio}: o único requisito é que a página possua
conteúdo estruturado, não importando a que domínio pertença;
\item{Independente da sintaxe HTML}: não existem regras associadas a $tags$
HTML específicas;
\item{Funciona em uma página}: é necessária apenas uma página como entrada, o
que constitui uma grande vantagem desta proposta, como discutido em
\citeonline{Editorial04}.
\end{itemize}

Durante a discussão dos resultados, são apresentadas e discutidas, também, as
situações em que o algoritmo não obteve sucesso (total ou parcial). São
apontadas as causas e sugeridas maneiras de se atacar as limitações
identificadas durante a realização dos experimentos.

Para avaliar o quão efetivo é o algoritmo proposto neste trabalho, foi comparado
o ruído de saída do \abreviatura{MDR}{mine data records}MDR \cite{MDR03}, uma
conhecida técnica de extração estruturada, contra o ruído de saída do MDR
combinado com o algoritmo proposto. O conjunto de testes consiste em uma coleção
de 51 páginas de $sites$ com conteúdo estruturado em diversos domínios como:
vídeos, buscas, comércio eletrônico de vários segmentos, notícias, periódicos
científicos, bancos, entre outros. O resultado obtido foi uma média de 88,86\%
de ruído removido das páginas do conjunto de teste.

Esta dissertação está organizada da seguinte maneira: %no Capítulo
% \ref{ch:obj} são apresentados os objetivos da pesquisa; no Capítulo
% \ref{ch:contr} são enunciadas as principais contribuições do trabalho;
no Capítulo \ref{ch:rev} é feita uma revisão dos trabalhos relacionados,
observando as limitações das propostas existentes para solução do problema; no
Capítulo \ref{ch:met} são enunciadas as definições necessárias ao entendimento
da proposta e são detalhados e ilustrados os algoritmos desenvolvidos a partir
da formulação do problema e das hipóteses; no Capítulo \ref{ch:results} o método de
avaliação dos resultados é detalhado e também são apresentados os resultados
obtidos e as limitações do algoritmo proposto e, finalmente; no Capítulo
\ref{ch:conclusion} uma breve conclusão e sugestões de trabalhos futuros.

%\section{Objetivos}\label{ch:obj}

%O objetivo desta pesquisa é contribuir para área de extração estruturada de
%dados da $web$, propondo uma nova técnica para segmentação e remoção de ruído
% de páginas que possibilite incrementar a precisão dos métodos atuais, com o mínimo
%de intervenção manual e o máximo de independência com relação a aspectos da
%linguagem HTML e ao domínio da informação.

%\section{Contribuições}\label{ch:contr}

%A contribuição desta pesquisa é uma nova técnica de segmentação e remoção de
%ruído de páginas $web$ com as seguintes características:
%\begin{itemize}
%\item{Técnica completamente automática}: não é necessário treinamento nem
%intervenção humana;
%\item{Independente de domínio}: é necessário apenas que a página contenha
%conteúdo estruturado, não importando a que domínio a informação pertence;
%\item{Independente da sintaxe HTML}: não são utilizadas regras associadas a
%$tags$ HTML;
%\item{Necessita de apenas uma página}: não requer várias páginas do mesmo
% $site$ para funcionar, o que é um grande vantagem, como discutido em
%\citeonline{Editorial04}.
%\end{itemize}

\chapter{Trabalhos relacionados}\label{ch:rev}

Existem muitos trabalhos propondo formas de segmentar páginas $web$ e
identificar o que é conteúdo e o que é ruído nelas. Elas podem ser agrupadas em
três categorias: as que utilizam o conteúdo textual da página e $tags$ HTML
(porém sem considerar a estrutura da árvore DOM), as que utilizam a árvore
\abreviatura{DOM}{document object model}DOM e as que utilizam informação visual
(o documento renderizado).

\section{Propostas baseadas no texto}\label{sec:txt}
Nos trabalhos detalhados nesta seção 
\cite{Densiometric08,Boilerplate10,CETR10} a
%\cite{Densiometric08,Boilerplate10,CETR10,NonTemplate13} a
segmentação das partes de uma página é realizada utilizando o conteúdo textual
do documento. O foco destes trabalhos, entretanto, não é em extração
estruturada, mas em indexação e $clustering$ de páginas. A maioria dos trabalhos
existentes sobre segmentação e remoção de ruído, que atuam sobre o texto, são
direcionados para este tipo de aplicação, cujo objetivo principal é identificar
o ``assunto'' da página.

\begin{itemize}

\item Em \citeonline{Densiometric08} a página é modelada como uma seqüência de
blocos de texto delimitados por $tags$ de abertura e fechamento (chamados de
$gaps$), os blocos de texto são analisados segundo sua densidade textual e os
blocos contíguos, com aproximadamente a mesma densidade, são agrupados para
formar blocos maiores e a região principal da página seria a de maior densidade
textual. A densidade de texto é calculada segundo a Fórmula \ref{eq:dt}, onde
$p(b_x)$ é a densidade do bloco $b_x$, $n_{tokens_x}$ é o número de palavras do
bloco e $n_{lines_x}$ é o número de linhas do bloco, sendo que cada linha tem
um tamanho arbitrário de 80 caracteres.

\begin{equation}\label{eq:dt}
p(b_x) = \frac{n_{tokens_x}}{n_{lines_x}}
\end{equation}

\item Em \citeonline{Boilerplate10}, a mesma medida de densidade textual
proposta em \citeonline{Densiometric08} (Fórmula \ref{eq:dt}) é utilizada, porém
uma série de outras características são agregadas ao problema como: o tamanho das
palavras, tamanho das sentenças, número de palavras e a densidade de $links$,
além de um conjunto de regras bem específicas para tratar textos e $tags$ do
documento. Estas outras informações agregadas são utilizadas para gerar uma
árvore de decisão utilizada para identificar o conteúdo principal do documento.
	
\item Em \citeonline{CETR10} é mencionada a desvantagem de se utilizar regras
vinculadas a $tags$ HTML específicas, o que pode invalidar uma abordagem no caso
de uma eventual mudança na sintaxe HTML ou nas práticas de desenvolvimento
atuais. Neste trabalho, em cada linha do código fonte HTML do documento, é
calculada a taxa de $tags$, que consiste na relação entre o número de caracteres
que não são $tag$ e o número de $tags$ existentes em uma determinada linha do
código fonte do documento. O histograma resultante é suavizado com uma curva
gaussiana e a região com o conteúdo principal (com taxa de $tags$ elevada) é
identificada por $clustering$ ou pela escolha de um limiar. No caso da
identificação por limiar, este tem que ser encontrado por experimentação.
\end{itemize}
		

\section{Propostas baseadas na árvore DOM}\label{sec:dom} 
Nos trabalhos detalhados nesta seção
\cite{Noisy03,Graph08,Entropy09,SiteOriented11,Entropy12,desc2013} a
segmentação é realizada utilizando a árvore DOM da página, portanto estas
técnicas levam em consideração a estrutura da página $web$ e, conseqüentemente,
podem ser aplicadas no contexto de extração estruturada.

\begin{itemize}

\item Em \citeonline{Noisy03} é construída uma árvore de estilos (com base
nas definições \abreviatura{CSS}{cascade style sheet}CSS) para um determinado
$site$, a partir de várias amostras de suas páginas, para determinar o que seria
ruído e o que seria conteúdo nas páginas deste $site$, especificamente. Após a
construção da árvore de estilos do $site$ o ruído é determinado com base na
seguinte observação feita pelos autores: os blocos de ruído possuem formatação e
conteúdo semelhantes, enquanto que o conteúdo é diverso e possui formatação diversa.

\item Em \citeonline{Graph08} o problema de segmentar uma página é modelado como
um problema de minimização de energia em cima de um grafo ponderado construído a
partir de amostras de páginas anotadas manualmente com relação à sua
segmentação. Os nós da árvore DOM são os nós do grafo e os pesos das arestas
representam o custo de se segmentar os dois nós, que estão interligados no
grafo, juntos ou separados. O custo é calculado a partir das características de
cada nó (formato, cor de fundo, fonte, tamanho, etc.) e o peso de cada
característica é determinado a partir do conjunto de treinamento. Apenas a
questão da segmentação é abordada, sem a preocupação em identificar quais partes
da página contém conteúdo relevante e quais contém ruído.

\item Em \citeonline{Entropy09}, uma árvore ``visual'' é gerada a partir da
árvore DOM, redefinindo a árvore original com o auxílio de um dicionário de
$tags$ pré-definido que indica quais $tags$ são relevantes na estrutura do
documento. Cada nó da árvore visual tem sua entropia calculada com relação ao
atributo $CLASS$. O trabalho propõe três metodologias distintas de avaliação da
entropia: entropia dos nós filhos (\textit{child entropy}), entropia do caminho
(\textit{path entropy}) e entropia do caminho parcial (\textit{partial path
entropy}). Após o cálculo da entropia, estes valores são analisados, comparados
a um limiar estipulado e cada bloco da página é, então, classificado como: CB
(\textit{container block}), PB (\textit{presentation block}), FB
(\textit{functional block}) ou IB (\textit{informative block} (conteúdo
relevante).

\item Em \citeonline{SiteOriented11} é adotada uma abordagem orientada a $site$,
assim como \citeonline{Noisy03}, onde um conjunto de árvores DOM de páginas de
um mesmo $site$ são alinhadas para identificar os segmentos de uma página a
partir das semelhanças entre elas. O resultado do alinhamento é, então,
refinado, de acordo com duas regras heurísticas (agrupamento de nós textuais
próximos e agrupamento dos nós filhos com o nó pai), e o resultado final
representa a subdivisão/organização/$template$ do $site$ analisado pelo algoritmo. Este
trabalho apenas segmenta a página, sem identificar quais blocos/segmentos seriam
ruído ou conteúdo. Apesar de necessitar de treinamento, este é realizado de
maneira não supervisionada.

\item Em \citeonline{Entropy12} é utilizada uma base de dados de termos
associados a ``papéis semânticos'' para encontrar, na árvore DOM, os nós folha
que desempenham estes papéis, atribuindo, a cada um, uma medida de ``entropia
semântica-estrutural'' de acordo como papel desempenhado por aquele nó folha.
A entropia dos demais nós é calculada recursivamente, a partir da entropia dos
nós descendentes. Quanto maior for esta medida de entropia, maior a
probabilidade de se tratar de uma região de interesse. A entropia é calculada
conforme a Fórmula \ref{eq:ese}, onde $n$ é um nó da árvore DOM e $p_i$ é a
proporção de nós folha, descendentes de $n$, que desempenham o papél $i$.

\begin{equation}\label{eq:ese}
H_n = \sum_{i=1}^m p_ilog(p_i)
\end{equation}


\item Em \citeonline{desc2013} de forma semelhante à \citeonline{Boilerplate10},
são utilizadas as densidades de texto e de $links$, porém, neste trabalho, o
documento é tratado como uma árvore DOM e não como uma seqüência de blocos. Além
destas informações (densidade e texto e $links$), são utilizados os dados dos
formulários da $web$ oculta para formar mais um parâmetro ($QueryScore$) de
treinamento para uma árvore de decisão gerada com o algoritmo C4.5
\cite{quinlan1993c4}. A árvore de decisão é, então, utilizada para identificar
quais nós da árvore DOM são ruído e quais são conteúdo relevante na resposta à
submissão de um formulário.

\end{itemize}
	
\section{Propostas baseadas em informação visual}\label{sec:vis}
Além das técnicas baseadas no texto e na árvore DOM da página, existem também
algumas que utilizam informação visual, como
\citeonline{vips03,viper05,BlockImp07,vide10}.
Em geral utilizam a árvore DOM em conjunto com as características visuais do
documento, as quais são obtidas por intermédio de um navegador $web$, utilizado
para renderizar a página e fornecer as informações necessárias ($e.g.$
coordenadas de tela, definições de estilo, etc.). Não existe um consenso, na
comunidade científica, com relação ao custo-benefício (complexidade
computacional $versus$ ganho na precisão) de se utilizar um renderizador para
esta finalidade. Em \citeonline{Densiometric08}, é mencionada a desvantagem
destas abordagens visuais, devido a grande complexidade computacional das
mesmas, pois precisam renderizar a página para obter as informações de $layout$.

\begin{itemize}

\item Em \citeonline{vips03} é utilizado um renderizador de um navegador $web$
para obter informação visual (principalmente coordenadas e formatação) de uma
página e, assim, construir uma estrutura de árvore que representa a hierarquia
dos blocos visuais do documento. Além da informação visual, também é utilizado
um extenso conjunto de regras que servem para dar tratamento diferenciado a
$tags$ HTML específicas com objetivo principal de identificar separadores
visuais. Neste trabalho, assim como em \citeonline{Graph08}, não há preocupação
em identificar quais blocos visuais da página contém conteúdo relevante e quais
contém ruído.

\item Em \citeonline{viper05} é proposta uma técnica de extração estruturada,
com identificação do conteúdo/ruído que utiliza o renderizador de um navegador
$web$ para corrigir (no caso de HTML mal formado) e processar o HTML e para
obter as informações visuais necessárias ao algoritmo.
A etapa de extração é uma adaptação/melhoramento do algoritmo MDR \cite{MDR03}
que incorpora o uso das informações visuais.
A etapa de identificação do conteúdo principal ocorre após a identificação dos
padrões, onde recebem um peso, inversamente proporcional à distância que se
encontram do centro da página. Outra melhoria proposta, relativa à identificação
de padrões dos registros, é o tratamento de repetições consecutivas
(\textit{tandem repeats}) na comparação da estrutura dos registros.
Esta abordagem identificada o ruído apenas após a etapa de extração, sem evitar
o custo computacional de procurar por padrões no ruído.

\item Em \citeonline{BlockImp07}, é utilizada a abordagem de \citeonline{vips03}
para segmentar a página em blocos. Um conjunto de páginas são analisadas e suas
estruturas são comparadas para verificar se são equivalentes. Páginas com
estruturas equivalentes são então analisadas com respeito às informações
estatísticas dos termos contidos em seus blocos (subdivisões) e um $score$ de
importância é atribuído a cada bloco. Apesar de necessitar de um conjunto de
páginas para treinamento, toda a análise do conjunto é realizada de forma não
supervisionada.

\item Em \citeonline{vide10} é proposta uma abordagem para extração estruturada
que, assim como no trabalho de \citeonline{BlockImp07}, se baseia em
\citeonline{vips03} para criar a representação visual do documento e, então,
utilizar as características visuais das páginas da $web$ oculta para localizar e
extrair os registros contidos no documento. As características visuais
utilizadas para identificação do conteúdo principal são:
posição (centralização horizontal), tamanho (conteúdo relevante é maior que
demais blocos), $layout$ (registros possuem arranjos semelhantes), semelhança
visual (registros possuem formatação semelhante).

\end{itemize}


\section{Representação TPS}
A representação da página $web$ utilizada nesta pesquisa (seqüência de $tag$
$paths$) foi anteriormente empregada em \citeonline{TPC09} e
\citeonline{SuffixTree12}, embora em ambos os casos para a extração e não para
segmentação e remoção de ruído. Estes dois trabalhos são citados aqui apenas
para ilustrar que, de acordo com seus resultados, esta representação, assim como
a árvore DOM, também é capaz de expor a estrutura da página $web$ e, portanto,
serve para os propósitos desta pesquisa.
A Figura \ref{fig:ex1} ilustra como o código HTML de uma página $web$ é
convertido para representação de $tag$ $paths$.

\begin{figure}[h]
  \centering
     \includegraphics[scale=0.51]{example1-pt.jpg}
  \caption{Exemplo de uma seqüência de $tag$ $paths$ construída a partir do código HTML.}
  \label{fig:ex1}
\end{figure}

\section{Análise Comparativa dos Trabalhos}\label{sec:tbcomp}
Na Tabela \ref{table:comp} é feito um comparativo entre as técnicas que foram
encontradas na literatura e a proposta apresentada, com objetivo de explicitar
as diferenças entre estes trabalhos. As diversas técnicas de remoção de ruído
são comparadas com relação às seguintes características:
\begin{itemize}
  \item Entrada do algoritmo: conteúdo textual da página, árvore DOM,
  informações visuais ou TPS;
  \item Necessita de apenas uma página como entrada ou de um
  conjunto de páginas;
  \item Necessita de treinamento e/ou supervisão;
  \item Necessita de informações pré-definidas;
  \item Independência da sintaxe HTML: técnicas que não tratam $tags$ HTML de
  forma específica, são consideradas independentes.
\end{itemize}

\begin{table}[h]
\centering
\begin{tiny}
\caption{Comparativo entre técnicas de segmentação de página}
\label{table:comp}
\begin{tabular}{p{0.1cm}|p{3cm} p{0.5cm} p{0.5cm} p{1cm} p{1cm} p{1cm}}
\hline\hline
\# 
&Técnica   
& Entrada
& Única página   
& Treinamento     
& Informação a priori 
& Independe HTML \\
\hline
1 & \citeonline{Densiometric08} & Texto & - & - & - & - \\
2 & \citeonline{Boilerplate10} & Texto & - & - & - & - \\
3 & \citeonline{CETR10} & Texto & - & - & - & - \\
%- & \citeonline{NonTemplate13} & Texto & - & - & - & - \\
4 & \citeonline{Noisy03} & DOM & Não & Sim & Não & Sim \\
%11 & \citeonline{Jointop07} & DOM & Não & Sim & Não & Sim \\
5 & \citeonline{Graph08} & DOM & Não & Sim & Não & Sim \\
6 & \citeonline{Entropy09} & DOM & Sim & Não & Sim & Não \\
7 & \citeonline{SiteOriented11} & DOM & Não & Sim & Não & Sim \\
8 & \citeonline{Entropy12} & DOM & Sim & Não & Sim & Sim \\
9 & \citeonline{desc2013} & DOM & Não & Sim & Não & Sim \\
10 & \citeonline{vips03} & Visual & Sim & Não & Sim & Não \\
11 & \citeonline{viper05} & Visual & Sim & Não & Não & Sim \\
12 & \citeonline{BlockImp07} & Visual & Não & Sim & Não & Não \\
13 & \citeonline{vide10} & Visual & Sim & Não & Sim & Não \\
\hline
14 & \citeonline{TPS2013} & TPS & Sim & Não & Não & Sim \\
\end{tabular}
\end{tiny}
\end{table}

Como é possível observar, na Tabela \ref{table:comp}, as técnicas existentes,
que foram analisadas, para eliminação de ruído, aplicáveis à extração
estruturada (Linhas \#4 a \#13), com exceção de \citeonline{viper05} (Linha
\#11), não atendem a todos os requisitos que o algoritmo proposto atende (Linha
\#14). As técnicas que não dependem da sintaxe HTML, necessitam de
treinamento e/ou definições \textit{a priori}. A abordagem adotada
em \citeonline{viper05} atende à todos os requisitos, no entanto, utiliza um
renderizador (que é computacionalmente dispendioso) e só identifica o ruído
posteriormente à identificação dos registros, desperdiçando uma
oportunidade de otimizar o processo, identificando e eliminando o ruído antes
de submeter a página à extração.

\chapter{Algoritmo para remoção de ruído}\label{ch:met}

Neste capítulo são apresentadas, primeiro, na Seção \ref{sec:def}, as definições
necessárias para formular o problema. Na Seção \ref{sec:probform} o
problema é enunciado a partir das definições e das hipóteses levantadas. Na
Seção \ref{sec:alg} são apresentados os algoritmos desenvolvidos a partir do
enunciado do problema e na Seção \ref{sec:compl} é feita a análise da
complexidade destes algoritmos.

\section{Definições básicas}\label{sec:def}

Nesta seção são apresentados os conceitos e definições necessários para o
entendimento do problema e do algoritmo proposto. As definições são
exemplificadas com o auxílio da Figura \ref{fig:ex1}.
%\begin{definition}[\textbf{Árvore DOM}]\label{def:domtree}
%A árvore DOM é uma estrutura hierárquica, derivada do código HTML, que
%representa a página $web$.
%\end{definition}
%Na Figura \ref{fig:ex1} um pequeno trecho de código é utilizado para ilustrar a
%árvore DOM e as próximas definições.

\begin{definition}[\textbf{$tag$ $paths$}]\label{def:tp} Um $tag$ $path$
(\abreviatura{TP}{tag path}TP) é uma $string$ descrevendo o caminho absoluto, a
partir da raiz da árvore DOM, até um dado nó da árvore. Seja ``i'' a posição em
profundidade do nó $``n_i"$ na árvore, então é dito que $TP_i$ é a $string$
descrevendo o caminho, a partir da raiz da árvore DOM, até o nó $``n_i"$.
\end{definition}
Na Figura \ref{fig:ex1}, o caminho absoluto $TP_4$ do nó $body$ até a célula da
tabela $td_4$ é $TP_4=``body/table/tr/td"$.

\begin{definition}[\textbf{Seqüência de \textit{$tag$ $paths$}}]\label{def:tps}
Uma seqüência de $tag$ $paths$ (\abreviatura{TPS}{tag path sequence}TPS) de uma
árvore DOM com ``n'' nós é definida como sendo a seqüência ordenada $TPS[1..n] = (TP_1,TP_2,TP_3, ...,TP_{n-1},TP_n)$
onde dois $tag$ $paths$ $TP_i$ e $TP_j$, com $i\neq{}j$, são considerados iguais
apenas se seus caminhos e suas definições de estilo são iguais, caso
contrário eles são considerados diferentes.
\end{definition}
Esta definição é similar à utilizada em \citeonline{SuffixTree12}, onde cada
$tag$ $path$ é representado na seqüência por um símbolo, exceto que neste trabalho
foram incorporadas as definições de estilo das $tags$. Na Figura \ref{fig:ex1}
é mostrada a $TPS$ de um dado trecho de código HTML, onde cada $TP$ recebe um
código, produzindo $TPS = (1,2,3,4,4,3,4,4)$.

\begin{definition}[\textbf{Alfabeto da TPS}]\label{def:alpha} Seja $\Sigma_a$ o
conjunto contendo todos os símbolos de uma dada seqüência $TPS_a$ de tamanho
``n'', $\Sigma_a$ é dito o alfabeto de $TPS_a$ definido como $\Sigma_a =
\{\alpha | \exists{TPS_a[i]}=\alpha \wedge 1 \leq i \leq n\}$, onde $\alpha$ é
um símbolo do alfabeto.
\end{definition}
Informalmente falando, o alfabeto contém todos os símbolos distintos de uma
determinada seqüência. Na Figura \ref{fig:ex1}, a $TPS$ é formada apenas pelos
símbolos ``1'',``2'', ``3'' e ``4'', então seu alfabeto é $\Sigma=\{1,2,3,4\}$.

\begin{definition}[\textbf{Conjunto de freqüências de $tag$
$paths$}]\label{def:tpfs} Seja $(s,f)$ um par ordenado onde ``s'' é um símbolo
de um alfabeto de uma determinada TPS e ``f'' é o número de vezes que o
símbolo ``s'' ocorre nesta TPS, então o conjunto de freqüências $FS$ de uma
TPS é definido como sendo o conjunto de todos os pares $(s,f)$
existentes nesta TPS, ou seja $FS =
\{(s_1,f_{s1}),(s_2,f_{s2}),(s_3,f_{s3}),\ldots,(s_{n-1},f_{sn-1}),(s_n,f_{sn})\}$,
onde ``n" é o tamanho do alfabeto da TPS.
\end{definition}
Na Figura \ref{fig:ex1}, o símbolo ``1'' ocorre uma vez na seqüência, o símbolo
``2'' ocorre uma vez também, o símbolo ``3'' duas vezes e o símbolo ``4'' quatro
vezes, então para esta seqüência $FS = \{(1,1), (2,1), (3,2), (4,4)\}$. O
conjunto $FS$ é o mapeamento de todos os símbolos de uma seqüência para suas
respectivas freqüências.

\begin{definition}[\textbf{Limiares de freqüência}]\label{def:ft}
Dada uma $TPS_a$ com alfabeto $\Sigma_a$ e conjunto de freqüências $FS_a$, os
limiares de freqüência $FT_a$ são definidos como sendo o conjunto ordenado
contendo apenas as freqüências de $FS_a$. Seja $FT_a = \{f|\exists{(s,f)} \wedge
(s,f) \in FS_a \wedge s \in \Sigma_a \}$, onde ``f'' é uma freqüência e ``s'' é
o símbolo correspondente do alfabeto $\Sigma_a$.
\end{definition}
Na TPS exibida na Figura \ref{fig:ex1}, o conjunto de freqüências é $FS=\{(1,1),
\\ (2,1), (3,2), (4,4)\}$, neste caso o conjunto dos limiares de freqüência é
igual a $FT=\{1,2,4\}$, pois os símbolos ``1'' e ``2'' tem freqüência igual a
\textbf{1}, o símbolo ``3'' tem freqüência igual a \textit{2} e o símbolo ``4''
tem freqüência igual a \textit{4}. O conjunto $FT$ é necessário para filtrar os
símbolos da TPS. Se $FT=\{1,2,4\}$, então não faria sentido filtrar símbolos com
freqüência igual a \textit{3}, pois não existem símbolos na seqüência com esta
freqüência.

\begin{definition}[\textbf{Região}]\label{def:region} 
Seja uma TPS a concatenação de duas outras seqüências $TPS=TPS_a . TPS_b$, então
é dito que $TPS_a$ e $TPS_b$ são regiões de $TPS$, $sse$ $\Sigma_a \cap \Sigma_b
= \emptyset$.
\end{definition}
Na Figura \ref{fig:ex1} a TPS pode ser dividida em duas subseqüências
$TPS_a=TPS[1..2]=(1,2)$ e $TPS_b=TPS[3..8]=(3,4,4,3,4,4)$, com alfabetos
$\Sigma_a=\{1,2\}$ e $\Sigma_b=\{3,4\}$, então $TPS_a$ a $TPS_b$
são regiões distintas de $TPS$, pois a intersecção de seus respectivos
alfabetos é vazia. % $\Sigma_a \cap
% \Sigma_b=\emptyset$.

\section{Proposta}\label{sec:probform}

Nesta pesquisa é proposto um novo algoritmo para segmentação das regiões de uma
página $web$ e identificação da região principal. O algoritmo proposto leva em
consideração a estrutura e o estilo de formatação das páginas e é
computacionalmente eficiente pois, conforme demonstrado na Seção
\ref{sec:compl}, sua complexidade é, na média, linear em relação ao tamanho da
página. A página $web$, que é submetida ao algoritmo, é representada como uma
seqüência de $tag$ $paths$.
O algoritmo proposto é formulado a partir de duas hipóteses:

\begin{enumerate}
  \item\label{ass:1} diferentes regiões de uma página $web$ são descritas por
  diferentes $tag$ $paths$;
  
A formulação da hipótese \ref{ass:1} vem da observação de que as várias regiões
de uma página $web$ são diferentes ramificações da árvore DOM e essas regiões
são descritas ou utilizando $tags$ distintas para cada região ou, se as $tags$
forem semelhantes, com diferentes estilos, para que cada região possa ser
facilmente distinguida pelo usuário/leitor da página. Se todas as regiões de uma
página se parecessem, seria mais difícil, para o usuário, distingui-las.
Então, pela Definição \ref{def:tps}, pode-se notar que os símbolos utilizadas em
cada região de uma página $web$ deveriam ser diferentes, e então deve ser
possível segmentar a página como na Definição \ref{def:region}. Esta hipótese,
como é possível observar, é geral o suficiente para poder ser reutilizada em
outros contextos, com ojetivo de segmentar páginas web, que não o de
extração estruturada.
  
  \item\label{ass:2} em $sites$ com conteúdo estruturado, a região
  principal é mais densa/maior que as demais regiões (menus, anúncios, text, etc.). 

A hipótese \ref{ass:2} vem do contexto no qual a segmentação proposta neste
trabalho é aplicada ($i.e.$ extração estruturada). Como se está considerando
apenas páginas com registros, e para descrever a estrutura desses registros são
necessários nós da árvore DOM, então são necessários mais nós para descrever
conteúdo estruturado do que é necessário para conteúdo não estruturado ($e.g.$
texto). Portanto, é razoável assumir que, para páginas que contém registros, a
região principal da página é a maior de todas ($i.e.$ a que contém mais nós).

\end{enumerate}

Utilizando as definições da Seção \ref{sec:def} e as hipóteses acima,
podemos formular o problema de segmentação de páginas e identificação da região
principal como: \textbf{``encontrar a maior \textit{região} da $TPS$ de uma
página $web$ que possua um $alfabeto$ que não se intersecte com o $alfabeto$ de
outras \textit{regiões} menores''}.

Um detalhe \textbf{crucial} que deve ser levado em consideração, é que podem
existir $tag$ $paths$ em um documento que representam divisões estruturais da
página ($i.e.$ formatação visual da página). Estes $tag$ $paths$, se forem
divisões, ocorrerão ao longo de toda a $TPS$, impedindo a identificação das
regiões, pois sempre haverá uma intersecção dos alfabetos. Para remover este
``ruído'' da seqüência, podemos filtrar os símbolos com freqüências mais baixas,
sem prejudicar o processo de segmentação, pois os símbolos com freqüências mais
altas continuam sendo considerados. Para ilustrar este procedimento, é
apresentado um exemplo de uma página $web$ na Listagem \ref{lst:html} que inicia
e termina com o mesmo $tag$ $path$ (``/body/br'', Linhas 2 e 18) e tem três
regiões delimitadas, também, pelo mesmo $tag$ $path$ (``/body/div'', Linhas 3, 8
e 13). Partindo da hipótese 1, em que diferentes $tag$ $paths$ são utilizados
para descrever cada região, sem filtrar os símbolos de baixa freqüência da
$TPS$, não seria possível identificar essas regiões utilizando o algoritmo
proposto.

\lstset{numbers=left, language=HTML, frame=single, tabsize=2, caption=Código
HTML, label=lst:html}
\begin{lstlisting}
<body>
	<br> <!-- cabeçalho -->
	<div> <!-- menu -->
		<span class='region1'>...</span>
		...
		<span class='region1'>...</span>
	</div> 
	<div> <!-- região principal -->
		<span class='region2'>...</span>
		...
		<span class='region2'>...</span>
	</div>
	<div> <!-- publicidade -->
		<span class='region3'>...</span>
		...
		<span class='region3'>...</span>
	</div>
	<br> <!-- rodapé -->
</body>
\end{lstlisting}

A TPS, do código HTML apresentado na Listagem \ref{lst:html}, é 
$TPS=(1,2,3,4,...,4,3,5,...,5,3,6,...,6,2)$.
Como os símbolos $2$ e $3$ ocorrem ao longo de toda a seqüência, os alfabetos
das regiões de menus, publicidade e da região principal, sempre terão uma
intersecção. Porém, considerando agora apenas os símbolos com freqüência maior
que $3$, durante o processo de segmentação é possível encontrar
as regiões com alfabetos distintos (sem intersecções) e dividir a $TPS$ da
seguinte maneira:
 
$TPS filtrada = ( , , ,4,...,4, ,5,...,5, ,6,...,6, )$

$menus = (4,...,4)$

$principal = (5,...,5)$

$publicidade = (6,...,6)$


\section{Algoritmos}\label{sec:alg}

Nesta seção são apresentados os algoritmos desenvolvidos para tratar o
problema definido na Seção \ref{sec:probform}. Quais sejam:

\begin{itemize}
  \item{\textbf{$tagPathSequenceFilter()$}} é o algoritmo
  de nível mais alto, que recebe um arquivo HTML como entrada, chama os demais
  algoritmos e retorna uma árvore DOM, podada, apenas com o conteúdo da região principal;
  \vspace{-8mm}
  
  \item{\textbf{$convertToSeq()$}} converte a árvore DOM de uma página
  $web$ para sua representação $TPS$;
  \vspace{-8mm}
  
  \item{\textbf{$searchRegion()$}} é o algoritmo principal, que realiza a busca,
  propriamente dita, pela região principal da $TPS$;
  \vspace{-8mm}
  
  \item{\textbf{$filterAlphabet()$}} filtra um alfabeto, removendo os
  símbolos de menor freqüência, tornando o algoritmo de busca mais robusto e resistente ao ruído;
  \vspace{-8mm}
  
  \item{\textbf{$pruneDOMTree()$}} poda a árvore DOM original,
  deixando apenas o conteúdo da região principal encontrado com $searchRegion$,
  mantendo a estrutura do documento original.
  
\end{itemize}
\pagebreak

\subsection{Algoritmo $tagPathSequenceFilter()$}
\begin{algorithm}[h]
\caption{Filtra o ruído de uma página $web$}
\label{alg:tpsfilter}
\textbf{Input:} $inputFile$ - um arquivo HTML \\
\textbf{Output:} árvore DOM de $inputFile$ podada
\begin{algorithmic}[1]
\Procedure{tagPathSequenceFilter}{$inputFile$}
\State $DOMTree \leftarrow parseHTML(inputFile)$
\State $convertToSeq(DOMTree.body,$`` ''$,TPS)$
\State $searchRegion(TPS)$
\State $pruneDOMTree(DOMTree.body,TPS)$
\State return $DOMTree$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $tagPathSequenceFilter()$ no Algoritmo \ref{alg:tpsfilter}
retorna a região principal de $inputFile$. O procedimento $parseHTML()$, na
Linha $2$, converte o código HTML em uma árvore DOM; $convertToSeq()$, na Linha
$3$, converte a árvore DOM em uma TPS; o procedimento $searchRegion()$, na Linha
$4$, procura recursivamente pela maior região da TPS que possui um alfabeto
único e, finalmente; $pruneDOMTree()$, na Linha $5$, retira da árvore DOM (poda)
todos os nós que não estão na TPS resultante, preservando a estrutura do
documento retornado na Linha $6$.

Em seguida são detalhados os algoritmos $convertToSeq()$, \\$searchRegion()$,
$filterAlphabet()$ e $pruneDOMTree()$. O algoritmo $parseHTML()$, que
consistiria em gerar a árvore DOM a partir do código HTML, não faz parte do
escopo deste trabalho e, portanto, não será discutido aqui.
\vspace{5cm}
\pagebreak

\subsection{Algoritmo $convertToSeq()$}

\begin{algorithm}[h]
\caption{Converte uma árvore DOM para a representação de seqüência de $tag$ $path$}
\label{alg:tree2seq}
\textbf{Input:} $node$ - um nó da árvore DOM, inicialmente a raiz da árvore \\
\textbf{Input:} $tp$ - o $tag$ $path$ anterior, inicialmente vazio \\
\textbf{Input:} $TPS$ - a $TPS$ de uma árvore DOM, inicialmente vazia \\
\textbf{Output:} a $TPS$ de uma árvore DOM armazenada em $TPS$
\begin{algorithmic}[1]
\Procedure{convertToSeq}{node,tp,TPS by ref.}
\State $tp \leftarrow concat(tp,$``/''$,node.$tag$,node.style)$
\If {$tp \ni tagPathMap$}
\State $tagPathMap \leftarrow tagPathMap + \{tp\}$
\State $tagPathMap[tp].code \leftarrow tagPathMap.size$
\EndIf
\State $TPS \leftarrow concat(TPS,tagPathMap[tp].code);$
\For {each $child$ of $node$}
\State $convertToSeq(child,tp,TPS)$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $convertToSeq()$ no Algoritmo \ref{alg:tree2seq} converte uma
página $web$ de sua representação em árvore DOM para uma representação em
formato TPS, percorrendo a árvore DOM em profundidade. Inicialmente é chamado no
Algoritmo \ref{alg:tpsfilter} com um $tag$ $path$ vazio como parâmetro, que
representa a $string$ do $tag$ $path$ anterior ($i.e.$ da chamada recursiva
anterior).
Na Linha $2$, o $tag$ $path$ anterior é concatenado com a $tag$ atual e com suas
definições de estilo, com objetivo de distinguir caminhos repetidos com estilos
diferentes; na Linha $3$, é verificado se o $tag$ $path$ atual já foi visto
antes pelo algoritmo ou não ($tagPathMap$ é utilizado para este propósito) e, se
não, na Linha $4$, ele é inserido no conjunto $tagPathMap$ e um novo código é
atribuído à ele na Linha $5$, como mostrado na Definição \ref{def:tps}; na Linha
$7$, o código do $tag$ $path$ é concatenado ao final da seqüência e, finalmente,
o procedimento é invocado recursivamente na Linha $9$ para cara filho de $node$.

\subsection{Algoritmo $searchRegion()$}

Este é o algoritmo central, já que é responsável por encontrar a região
principal e, para facilitar seu entendimento, foi incluída uma ilustração do seu
funcionamento (a Figura \ref{fig:alg}). A Figura \ref{fig:alg} apresenta uma
ilustração da execução do algoritmo na qual foi omitido o procedimento de
filtragem dos símbolos com baixas freqüências para maior clareza, pois a
intenção é passar a idéia principal. Na Figura \ref{fig:alg} estão representadas
as várias iterações feitas em cima da seqüência até o ponto em que a região
principal é localizada; em cada iteração o cursor percorre as posições da
seqüência e verifica, em cada posição, se não há intersecção entre os alfabetos
das subseqüências ``a'' e ``b'' e, neste caso, inicia uma nova iteração em cima
da maior subseqüência resultante, até que esta não possa mais ser subdividida,
situação em que a região principal foi localizada. Uma descrição detalhada é
apresentada para a listagem do algoritmo.

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.45]{alg-pb-pt.jpg}
  \caption{Ilustração da execução do algoritmo.}
  \label{fig:alg}
\end{figure}
\pagebreak

\begin{algorithm}[H]
\begin{tiny}
\caption{Busca regiões da TPS com alfabetos diferentes}
\label{alg:searchreg}
\textbf{Entrada:} $tagPathSequence$ - a $TPS$ de uma determinada página $web$ \\
\textbf{Saída:} a região principal da $TPS$, armazenada em $tagPathSequence$
%\end{algorithm}
\begin{algorithmic}[1]
\Procedure{searchRegion}{$tagPathSequence[1..n]$ by reference}
\State $\Sigma \leftarrow \emptyset$ \Comment{alfabeto da seqüência}
\State $t \leftarrow 0$ \Comment{índice dos limiares de freqüência}
\For {$i \leftarrow 1..n$}
\State $sym \leftarrow tagPathSequence[i]$
\If {$sym \ni \Sigma$}
\State $\Sigma \leftarrow \Sigma \cup \{sym\}$
\State $symCount[sym] \leftarrow 0$
\EndIf
\State $symCount[sym] \leftarrow symCount[sym]+1$
\EndFor
\State $thresholds \leftarrow Ordered Set Of Frequencies(symCount)$
\State $regionFound \leftarrow false$
\While {not $regionFound$}
\State $t \leftarrow t + 1$
\State $curr\Sigma \leftarrow filterAlphabet(\Sigma,symCount,thresholds[t])$
\If {$curr\Sigma.size < 2$}
\State $break$
\EndIf
\State $currSymCount \leftarrow symCount$
\State $region\Sigma \leftarrow \emptyset$
\For {$i \leftarrow 1..n$}
\State $sym \leftarrow tagPathSequence[i]$
\If {$sym \in curr\Sigma$}
\State $region\Sigma \leftarrow region\Sigma \cup \{sym\}$
\State $currSymCount[sym] \leftarrow currSymCount[sym]-1$
\If {$currSymCount[sym] = 0$}
\State $curr\Sigma \leftarrow curr\Sigma - \{sym\}$
\If {$curr\Sigma \cap region\Sigma = \emptyset$}
\If {$(curr\Sigma \neq \emptyset) \wedge (n-2i)/n > 0.20$}
\State $regionFound \leftarrow true$
\EndIf
\State $break$
\EndIf
\EndIf
\EndIf
\EndFor
\EndWhile
\If {$regionFound$}
\If {$i < n/2$}
\State $tagPathSequence \leftarrow tagPathSequence[i+1..n]$
\Else
\State $tagPathSequence \leftarrow tagPathSequence[1..i]$
\EndIf
\State $searchRegion(tagPathSequence)$
\EndIf
\EndProcedure
\end{algorithmic}
\end{tiny}
\end{algorithm}

O procedimento $searchRegion()$ no Algoritmo \ref{alg:searchreg} calcula o
alfabeto da TPS e as freqüências correspondentes de cada símbolo nas Linhas $4$
a $11$; na Linha $12$, os limiares de freqüência, como especificado Definição
\ref{def:ft}, são calculados; nas Linhas $14$ a $38$ é realizada a busca por uma posição na
seqüência onde é possível dividi-la ($i.e.$ onde existe uma região); na Linha
$15$ os limiares de freqüência são iterados; na Linha $16$ o alfabeto da TPS, da
Definição \ref{def:alpha}, é filtrado, como descrito na Seção
\ref{sec:probform}; na Linha $22$ a TPS é iterada; na Linha $25$ o alfabeto da
região é calculado e; nas Linhas $27$ a $35$ é verificado se existe uma
intersecção entre os alfabetos das duas partes da TPS (uma intersecção vazia
indica que uma possível região foi encontrada, como na Definição
\ref{def:region}). A região encontrada só é considerada válida se for ao menos
20\% maior que o restante da seqüência, caso contrário o filtro de freqüência é
iterado e a busca prossegue.
Este percentual é, na verdade, um parâmetro com o propósito de evitar relatar
uma região sob condições ambíguas (nos experimentos foi utilizado o valor de
20\%); finalmente nas Linhas $39$ a $46$ a TPS é dividida se uma região foi
encontrada, chamando o procedimento $searchRegion()$ recursivamente na Linha
$45$.

\subsection{Algoritmo $filterAlphabet()$}

\begin{algorithm}[H]
\caption{Filtra símbolos de baixa freqüência no alfabeto}
\label{alg:filteralpha}
\textbf{Input:} $\Sigma$ - alfabeto a ser filtrado \\
\textbf{Input:} $symCount$ - conjunto das freqüências ($FS$) dos símbolos de
$\Sigma$ \\
\textbf{Input:} $threshold$ - limiar de freqüência \\
\textbf{Output:} alfabeto filtrado
\begin{algorithmic}[1]
\Procedure{filterAlphabet}{$\Sigma$, symCount, threshold}
\State $filtered\Sigma \leftarrow \emptyset$
\For {$i \leftarrow 1..n$}
\If {$symCount[\Sigma[i]] \geq threshold$}
\State $filtered\Sigma \leftarrow filtered\Sigma \cup \{\Sigma[i]\}$
\EndIf
\EndFor
\State return $filtered\Sigma$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $filterAlphabet()$ no Algoritmo \ref{alg:filteralpha} remove do alfabeto
$\Sigma$, todo símbolo com freqüência inferior a $threshold$.
Nas Linhas $3$ to $7$ apenas os símbolos com freqüência maior ou igual a
$threshold$ são inseridos no conjunto resultante. O resultado de $filterAlphabet()$ é
utilizado no Algoritmo \ref{alg:searchreg}, Linha $24$, onde apenas os símbolos
em $curr\Sigma$ são considerados durante a busca pela região principal.
\vspace{3.3cm}
\pagebreak

\subsection{Algoritmo $pruneDOMTree()$}

\begin{algorithm}[H]
\caption{Poda da árvore DOM nós que não pertencem à seqüência}
\label{alg:prune}
\textbf{Input:} $node$ - um nó da árvore DOM, inicialmente a raiz \\
\textbf{Input:} $sequence$ - a TPS que deve permanecer na árvore DOM \\
\textbf{Output:} árvore DOM podada, referenciada em $node$
\begin{algorithmic}[1]
\Procedure{pruneDOMTree}{$node$ by ref.,$sequence$}
\For {each child of node}
\If {$pruneDOMTree(child,sequence) = true$}
\State remove child from $node$
\EndIf
\EndFor
\If {$node \ni sequence$ and $node.childCount = 0$}
\State return $true$
\EndIf
\State return $false$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $pruneDOMTree()$ no Algoritmo \ref{alg:prune}, percorre a árvore
DOM, em profundidade, removendo os nós que não pertencem a $sequence$. Na Linha
$3$ a árvore DOM é percorrida; nas Linhas $7$ a $9$ é decidido se o nó $node$
deve ser removido ou não.

Um nó é removido da árvore, apenas se ele não está em $sequence$ \textbf{e} não
tem nenhum filho. Desta forma é possível manter a estrutura da árvore
remanescente intacta, evitando afetar, negativamente, a etapa subseqüente de
extração estruturada.

\section{Complexidade do Algoritmo}\label{sec:compl}

Com relação à complexidade do algoritmo é suficiente analisar apenas o
procedimento $searchRegion()$, pois é este procedimento que concentra o
processamento principal do algoritmo. Observando as Linhas $14$ e $22$ do
procedimento $searchRegion()$, pode-se ver que o laço de repetição da Linha $14$
itera os limiares de freqüência até encontrar uma região e na Linha $22$ a TPS,
filtrada em uma determina freqüência, é iterada, também, até encontrar uma
região e, se encontrada, a região é processada recursivamente.

No pior caso, quando a intersecção dos alfabetos só é vazia na última posição da
TPS, a complexidade é, no máximo, $O(n^2 f)$, onde $n$ é o tamanho da TPS e $f$
é o tamanho do conjunto $thresholds$. Na prática, o tamanho do conjunto
$thresholds$ é bem menor do que o tamanho da TPS, então é possível afirmar que
a complexidade de tempo se aproxima de $O(n^2)$ como mostrado na Equação
\ref{eq:w}.
  \begin{equation}\label{eq:w}
  T(n) = T(n - 1) + \Theta(n) \Longrightarrow \sum_{i=1}^{n}
  i = \frac{n(n+1)}{2} = O(n^2)
  \end{equation} 

Na média, se a TPS é dividida na metade, a complexidade é $O(n)$, como mostrado
na Equação \ref{eq:a}.
  \begin{equation}\label{eq:a}
  T(n) = T(n/2) + \Theta(n) \Longrightarrow \sum_{i=1}^{log_{2}^{n}}
  \frac{n}{2^i} = n - 1 = O(n)
  \end{equation} 

No melhor caso, a TPS é dividida na primeira posição, resultando em uma
complexidade de $O(n)$ como mostrado na Equação \ref{eq:b}.
  \begin{equation}\label{eq:b}
  T(n) = T(n - 1) + \Theta(1) \Longrightarrow \sum_{i=1}^{n} 1 = n = O(n)
  \end{equation} 


Em situações reais, como as que ocorreram na avaliação empírica do algoritmo,
as seqüências são divididas aproximadamente quatro ou cinco vezes até que não
seja mais possível dividi-las. Portanto, para situações reais, é possível
afirmar que o algoritmo possui complexidade de tempo $O(in)$, onde $n$ é o
tamanho da TPS e $i$ é o número de vezes que a seqüência é dividida, o qual
podemos considerar como sendo uma constante (sem impacto assintótico) e, sendo
assim, afirmar que a complexidade de tempo é $O(n)$.

\chapter{Resultados Experimentais}\label{ch:results}

Neste capítulo são descritos e discutidos os resultados dos experimentos e sua
realização.
Para obter os resultados apresentados na Subseção \ref{sub:res}, o algoritmo
proposto foi implementado e testado em vários $web$ $sites$ comerciais e
institucionais. Na Seção \ref{sec:clar} um destes resultados é detalhado, para
exemplificar e esclarecer como os dados foram obtidos e compilados na Tabela
\ref{table:results}.

\section{Metodologia de avaliação dos resultados}\label{sec:met}

Para avaliar os resultados da técnica proposta, foi utilizado o algoritmo MDR
\cite{MDR03}, uma conhecida técnica de extração estruturada. O resultado da
extração pura (apenas MDR) será considerado como $baseline$ para comparação com
o resultado da extração combinada com a técnica proposta de remoção de ruído,
como ilustrado na Figura \ref{fig:method}.

A avaliação dos resultados é constituída das seguintes etapas:
\begin{itemize}
  \item escolha de uma técnica de extração automática de dados estruturados;
  
  \item definição de um conjunto de páginas para teste;

  \item aplicação da técnica de extração no conjunto de teste, documentando os
  resultados obtidos como \textit{target} (registro de interesse) ou
  \textit{noise} (ruído);

  \item nova aplicação da técnica de extração no mesmo conjunto de teste,
  mas desta vez filtrando-o antes, utilizando a técnica proposta neste trabalho,
  documentando os resultados da mesma maneira;

  \item comparação de ambos os resultados e medição do aumento/redução da
  precisão (remoção de ruído).
\end{itemize}
\pagebreak

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.19]{metodo.jpg}
  \caption{Método de avaliação adotado.}
  \label{fig:method}
\end{figure}

Para primeira etapa foi escolhido o MDR para realizar a extração, pois esta
técnica possui diversas implementações disponíveis, vários trabalhos publicados
a respeito, funciona em uma única página e os resultados da extração são
razoáveis para avaliar a técnica proposta neste trabalho. Outras técnicas
poderiam ser utilizadas, se estivessem disponíveis, e desde que não necessitem
de várias páginas HTML como entrada ($e.g.$ $RoadRunner$ \cite{RRunner01}).


\section{Detalhamento dos Experimentos}\label{sec:clar}

Os resultados da extração obtidos utilizando apenas o MDR, foram considerados
como $baseline$ a ser comparado com os resultados obtidos através do uso
combinado da técnica proposta com o MDR, como ilustrado na Figura
\ref{fig:method}. A seguir um dos experimentos realizado é detalhado para
exemplificar como os resultados estão compilados na Tabela \ref{table:results}.
   
Quando ambas abordagens (MDR e filtragem TPS + MDR) são aplicadas a uma página
de busca com 20 resultados do $web$ $site$ ``YouTube'' (\#38), os seguintes
resultados são obtidos:

\begin{itemize}
\item{}página $web$ não filtrada ($i.e.$ página original)
\begin{itemize}
\item{}árvore DOM processada: $1424$ nós;
\item{}resultados do MDR: $82$ registros no total ($62$ ruído/$20$ alvo);
\end{itemize}

\item{}página $web$ filtrada ($i.e.$ página após aplicação do filtro da
TPS)
\begin{itemize}
\item{}árvore DOM processada: $674$ nós, tamanho $47,33\%$ da página original,
redução de $-52,67\%$
\item{}resultados do MDR: $20$ registros no total ($0$ ruído/$20$ alvo), ruído
removido $100\%$
\end{itemize}
\end{itemize}
\pagebreak

Neste resultado observa-se uma melhora na extração dos registros, assim como uma
redução considerável do tamanho da árvore DOM que precisou ser processada. Um
percentual de $52.67\%$ da árvore DOM foi podado sem que ocorresse a perda dos
registros de interesse durante o processo. Tudo o que foi podado da árvore,
neste caso, era ruído. A Figura \ref{fig:ex2} ilustra a região principal da
página e a Figura \ref{fig:tps} mostra o gráfico da respectiva TPS e a
região principal detectada pelo algoritmo proposto.

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.32]{example2-gs-pt.jpg}
  \caption{Um página de resultados do $site$ YouTube com sua região principal
  delimitada.}
  \label{fig:ex2}
\end{figure}
\vspace{5cm}
\pagebreak

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.28]{tps-pt.jpg}
  \caption{Gráfico da TPS ``código TP vs posição'' de uma página do $site$
  YouTube com a região principal que foi detectada pelo algoritmo.}
  \label{fig:tps}
\end{figure}


Sem aplicar o filtro na TPS, são obtidos $82$ registros no total e, como sabe-se
de antemão que existem $20$ registros de interesse nesta página, considera-se
que a quantidade de $62$ registros representam $100\%$ do ruído presente nesta
página e que deve ser removido. Quando o filtro é utilizado, desta vez são
obtidos apenas os $20$ registros de interesse na fase de extração, resultando em
uma precisão de $100\%$, o que significa que todo o ruído da página foi
removido, neste caso. O percentual de ruído removido é calculado como
\begin{equation}\label{eq:noise}
NoiseRemoved=1-\frac{\#Rec_{totalTPS}-\#Rec_{targetTPS}}{\#Rec_{total}-\#Rec_{target}}
\end{equation}
Onde $\#Rec_{total}$ e $\#Rec_{target}$ são o total de registros e o total de
registros de interesse, respectivamente, da página original, e
$\#Rec_{totalTPS}$ e $\#Rec_{targetTPS}$ são o total de registros e o total de
registro de interesse, respectivamente, da página $web$ filtrada.

\subsection{Resultados}\label{sub:res}

Na Tabela \ref{table:results} são apresentados, nas três primeiras colunas, o
tamanho da árvore DOM processada pelo MDR ($i.e.$ original) e a redução obtida
após remoção do ruído ($i.e.$ podado). A coluna ``Conteúdo principal'' indica se
o processo de remoção de ruído preservou a região principal ou não. As próximas
quatro colunas são os resultados obtidos com o MDR sem e com remoção de ruído
respectivamente, exibindo o total geral de registros extraídos e o total de
registros de interesse ($target$) extraídos para ambas abordagens. A última
coluna mostra o percentual de ruído removido, calculado conforme a Equação
\ref{eq:noise}.

\begin{table}[H]
\caption{Resultados}
\label{table:results}
\centering
\begin{tiny}
\begin{tabular}{p{1mm}| p{19mm}| p{3mm} p{6mm} p{9mm}| p{6mm}| p{2mm} p{2mm} |
p{2mm} p{2mm} |r} 
\hline\hline & \multicolumn{5}{c}{} & \multicolumn{4}{c}{MDR (\# registros)} & (eq. \ref
{eq:noise})\\
& & \multicolumn{3}{c}{Tam. DOM (\# nós)} & Cont. & \multicolumn{2}{c}{Orig.}
& \multicolumn{2}{c}{Podado} & Ruído\\
\# & $Site$ & Orig. & Podado & Red. & princ. & Tot & Tgt & Tot & Tgt & rem. \\
\hline
1 & ACM & 601 & 340 & -43,43\% & Sim & 61 & 10 & 16 & 10 & 88,24\% \\
2 & Amazon & 3499 & 1069 & -69,45\% & Sim & 344 & 16 & 41 & 16 & 92,38\% \\
3 & Apple & 1422 & 962 & -32,35\% & Sim & 63 & 15 & 23 & 15 & 83,33\% \\
4 & Avon & 911 & 575 & -36,88\% & Sim & 69 & 20 & 29 & 20 & 81,63\% \\
5 & Barnes \& Noble & 1242 & 778 & -37,36\% & Sim & 147 & 30 & 54 & 30 & 79,49\% \\
6 & Bestbuy & 3632 & 1425 & -60,77\% & Sim & 299 & 15 & 15 & 15 & 100,00\% \\
7 & Blockbuster & 2176 & 1381 & -36,53\% & Sim & 84 & 25 & 32 & 25 & 88,14\% \\
8 & Bondfaro & 3897 & 1820 & -53,30\% & Sim & 160 & 28 & 30 & 28 & 98,48\% \\
9 & Bradesco & 1913 & 1113 & -41,82\% & Sim & 164 & 10 & 93 & 10 & 46,10\% \\
10 & Build & 2712 & 746 & -72,49\% & Não & 117 & 12 & 69 & 0 & N/A \\
11 & Buscapé & 3608 & 1607 & -55,46\% & Sim & 223 & 24 & 24 & 24 & 100,00\% \\
12 & Costco & 4504 & 326 & -92,76\% & Não & 246 & 96 & 8 & 8 & N/A \\
13 & Dell & 1737 & 881 & -49,28\% & Sim & 177 & 12 & 64 & 12 & 68,48\% \\
14 & Disney & 2778 & 2006 & -27,79\% & Sim & 193 & 96 & 116 & 96 & 79,38\% \\
15 & Drugstore & 1774 & 850 & -52,09\% & Sim & 156 & 18 & 52 & 18 & 75,36\% \\
16 & eBay & 2623 & 1801 & -31,34\% & Sim & 162 & 50 & 50 & 50 & 100,00\% \\
17 & Elsevier & 906 & 160 & -82,34\% & Sim & 120 & 10 & 32 & 10 & 80,00\% \\
18 & Footlocker & 2440 & 1106 & -54,67\% & Sim & 238 & 60 & 60 & 60 & 100,00\% \\
19 & Gamestop & 1947 & 935 & -51,98\% & Não & 86 & 12 & 6 & 0 & N/A \\
20 & Gap & 2249 & 1365 & -39,31\% & Sim & 235 & 124 & 126 & 124 & 98,20\% \\
21 & Globo & 400 & 193 & -51,75\% & Sim & 80 & 10 & 20 & 10 & 85,71\% \\
22 & Globo G1 & 900 & 619 & -31,22\% & Não & 225 & 10 & 202 & 0 & N/A \\
23 & Google & 1421 & 474 & -66,64\% & Sim & 118 & 11 & 21 & 11 & 90,65\% \\
24 & Home Depot & 5199 & 1304 & -74,92\% & Sim & 325 & 24 & 24 & 24 & 100,00\% \\
25 & HP & 1783 & 1258 & -29,44\% & Sim & 71 & 15 & 15 & 15 & 100,00\% \\
26 & Itaú & 1111 & 410 & -63,10\% & Não & 77 & 10 & 11 & 0 & N/A \\
27 & Lojas Americanas & 2660 & 710 & -73,31\% & Sim & 211 & 20 & 20 & 20 & 100,00\% \\
28 & Macy's & 5676 & 2158 & -61,98\% & Não & 164 & 40 & 43 & 0 & N/A \\
29 & Magazine Luiza & 3167 & 1115 & -64,79\% & Sim & 314 & 40 & 44 & 40 & 98,54\% \\
30 & Mercadolivre & 2401 & 1771 & -26,24\% & Sim & 136 & 50 & 52 & 50 & 97,67\% \\
31 & Microsoft & 871 & 272 & -68,77\% & Sim & 57 & 16 & 16 & 16 & 100,00\% \\
32 & Newegg & 8481 & 3419 & -59,69\% & Não & 965 & 20 & 307 & 0 & N/A \\
33 & Nike & 4082 & 1829 & -55,19\% & Sim & 329 & 23 & 23 & 23 & 100,00\% \\
34 & Office Depot & 3363 & 1111 & -66,96\% & Sim & 108 & 24 & 38 & 24 & 83,33\% \\
35 & PC Mall & 4285 & 2548 & -40,54\% & Sim & 216 & 25 & 53 & 25 & 85,34\% \\
36 & Rakuten & 3386 & 2768 & -18,25\% & Sim & 112 & 35 & 61 & 35 & 66,23\% \\
37 & Ralph Lauren & 995 & 564 & -43,32\% & Sim & 46 & 12 & 12 & 12 & 100,00\% \\
38 & Reuters & 1202 & 198 & -83,53\% & Sim & 136 & 10 & 10 & 10 & 100,00\% \\
39 & Scopus & 4929 & 4688 & -4,89\% & Sim & 114 & 20 & 75 & 20 & 41,49\% \\
40 & Sears & 5726 & 3890 & -32,06\% & Sim & 397 & 50 & 75 & 50 & 92,80\% \\
41 & Sephora & 3022 & 1440 & -52,35\% & Sim & 365 & 60 & 60 & 60 & 100,00\% \\
42 & Sony & 2200 & 1316 & -40,18\% & Sim & 98 & 15 & 18 & 15 & 96,39\% \\
43 & Staples & 2959 & 1611 & -45,56\% & Sim & 178 & 24 & 24 & 24 & 100,00\% \\
44 & Submarino & 2389 & 1268 & -46,92\% & Sim & 116 & 20 & 22 & 20 & 97,92\% \\
45 & Terra & 869 & 588 & -32,34\% & Sim & 122 & 50 & 76 & 50 & 63,89\% \\
46 & Tiffany & 3899 & 2753 & -29,39\% & Não & 183 & 12 & 67 & 0 & N/A \\
47 & Valor Econômico & 514 & 126 & -75,49\% & Não & 55 & 10 & 2 & 0 & N/A \\
48 & Wal-Mart & 1576 & 808 & -48,73\% & Sim & 110 & 20 & 40 & 20 & 77,78\% \\
49 & Webmotors & 2119 & 1361 & -35,77\% & Sim & 113 & 14 & 19 & 14 & 94,95\% \\
50 & Yahoo! & 760 & 290 & -61,84\% & Sim & 67 & 10 & 10 & 10 & 100,00\% \\
51 & YouTube & 1424 & 674 & -52,67\% & Sim & 82 & 20 & 20 & 20 & 100,00\% \\
\hline
& Média/Total &  &  & -50,18\% & 82,35\%&  &  &  &  & 88,86\% \\
&             &  &  &          & 17,65\%&  &  &  &  &  \\
\hline

\end{tabular}
\end{tiny}
\end{table}
 
Como se pode observar na Tabela \ref{table:results}, o total da coluna
``Conteúdo principal'' indica que o algoritmo funcionou em 82,35\% dos casos e
removeu, com este conjunto de teste, uma média de 88,86\% de todo o ruído
presente nos dados, como mostra a média da coluna ``Ruído rem.''.

A média de 50,18\% de redução no tamanho da árvore DOM é um resultado
interessante. Primeiro, por que significa que praticamente metade da árvore DOM
é ruído, na média. Segundo, por que este valor ficou bem próximo do valor
relatado, de forma independente, em \citeonline{Volume05} como sendo o tamanho
do ``template'' das páginas (em torno de 50\%).

Uma situação que chama a atenção na Tabela \ref{table:results} é o resultado
obtido para a página do $site$ ``Build'' (\#10). Sem filtrar o ruído, o MDR
retornou um total de $117$ registros, incluindo os $12$ registros de interesse.
Após aplicar o filtro de ruído, um total de $69$ registros são retornados,
nenhum deles de interesse, todos ruído. Então, após a filtragem, se a árvore
retornada fosse, na realidade, o seu complemento, o resultado seria de $48$
registros no total ($117-69=48$), incluindo os $12$ registros de interesse, o
que seria um bom resultado já que significa uma remoção de 65,71\% do ruído. O
que se pode deduzir disto é que, a segmentação da página ocorreu corretamente,
apenas a identificação da região principal falhou, já que neste caso, era
relativamente pequena.

Como conseqüência da remoção de ruído, podemos observar um aumento considerável
na precisão média da extração, como relatado na Tabela \ref{table:prec}. A
redução no $recall$ corresponde às situações onde o filtro falhou, as quais são
discutidas e tratadas na Subseção \ref{sub:disc}, e, apesar desta redução, as
medidas F e G apresentaram melhora significativa.

\begin{table}[H]
\centering
\caption{Precisão, $recall$, F-$measure$ e G-$measure$.}
\label{table:prec}
\begin{tabular}{l l l l}
\hline\hline
& MDR ($a$) & MDR+filtro ($b$) & Variação ($b-a$)\\
\hline
Precisão & 18,93\%  & 75,08\% & 56,15\% \\
$Recall$   & 100,00\% & 82,35\% & -17,65\% \\
F-$measure$ & 31,83\% & 78,55\% & 46,72\% \\
G-$measure$ & 43,51\% & 78,63\% & 35,13\% \\
\hline
\end{tabular}
\end{table}
\vspace{3cm}
\pagebreak

\subsection{Discussão dos Resultados}\label{sub:disc}

Existem três situações principais onde o algoritmo poderia ser melhorado e,
destas, apenas duas podem levar a perda de conteúdo de interesse. Na Tabela
\ref{table:results}, coluna ``Conteúdo principal'', estas duas situações são
responsáveis por 17,65\% dos casos, onde o conteúdo principal foi removido
durante o processo de filtragem.

\begin{enumerate}
  \item{\textbf{páginas com \textit{templates} muito homogêneos}}. São páginas
  com pouca diferença entre as regiões. Neste caso, utilizando a técnica
  proposta, não resta muito a fazer, pois simplesmente não há informação
  suficiente para uma diferenciação entre as regiões da página, já que todas se
  parecem. Não são perdidos registros de interesse, mas a quantidade total de
  ruído removido é baixa;
  \item{\textbf{páginas com \textit{templates} muito heterogêneos}}. São paginas
  onde o conteúdo principal é subdividido em mais de uma região. Neste caso, a
  região principal acaba por ser dividida várias vezes durante o processo de
  filtragem e apenas a parte maior acaba passando pelo filtro (e ainda assim,
  pode ser que seja ruído). Mais a frente é proposta uma forma de contornar
  este problema;
  \item{\textbf{páginas com região principal menor que o restante}}. Este
  problema é uma conseqüência da segunda hipótese levantada na Seção 
  \ref{sec:probform}: ``a região principal é mais densa, ou maior, que as demais
  regiões''. Neste caso, ruído sempre será retornado no lugar do conteúdo
  principal. A mesma proposta apresentada para situação anterior também pode ser
  utilizada neste caso.
\end{enumerate}

No caso de \textit{templates} heterogêneos, a filtragem da TPS ainda pode ser
utilizada realizando algumas modificações no algoritmo. Um exemplo de
\textit{template} heterogêneo são os $sites$ de notícias, onde cada registro de
interesse tem uma estrutura diferente, mas todos pertencem ao mesmo domínio
($i.e.$ pertencem à mesma entidade). Nesta situação específica, a segmentação da
TPS ainda pode ser utilizada para segmentar a página em diferentes regiões, e
uma abordagem semântica pode ser usada para combinar as várias regiões de
interesse, retornando um conjunto de regiões como resultado e não apenas uma
única região.
 
Para a situação descrita para o $site$ ``Build'' (que ocorreu, também, em outros
oito $sites$, ou seja, em 17,65\% dos casos), quando a região principal é menor
que as demais, uma abordagem semântica também pode ser utilizada para validar se
a região encontrada, de fato, possui conteúdo de interesse e, se não possuir
deve ser descartada e a árvore complementar ($i.e.$ inverter a poda) deve ser
retornada em seu lugar. O algoritmo principal ficaria assim:

\begin{algorithm}[H]
\caption{Filtra o ruído de uma página $web$}
\label{alg:tpsfilter2}
\textbf{Input:} $inputFile$ - um arquivo HTML \\
\textbf{Output:} árvore DOM de $inputFile$ podada
\begin{algorithmic}[1]
\Procedure{tagPathSequenceFilter}{$inputFile$}
\State $DOMTree \leftarrow parseHTML(inputFile)$
\State $convertToSeq(DOMTree.body,$`` ''$,TPS)$
\State $backupTPS \leftarrow TPS$
\State $searchRegion(TPS)$
\If {$TPS$ not $content$}
\State $TPS = backupTPS - TPS$   
\EndIf
\State $pruneDOMTree(DOMTree.body,TPS)$
\State return $DOMTree$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algoritmo \ref{alg:tpsfilter2} é igual ao Algoritmo \ref{alg:tpsfilter}, exceto
pela Linha $6$ onde é verificado se o conteúdo principal está presente na região
e, caso não esteja, a seqüência complementar é utilizada (Linha $7$), garantindo
a presença do conteúdo principal.

Técnicas como a apresentada em \citeonline{Adaptive07}, que classifica uma
seqüência de blocos como sendo conteúdo ou não, pode ser utilizada para
implementar a Linha $6$ do algoritmo \ref{alg:tpsfilter2}.

\chapter{Conclusões e Trabalhos Futuros}\label{ch:conclusion}

Como mostrado nos resultados, o algoritmo proposto para segmentação da página e
remoção do ruído foi bastante efetivo em várias páginas
comerciais/institucionais.
Na maioria dos casos uma grande quantidade de ruído foi removido sem comprometer
os registros de interesse. Também, quando aplicada em conjunto com o MDR, foi
observado um incremento considerável na precisão da extração e na qualidade
geral da extração ($e.g.$ aumento do f-measure de 31,83\% para 78,55\%).

Nas situações em que o algoritmo proposto atua aquém das expectativas, outras
técnicas podem ser combinadas dependendo da aplicação. Em casos extremos, onde a
página possuir estrutura muito homogênea (de forma que não seja possível
encontrar as diferentes regiões) ou muito heterogênea (onde o conteúdo principal
está espalhado em diversas regiões diferentes), o conteúdo principal poderia,
talvez, ser detectado através do uso de uma técnica semântica.

O algoritmo mostrou excelente performance, pois apresentou resultados
satisfatórios para a maioria dos $sites$ comerciais nos quais foi testado. Ele
também supera as limitações (necessidade de treinamento, dependência de $tags$
HTML, anotações manuais, entre outras) de trabalhos anteriores na área de
limpeza, remoção de ruído e segmentação de página, como mencionado no Capítulo
\ref{ch:rev}.

Uma sugestão para trabalho futuro seria encontrar maneiras de adaptar o
algoritmo para melhorar seus resultados nas seguintes situações:
\begin{itemize}
  \item páginas que apresentem estrutura muito heterogênea: encontrar uma forma
  de agrupar as várias regiões principais em uma só;
  \item páginas que apresentem estrutura muito homogênea: encontrar uma forma
  de diferenciar o ruído da região principal;
  \item páginas onde a região principal tem tamanho inferior ao ruído total:
  identificar a região principal através de outros atributos que não o tamanho.
\end{itemize}

Este trabalho também foi publicado no \textit{JIDM - Journal of Information and
Data Management} no ano de 2013, como artigo completo \cite{TPS2013}, e os
algoritmos descritos neste trabalho foram implementados nas linguagens
\textit{C++} e $M$ (MatLab/Octave) e estão disponíveis para acesso.

\bibliographystyle{ufscThesis/ufsc-alf}
\bibliography{refs}

%\apendice
%\chapter{Tabela de resultados}

\end{document}
