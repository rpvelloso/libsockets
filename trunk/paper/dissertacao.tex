\documentclass{ufscThesis}
\usepackage{graphicx}
\usepackage[labelsep=endash]{caption} % O separador de legenda é um -
\usepackage{algorithmicx}
\usepackage[Algoritmo,ruled]{algorithm}
\usepackage{algpseudocode}

\newtheorem{definition}{Definição}

\titulo{Segmentação e Remoção Automática de Ruído de Páginas Web Utilizando
$Tag$ $Paths$}
% Titulo do trabalho \subtitulo{Estilo \LaTeX~ padrão}                % Subtitulo do trabalho (opcional)
\autor{Roberto Panerai Velloso}
\data{22}{maio}{2013}                           % Data da publicação do
\orientador{Profa. Dra. Carina F. Dorneles}                    % Nome do
\begin{document}
\capa
\folhaderosto[comficha] % Se nao quiser imprimir a ficha, é só não usar o
% parâmetro
%\folhaaprovacao
%\paginadedicatoria
%\paginaagradecimento
%\paginaepigrafe
%\listadefiguras
%\listadetabelas 
%\listadeabreviaturas
%\listadesimbolos
%\sumario

\textoResumo{Segmentação e remoção de ruído de páginas web são etapas essenciais 
no processo de extração de dados estruturados. Identificar a região principal da 
página, eliminando o que não é importante (menus, anúncios, etc.), pode melhorar 
significativamente o desempenho do processo de extração. Para essa tarefa é proposto 
um novo algoritmo, totalmente automático, que utiliza uma sequência tag paths (TPS) 
como representação da página web. A TPS é composta por uma sequência de símbolos 
(string), cada um representando um tag path diferente. A técnica proposta procura 
por posições na TPS onde é possível dividi-la em duas regiões de tal forma que seus 
alfabetos não se intersectem, o que significa que as regiões têm conjuntos de tag 
paths completamente distintos e, portanto, são regiões diferentes da página. Os 
resultados mostram que o algoritmo é muito efetivo em identificar o conteúdo principal 
de vários sites, e melhora a precisão da extração, removendo resultados irrelevantes.}

\textAbstract{Web page segmentation and data cleaning are essential steps in structured web
data extraction. Identifying a web page main content region, removing what is
not important (menus, ads, etc.), can greatly improve the performance of the extraction
process. We propose, for this task, a novel and fully automatic algorithm that
uses a tag path sequence (TPS) representation of the web page. The TPS consists
of a sequence of symbols (string), each one representing a different tag path.
The proposed technique searches for positions in the TPS where it is possible to
split it in two regions where each region's alphabet do not intersect,
which means that they have completely different sets of tag paths and, thus, are
different regions. The results show that the algorithm is very effective in
identifying the main content block of several major websites, and improves the
precision of the extraction step by removing irrelevant results.
}

\paginaresumo
\paginaabstract

\chapter{Contextualização do problema}

Um passo fundamental da mineração de dados na web, inclusive em extração
estruturada, é a fase de limpeza que deve ocorrer antes da extração da
informação. Não se pode esperar bons resultados na fase de extração sem efetuar
a limpeza da página, removendo o ruído indesejado. Em \cite{Noisy03}, é
mencionado que apesar da importância desta tarefa, existem relativamente poucos
trabalhos nesta área e, de acordo com \cite{Editorial04}, o ruído existe nas
páginas pode prejudicar consideravelmente a mineração de dados na web.

Na extração estruturada, a maioria das técnicas existentes utilizam algum tipo
de reconhecimento de padrão para identificar os registros (como definido em
\cite{MDR03}) presentes nas páginas. O problema é que, em geral, apenas os
registros da região principal da página interessam, porém outras regiões da
página (menus, anúncios, etc.) freqüentemente contém padrões que também são
extraídos. Portanto, é importante efetuar a limpeza das páginas web antes de
extrair seu conteúdo estruturado.

Atualmente, alguns trabalhos na área de remoção de ruído e segmentação são
direcionados para indexação e $clustering$ de páginas ($i.e.$ é assumido que a
região principal é textual/não estruturada) como em
\cite{SiteOriented11,Noisy03} e, devido às diferenças intrínsecas entre
conteúdo estruturado e não estruturado, estas técnicas não podem ser
utilizadas para extração estruturada. As técnicas existem que podem ser
utilizadas para este fim requerem definições \textit{a priori} (\cite{vips03}),
ou treinamento prévio (\cite{Graph08}), ou então dependem de aspectos
específicos da linguagem HTML para funcionar, como em \cite{Entropy09}.

Este trabalho propõe e descreve um algoritmo para segmentação
e remoção de ruído de páginas web que utiliza a representação de seqüência de  
$tag$ $paths$ de uma página e que é simples e computacionalmente eficiente.
Trata-se de uma técnica geral de segmentação de páginas, utilizada aqui no
contexto de extração estruturada, levando em consideração o estilo e a estrutura
da página. As principais contribuições são as seguintes: 

\begin{itemize}
\item{Totalmente automática}: não necessita de treinamento nem intervenção
humana;
\item{Independente de domínio}: o único requisito é que a página possua
conteúdo estruturado, não importando a que domínio pertença;
\item{Independente da sintaxe HTML}: não existem regras associadas a $tags$
HTML específicas;
\item{Funciona em uma página}: é necessária apenas uma página como entrada, o
que constitui uma grande vantagem desta proposta, como discutido em \cite{Editorial04}.
\end{itemize}

Para avaliar o quão efetiva é a abordagem proposta neste trabalho, comparamos o
ruído de saída do MDR \cite{MDR03}, uma conhecida técnica de extração
estruturada, com e sem o uso da técnica proposta neste trabalho, obtendo-se uma
média de 77,03\% de ruído removido das páginas do conjunto de teste.

Este documento está organizado da seguinte maneira: no Capítulo \ref{ch:obj} são
apresentados os objetivos da pesquisa; no Capítulo \ref{ch:contr}
são enunciadas as principais contribuições do trabalho; no Capítulo \ref{ch:rev} é feita uma
revisão dos trabalhos relacionados, observando as limitações das propostas
existentes para solução do problema; no Capítulo \ref{ch:def} são enunciadas
as definições necessárias ao entendimento da proposta; no Capítulo \ref{ch:probform}
é descrita a proposta deste trabalho e as hipóteses nas quais a mesma se baseia;
no Capítulo \ref{ch:alg} são detalhados e ilustrados os algoritmos desenvolvidos
a partir da formulação do problema e das hipóteses; no Capítulo \ref{ch:met}
o método de avaliação dos resultados é detalhado; no Capítulo \ref{ch:res}
são apresentados os resultados obtidos e; no Capítulo \ref{ch:conclusion} uma conclusão final.

\chapter{Objetivos}\label{ch:obj}

O objetivo desta pesquisa é contribuir para área de extração estruturada de
dados da web, propondo uma nova técnica para segmentação e remoção de ruído de
páginas que possibilite incrementar a precisão dos métodos atuais, com o
mínimo de intervenção manual e o máximo de independência com relação a aspectos
da linguagem HTML e ao domínio da informação.

\chapter{Contribuições}\label{ch:contr}

A contribuição desta pesquisa é uma nova técnica de segmentação e remoção de
ruído de páginas web com as seguintes características:
\begin{itemize}
\item{Técnica completamente automática}: não é necessário treinamento nem
intervenção humana;
\item{Independente de domínio}: é necessário apenas que a página contenha
conteúdo estruturado, não importando a que domínio a informação pertence;
\item{Independente da sintaxe HTML}: não são utilizadas regras associadas a
$tags$ HTML;
\item{Necessita de apenas uma página}: não requer várias páginas do mesmo $site$
para funcionar, o que é um grande vantagem, como discutido em \cite{Editorial04}.
\end{itemize}

\chapter{Revisão bibliográfica}\label{ch:rev}

Existem muitos trabalhos propondo formas de segmentar páginas web e identificar
o que é conteúdo e o que é ruído nelas. Elas podem ser agrupadas em três
categorias: as que se baseiam no texto da página, as que se baseiam na árvore
DOM e as que utilizam informação visual.

\textbf{Técnicas baseadas no texto}. Em
\cite{Densiometric08,Boilerplate10,CETR10,BlockImp07,NonTemplate13} a
segmentação é realizada utilizando o conteúdo textual da página web. O foco
destes trabalhos, entretanto, não é em extração estruturada, mas em indexação e
$clustering$ de páginas. A maioria dos trabalhos existentes sobre segmentação e
remoção de ruído são direcionados para este tipo de aplicação. 

\textbf{Técnicas baseadas na árvore DOM}. Em
\cite{Entropy09,SiteOriented11,Graph08,Noisy03,Entropy12} a segmentação é
realizada utilizando a árvore DOM da página, portanto, estas técnicas levam em
consideração a estrutura da página web. Entretanto, \cite{SiteOriented11} e
\cite{Noisy03} necessitam de várias páginas de um mesmo $site$, \cite{Graph08}
propõe um $framework$ de treinamento que necessita de dados catalogados
manualmente para funcionar e \cite{Entropy12} necessita de uma base de dados de
termos associados a ``papéis semânticos'' para poder detectar regiões de dados.

\textbf{Técnicas baseadas em informação visual}.
Além das técnicas baseadas no texto e na árvore DOM da página, existem também
aquelas que utilizam informação visual, como \cite{vips03,vide09,viper05}. Todas
dependem de um renderizador de um navegador web para obter as informações
visuais das páginas, o que pode ser computacionalmente dispendioso, e, além
disso, \cite{vips03} é baseado em um conjunto de regras heurísticas, cada uma
aplicada a $tags$ HTML específicas. Técnicas que dependem fortemente de aspectos
específicos da linguagem HTML tem a séria desvantagem de serem afetadas por
mudanças nas práticas de desenvolvimento assim como mudanças na sintaxe da
linguagem.

A representação da página web utilizada nesta pesquisa (seqüência de
$tag$ $paths$) foi anteriormente empregada em \cite{TPC09} e
\cite{SuffixTree12}, embora em ambos os casos para a extração e não para segmentação e remoção de
ruído. Estes dois trabalhos são citados aqui apenas para ilustrar que, de acordo
com seus resultados, esta representação, assim como a árvore DOM, também é capaz
de expor a estrutura da página web e, portanto, serve para os propósitos desta pesquisa.
A Figura \ref{fig:ex1} ilustra como o código HTML de uma página web é convertido para
representação de $tag$ $paths$.

\begin{figure}[h]
  \caption{Exemplo de uma seqüência de \textit{tag paths} contruída a
  partir do código HTML.}
  \label{fig:ex1}
  \centering
    \includegraphics[scale=0.39]{example1.jpg}
\end{figure}

\chapter{Definições}\label{ch:def}

Neste capítulo são apresentados os conceitos e definições necessários para o
entendimento do problema e do algoritmo proposto. As definições são
exemplificadas com o auxílio da Figura \ref{fig:ex1}.
\begin{definition}[\textbf{Árvore DOM}]\label{def:domtree}
A árvore DOM é uma estrutura hierárquica, derivada do código HTML, que
representa a página web.
\end{definition}
Na Figura \ref{fig:ex1} um pequeno trecho de código é utilizado para ilustrar a
árvore DOM e as próximas definições.

\begin{definition}[\textbf{$Tag$ $paths$}]\label{def:tp} Um tag path (TP) é uma
string descrevendo o caminho absoluto, a partir da raiz da árvore DOM, até um
dado nó da árvore. Seja ``i'' a posição em profundidade do nó $``n_i"$ na
árvore, então é dito que $TP_i$ é a string descrevendo o caminho, a partir da
raiz da árvore DOM, até o nó $``n_i"$.
\end{definition}
Na Figura \ref{fig:ex1}, o caminho absoluto $TP_4$ do nó $body$ até a
célula da tabela $td_4$ é $TP_4=``body/table/tr/td"$. 

\begin{definition}[\textbf{Seqüência de \textit{tag paths}}]\label{def:tps}
Uma seqüência de tag paths (TPS) de uma árvore DOM com ``n'' nós é definida como
sendo a seqüência ordenada $TPS[1..n] = (TP_1,TP_2,TP_3, ...,TP_{n-1},TP_n)$
onde dois tag paths $TP_i$ e $TP_j$, com $i\neq{}j$, são considerados iguais
apenas se seus caminhos e suas definições de estilo são iguais, caso
contrário eles são considerados diferentes.
\end{definition}
Esta é a mesma definição utilizada em \cite{SuffixTree12}, onde cada $tag$
$path$ é representado na seqüência por um símbolo, exceto que neste trabalho
foram incorporadas as definições de estilo das $tags$. Na Figura \ref{fig:ex1}
é mostrada a $TPS$ de um dado trecho de código HTML, onde cada $TP$ recebe um
código, produzindo $TPS = (1,2,3,4,4,3,4,4)$.

\begin{definition}[\textbf{Alfabeto da TPS}]\label{def:alpha} Seja $\Sigma_a$ o
conjunto contendo todos os símbolos de uma dada seqüência $TPS_a$ de tamanho
``n'', $\Sigma_a$ é dito o alfabeto de $TPS_a$ definido como $\Sigma_a = \{\alpha |
\exists{TPS_a[i]}=\alpha \wedge 1 \leq i \leq n\}$, onde $\alpha$ é um símbolo
do alfabeto.
\end{definition}
Informalmente, o alfabeto contém todos os símbolos distintos de uma determinada
seqüência. Na Figura \ref{fig:ex1}, a $TPS$ é formada apenas pelos símbolos
``1'',``2'', ``3'' e ``4'', então seu alfabeto é $\Sigma=\{1,2,3,4\}$.

\begin{definition}[\textbf{Conjunto de freqüências de tag paths}]\label{def:tpfs}
Seja $(s,f)$ um par ordenado onde ``s'' é um símbolo de um alfabeto de uma
determinada TPS e ``f'' é o número de vezes que ``s'' ocorre na TPS, então o conjunto de
freqüências de tag paths é definido como sendo o conjunto de todos os pares 
$(s,f)$ possíveis de uma TPS. Seja $FS =
\{(s_1,f_{s1}),(s_2,f_{s2}),(s_3,f_{s3})$
\\ $,\ldots,(s_{n-1},f_{sn-1}),(s_n,f_{sn})\}$, onde ``n" é o tamanho da TPS.
\end{definition}
Na Figura \ref{fig:ex1}, o símbolo ``1'' ocorre uma vez na seqüência, o símbolo
``2'' ocorre uma vez também, o símbolo ``3'' duas vezes e o símbolo ``4'' quatro
vezes, então para esta seqüência $FS = \{(1,1), (2,1), (3,2), (4,4)\}$. O
conjunto $FS$ é o mapeamento de todos os símbolos de uma seqüência para suas
respectivas freqüências.

\begin{definition}[\textbf{Limiares de freqüência}]\label{def:ft}
Dada uma $TPS_a$ com alfabeto $\Sigma_a$ e conjunto de freqüências $FS_a$, os
limiares de freqüência $FT_a$ são definidos como sendo o conjunto ordenado
contendo apenas as freqüências de $FS_a$. Seja $FT_a = \{f|\exists{(s,f)} \wedge
(s,f) \in FS_a \wedge s \in \Sigma_a \}$, onde ``f'' é uma freqüência e ``s'' é
o símbolo correspondente do alfabeto $\Sigma_a$.
\end{definition}
Na TPS exibida na Figura \ref{fig:ex1}, o conjunto de freqüências é $FS=\{(1,1),
\\ (2,1), (3,2), (4,4)\}$, neste caso o conjunto dos limiares de freqüência é
igual a $FT=\{1,2,4\}$, pois os símbolos ``1'' e ``2'' tem freqüência igual a
\textbf{1}, o símbolo ``3'' tem freqüência igual a \textit{2} e o símbolo ``4''
tem freqüência igual a \textit{4}. O conjunto $FT$ é necessário para filtrar os
símbolos da TPS. Se $FT=\{1,2,4\}$, então não faria sentido filtrar símbolos com
freqüência igual a \textit{3}, pois não existem símbolos na seqüência com esta
freqüência.

\begin{definition}[\textbf{Região}]\label{def:region} 
Seja uma TPS a concatenação de duas outras seqüências $TPS=TPS_a . TPS_b$, então
é dito que $TPS_a$ e $TPS_b$ são regiões de $TPS$, sse $\Sigma_a \cap \Sigma_b =
\emptyset$.
\end{definition}
Na Figura \ref{fig:ex1} a TPS pode ser dividida em duas subseqüências
$TPS_a=TPS[1..2]=(1,2)$ e $TPS_b=TPS[3..8]=(3,4,4,3,4,4)$, com alfabetos
$\Sigma_a=\{1,2\}$ e $\Sigma_b=\{3,4\}$, então $TPS_a$ a $TPS_b$
são regiões disintitas de $TPS$, pois $\Sigma_a \cap \Sigma_b=\emptyset$.

\chapter{Proposta}\label{ch:probform}

Nesta pesquisa é proposta uma nova técnica para segmentação das regiões de uma
página web e identificação da região principal. A técnica proposta é simples,
computacionalmente eficiente e leva em consideração a estrutura e os estilos de
formatação das páginas. A página web é representada na forma de $tag$ $paths$. É
um método de segmentação genérico, utilizado aqui no contexto da extração estruturada.

O algoritmo proposto é formulado a partir de duas hipóteses:
\begin{enumerate}
  \item\label{ass:1} diferentes regiões de uma página web são descritas por
  diferentes $tag$ $paths$;
  \item\label{ass:2} em \textit{sites} com conteúdo estruturado, a região
  principal é mais densa/maior que as demais regiões (menus, anúncios, text, etc.). 
\end{enumerate}

A formulação da Hipótese \ref{ass:1} vem da observação de que as várias
regiões de uma página $web$ são diferentes ramificações da árvore DOM e essas regiões
são descritas ou utilizando $tags$ distintas para cada região ou, se as $tags$
forem semelhantes, com diferentes estilos, para que cada região possa ser
facilmente distingüida pelo usuário/leitor da página. Se todas as regiões de uma
página fossem parecidas, seria mais difícil, para o usuário, distingüi-las.
Então, pela Definição \ref{def:tps}, pode-se notar que os símbolos utilizadas
em cada região de uma página web deveriam ser diferentes, e então deve ser possível
segmentar a página como na Definição \ref{def:region}.

A Hipótese \ref{ass:2} vem do contexto no qual a segmentação proposta neste
trabalho é aplicada (i.e. extração estruturada). Como se está considerando
apenas páginas com registros, e para descrever a estrutura desses registros são
necessários nós da árvore DOM, então são necessários mais nós para descrever
conteúdo estruturado do que é necessário para conteúdo não estruturado (e.g.
texto). Portanto, é razoável assumir que, para páginas que contém registros, a
região principal da página é a maior de todas (i.e. a que contém mais nós).

Utilizando as definições do Capítulo \ref{ch:def} e as hipóteses acima, podemos
formular o problema de segmentação de páginas e identificação da região
principal como: \textbf{``encontrar a maior \textit{região} da $TPS$ de uma
página $web$ que possua um $alfabeto$ que não se intersecte com o $alfabeto$ de
outras \textit{regiões} menores''}.

Um detalhe \textbf{crucial} que deve ser levado em consideração, é que podem
existir $tag$ $paths$ em uma página que representam divisões estruturais da
página (i.e. formatação visual da página). Estes $tag$ $paths$, se forem
divisões, irão ocorrer ao longo de toda a $TPS$, impedindo a identificação das
regiões, pois sempre haverá uma intersecção dos alfabetos. Para remover este
``ruído'' da seqüência, podemos filtrar os símbolos com freqüências mais baixas,
sem prejudicar o processo de segmentação, pois os símbolos com freqüências mais
altas continuam sendo considerados.

Apenas para ilustrar, é apresentado um exemplo de uma página $web$ que inicia e
termina com o mesmo $tag$ $path$ (``/body/br'') e tem três regiões
delimitadas, também, pelo mesmo $tag$ $path$ (``/body/div''). Assumindo que
diferentes $tag$ $paths$ são utilizados para descrever cada região, sem filtrar
os símbolos de baixa freqüência da $TPS$, não seria possível identificar essas
regiões.

\begin{itemize}
\item{código HTML}
\begin{small} 
\begin{verbatim}
<body>
<br>
<div>
  <span class='region1'>...</span>
  ...
  <span class='region1'>...</span>
</div> 
<div>
  <span class='region2'>...</span>
  ...
  <span class='region2'>...</span>
</div>
<div>
  <span class='region3'>...</span>
  ...
  <span class='region3'>...</span>
</div>
<br>
</body>
\end{verbatim}
\end{small}
\item{TPS}
\begin{verbatim}
TPS = (1,2,3,4,...,4,3,5,...,5,3,6,...,6,2)
\end{verbatim}

Os símbolos $2$ e $3$ ocorrem ao longo de toda a $TPS$.

\item{TPS filtrada}
\begin{verbatim}
TPS = ( , , ,4,...,4, ,5,...,5, ,6,...,6, )
\end{verbatim}

Apenas símbolos com freqüência maior que $3$ são considerados no processo de
segmentação. Agora é possível dividir a $TPS$ em regiões.

\end{itemize}

\chapter{Algoritmos}\label{ch:alg}

Neste capítulo são apresentados os algoritmos desenvolvidos para tratar o problema
definido no Capítulo \ref{ch:probform}. Quais sejam: 

\begin{itemize}
  \item {}\textit{\textbf{tagPathSequenceFilter()}}. É o algoritmo principal, que
   recebe um arquivo HTML como entrada e retorna uma árvore DOM, após a poda, com o conteúdo da região principal;
  \item {}\textit{\textbf{convertToSeq()}}. Converte a árvore DOM da página web em uma $TPS$;
  \item {}\textit{\textbf{searchRegion()}}. A busca, propriamente dita, pela região principal da $TPS$;
  \item {}\textit{\textbf{filter()}}. Filtra um alfabeto, removendo 
   símbolos de menor freqüência, tornando o algoritmo de busca mais robusto e resistente ao ruído;
  \item {}\textit{\textbf{pruneDOMTree()}}. Poda da árvore DOM original, deixando apenas o conteúdo
   da região principal encontrado com $searchRegion$, mantendo a estrutura original do documento.
\end{itemize}

\section{$tagPathSequenceFilter()$ Algorithm}

\begin{algorithm}[H]
\caption{Filters out noise from a web page}
\label{alg:tpsfilter}
\textbf{Input:} $inputFile$ - an HTML file \\
\textbf{Output:} pruned $inputFile$'s DOM tree
\begin{algorithmic}[1]
\Procedure{tagPathSequenceFilter}{$inputFile$}
\State $DOMTree \leftarrow parseHTML(inputFile)$
\State $convertToSeq(DOMTree.body,$`` ''$,TPS)$
\State $searchRegion(TPS)$
\State $pruneDOMTree(DOMTree.body,TPS)$
\State return $DOMTree$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $tagPathSequenceFilter()$ no Algoritmo \ref{alg:tpsfilter} retorna
a região principal de $inputFile$. O procedimento $parseHTML()$, na 
Linha $2$, converte o código HTML em uma árvore DOM;
$convertToSeq()$, na Linha $3$, converte a árvore DOM em uma TPS; o
procedimento $searchRegion()$, na Linha $4$, procura recursivamente pela maior 
região da TPS que possui um alfabeto único e, finalmente; $pruneDOMTree()$, na
Linha $5$, retira da árvore DOM (poda) todos os nós que não estão na TPS
resultante, preservando a estrutura do documento retornado na Linha $6$.

Aabixo são detalhados os algoritmos $convertToSeq()$, $searchRegion()$,
$filter()$ e $pruneDOMTree()$. O algoritmo $parseHTML()$ não faz parte do escopo 
deste trabalho, portanto não será discutido aqui.


\section{Algoritmo convertToSeq()}

\begin{algorithm}[H]
\caption{Converts a DOM tree to a tag path sequence representation}
\label{alg:tree2seq}
\textbf{Input:} $node$ - a DOM tree node, initially the root \\
\textbf{Input:} $tp$ - the previous tag path, initially empty \\
\textbf{Input:} $TPS$ - the $TPS$ built from the DOM tree,
initially empty \\
\textbf{Output:} the $TPS$ for the given DOM tree stored in $TPS$
\begin{algorithmic}[1]
\Procedure{convertToSeq}{node,tp,TPS by ref.}
\State $tp \leftarrow concat(tp,$``/''$,node.tag,node.style)$
\If {$tp \ni tagPathMap$}
\State $tagPathMap \leftarrow tagPathMap + \{tp\}$
\State $tagPathMap[tp].code \leftarrow tagPathMap.size$
\EndIf
\State $TPS \leftarrow concat(TPS,tagPathMap[tp].code);$
\For {each $child$ of $node$}
\State $convertToSeq(child,tp,TPS)$
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $convertToSeq()$ no Algoritmo \ref{alg:tree2seq}
converte uma página web de sua representação em árvore DOM para uma representação em formato TPS,
percorrendo a árvore DOM em profundidade. Inicialmente é chamado no
Algoritmo \ref{alg:tpsfilter} com um $tagPath$ vazio como parametro, que 
representa a $string$ do tag path anterior (i.e. da chamada recursiva anterior).
Na Linha $2$, o $tag$ $path$ anterior é concatenado com a $tag$ atual e com
suas definições de estilo, com objetivo de distinguir caminhos repetidos com
estilos diferentes; na Linha $3$, é verificado se o $tag$ $path$ atual já foi
visto antes pelo algoritmo ou não ($tagPathMap$ is used for this purpose) e, se
não, na Linha $4$, ele é inserido no conjunto $tagPathMap$ e um novo código é
atribuído à ele na Linha $5$, como definido na Definição \ref{def:tps}; na Linha
$7$, o código do tag path é concatenado ao final da sequencia e, finalmente, o
procedimento e invocado recursivamente na Linha $9$ para cara filho de $node$.

\section{Algoritmo searchRegion()}

Este é o algoritmo central, já que é responsável por encontrar a região
principal, portanto incluímos uma ilustração, na Figura \ref{fig:alg}, para
facilitar seu entendimento. A Figura \ref{fig:alg} apresenta uma ilustração da
execução do algoritmo na qual foi omitido o procedimento de filtragem dos
símbolos com baixas freqüências para maior clareza, pois a intenção é passar a
idéia principal. Os detalhes e minúcias podem ser verificados na listagem do
algoritmo e na sua descrição.

\begin{figure}[h]
  \caption{Ilustração da execução do algoritmo.}
  \label{fig:alg}
  \centering
    \includegraphics[scale=0.48]{alg-pb.jpg}
\end{figure}


\begin{tiny}
\begin{algorithm}[H]
\caption{Busca regiões da TPS com alfabetos diferentes}
\label{alg:searchreg}
\textbf{Entrada:} $tagPathSequence$ - a $TPS$ de uma determinada página web \\
\textbf{Saída:} a região principal da $TPS$, armazenada em $tagPathSequence$
\end{algorithm}
\begin{algorithmic}[1]
\Procedure{searchRegion}{$tagPathSequence[1..n]$ by reference}
\State $alphabet \leftarrow \emptyset$
\State $t \leftarrow 0$
\For {$i \leftarrow 1..n$}
\State $symbol \leftarrow tagPathSequence[i]$
\If {$symbol \ni alphabet$}
\State $alphabet \leftarrow alphabet \cup \{symbol\}$
\State $symbolCount[symbol] \leftarrow 0$
\EndIf
\State $symbolCount[symbol] \leftarrow symbolCount[symbol]+1$
\EndFor
\State $thresholds \leftarrow Ordered Set Of Frequencies(symbolCount)$
\State $regionFound \leftarrow false$
\While {not $regionFound$}
\State $t \leftarrow t + 1$
\State $currentAlphabet \leftarrow filterAlphabet(alphabet,symbolCount,thresholds[t])$
\If {$currentAlphabet.size < 2$}
\State $break$
\EndIf
\State $currentSymbolCount \leftarrow symbolCount$
\State $regionAlphabet \leftarrow \emptyset$
\For {$i \leftarrow 1..n$}
\State $symbol \leftarrow tagPathSequence[i]$
\If {$symbol \in currentAlphabet$}
\State $regionAlphabet \leftarrow regionAlphabet \cup \{symbol\}$
\State $currentSymbolCount[symbol] \leftarrow currentSymbolCount[symbol]-1$
\If {$currentSymbolCount[symbol] = 0$}
\State $currentAlphabet \leftarrow currentAlphabet - \{symbol\}$
\If {$currentAlphabet \cap regionAlphabet = \emptyset$}
\If {$currentAlphabet \neq \emptyset$ and $(n-2*i)/n > 0.20$}
\State $regionFound \leftarrow true$
\EndIf
\State $break$
\EndIf
\EndIf
\EndIf
\EndFor
\EndWhile
\If {$regionFound$}
\If {$i < n/2$}
\State $tagPathSequence \leftarrow tagPathSequence[i+1..n]$
\Else
\State $tagPathSequence \leftarrow tagPathSequence[1..i]$
\EndIf
\State $searchRegion(tagPathSequence)$
\EndIf
\EndProcedure
\end{algorithmic}
%\end{algorithm}
\end{tiny}

O procedimeto $searchRegion()$ no Algoritmo \ref{alg:searchreg}
calcula o alfabeto da TPS e as freqüências correspondentes de cada símbolo nas
Linhas $4$ a $11$; na Linha $12$, os limiares de freqüência, da Definição
\ref{def:ft}, são calculados; nas Linhas $14$ a $38$ é realizada a busca por uma
posição na seqüência onde é possível dividí-la (i.e. onde existe uma região); na
Linha $15$ os limiares de freqüência são iterados; na Linha $16$ o alfabeto da
TPS, da Definição \ref{def:alpha}, é filtrado, como descrito no Capítulo
\ref{ch:probform}; na Linha $22$ a TPS é iterada; na Linha $25$ o alfabeto da região
é calculado e; nas Linhas $27$ a $35$ é verificado se existe uma intersecção
entre os alfabetos das duas partes da TPS (uma intersecção vazia indica que uma
possível região foi encontrada, como na Definição \ref{def:region}). A região
encontrada só é considerada válida se for ao menos 20\% maior que o restante da
seqüência, caso contrário o filtro de freqüência é iterado e a busca prossegue.
Este percentual é, na verdade, um parâmetro com o propósito de evitar relatar
uma região sob condições ambíguas (nos experimentos foi utilizado o valor de
20\%); finalmente nas Linhas $39$ a $46$ a TPS é dividida se uma região foi
encontrada, chamando o procedimento $searchRegion()$ recursivamente na Linha
$45$.

\section{Algoritmo filter()}

\begin{algorithm}[H]
\caption{Filters out symbols with lower frequencies from the alphabet}
\label{alg:filteralpha}
\textbf{Input:} $\Sigma$ - the alphabet ot be filtered \\
\textbf{Input:} $symCount$ - the tag path frequency set ($FS$) of $\Sigma$ \\ 
\textbf{Input:} $threshold$ - a frequency threshold \\
\textbf{Output:} a filtered alphabet
\begin{algorithmic}[1]
\Procedure{filter}{$\Sigma$, symCount, threshold}
\State $filtered\Sigma \leftarrow \emptyset$
\For {$i \leftarrow 1..n$}
\If {$symCount[\Sigma[i]] \geq threshold$}
\State $filtered\Sigma \leftarrow filtered\Sigma \cup \{\Sigma[i]\}$
\EndIf
\EndFor
\State return $filtered\Sigma$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $filter()$ no Algoritmo \ref{alg:filteralpha}
remove do alfabeto $\Sigma$, todo símbolo com frequencia inferior a $threshold$.
Nas Linhas $3$ to $7$ apenas os símbolos com frequencia maior ou igual a
$threshold$ são inseridos no conjunto resultante. O resultado de $filter()$ é 
utilizado no Algoritmo \ref{alg:searchreg}, Linha $24$, onde apenas os
símbolos em $filtered$ são considerados durante a busca pela região principal.

\section{Algoritmo pruneDOMTree()}

\begin{algorithm}[H]
\caption{Prune from the DOM tree the nodes that are not in sequence}
\label{alg:prune}
\textbf{Input:} $node$ - a DOM tree node, initially the root \\
\textbf{Input:} $seq$ - the TPS that has to remain in the DOM tree \\
\textbf{Output:} the DOM tree pointed by $node$ pruned
\begin{algorithmic}[1]
\Procedure{pruneDOMTree}{$node$ by ref.,$seq$}
\For {each child of node}
\If {$pruneDOMTree(child,seq) = true$}
\State remove child from $node$
\EndIf
\EndFor
\If {$node \ni seq$ and $node.childCount = 0$}
\State return $true$
\EndIf
\State return $false$
\EndProcedure
\end{algorithmic}
\end{algorithm}

O procedimento $pruneDOMTree()$ no Algoritmo \ref{alg:prune}, percorre a árvore
DOM, em produndidade, removendo os nós que não pertencem a $sequence$. Na
Linha $3$ a árvore DOM é percorrida; nas Linhas $7$ a $9$ é decidido se o nó
$node$ deve ser removido ou não.

Um nó é removido da árvore, apenas se ele não está em $sequence$ \textbf{e} não
tem nenhum filho. Desta forma é possível manter a estrutura da árvore
remanescente intacta, evitando afetar, negativamente, a etapa subsequente de
extração estruturada.

\section{Complexidade do Algoritmo}

As for the algorithm's complexity, if we observe Lines $14$ and $22$ of the
procedure $searchRegion()$, we can see that the loop in Line $14$ iterates the
frequency thresholds until a region is found and Line $22$ iterates the TPS
(filtered at given frequency threshold) also until a region is found and, if
so, the reported region is recursively processed.

In the worst case, when the alphabet intersection is empty only in the last
index of the TPS, the complexity would be at most $O(n^2 f)$, where $n$ is the
length of the TPS and $f$ is the size of the set $thresholds$. In practice, the
size of the set $thresholds$ is much smaller than the length of the TPS, so we
can say the complexity approximates $O(n^2)$ as shown in Equation \ref{eq:w}.
  \begin{equation}\label{eq:w}
  T(n) = T(n - 1) + \Theta(n) \Longrightarrow \sum_{i=1}^{n}
  i = \frac{n(n+1)}{2} = O(n^2)
  \end{equation} 

In average, if the TPS gets split in half, the complexity would be $O(n)$ as in
Equation \ref{eq:a}.
  \begin{equation}\label{eq:a}
  T(n) = T(n/2) + \Theta(n) \Longrightarrow \sum_{i=1}^{log_{2}^{n}}
  \frac{n}{2^i} = n - 1 = O(n)
  \end{equation} 

In the best case, TPS is split in the first index, yielding $O(n)$ as in
Equation \ref{eq:b}.
  \begin{equation}\label{eq:b}
  T(n) = T(n - 1) + \Theta(1) \Longrightarrow \sum_{i=1}^{n} 1 = n = O(n)
  \end{equation} 

In real world scenarios, as we have seen while doing the evaluation of the
algorithm, the sequences get split approximately four of five times until they
can not be split no more. So we can say that in real cases, the algorithm executes in
$O(in)$ time, where $n$ is the size of the $TPS$ and $i$ is the number of times the 
sequence gets split, which we can consider as a small constant, in this case, and 
say that it runs in $O(n)$.

\chapter{Método}\label{ch:met}

Para avaliar os resultados da técnica proposta, será utilizado o algoritmo MDR
(\cite{MDR03}), uma conhecida técnica de extração estruturada. O resultado da
extração pura (apenas MDR) será considerado como $baseline$ para comparação
com o resultado da extração combinada com a técnica proposta de remoção
de ruído.

A avaliação dos resultados é constituída das seguintes etapas:
\begin{itemize}
  \item escolha de uma técnica de extração automática de dados estruturados;
  \item definição de um conjunto de páginas para teste;
  \item aplicação da técnica de extração no conjunto de teste, documentando os
  resultados obtidos como \textit{target} (registro de interesse) ou
  \textit{noise} (ruído);
  \item nova aplicação da técnica de extração no mesmo conjunto de teste,
  mas desta vez filtrando-o antes, utilizando a técnica proposta neste trabalho,
  documentando os resultados da mesma maneira;
  \item comparação de ambos os resultados e medição do aumento/redução da
  precisão (remoção de ruído).
\end{itemize}

Para primeira etapa foi escolhido o MDR para realizar a extração, pois esta
técnica possui diversas implementações disponíveis, vários trabalhos publicados
a respeito, funciona em uma única página e os resultados da extração são
razoáveis para avaliar a técnica proposta neste trabalho. Outras técnicas
poderiam ser utilizadas, se estivessem disponíveis, desde que não necessitassem
de várias páginas HTML como entrada (e.g. $RoadRunner$ \cite{RRunner01}).


\chapter{Resultados Experimentais}\label{sec:results}

In this section we describe and discuss the results of our experiments and how
they are presented. To obtain the results presented in Section \ref{sec:res}, 
we have implemented the algorithm and tested it against some commercial and 
institutional web sites. In Section \ref{sec:clar} we detail one of the results 
presented, as an example, to clarify how they are compiled in Table \ref{table:results}, appendix \ref{app:a}.

\section{Experimental setup}\label{sec:clar}

We considered the extraction results of MDR alone as our baseline to be compared with
the results obtained by the combined use of TPS filtering and MDR, as illustrated in Figure \ref{fig:method}.

When applying both approaches (MDR and TPS filtering + MDR) to a result page of ``YouTube'' (\#38) web site, the following 
results are obtained:

\begin{itemize}
\item{}raw web page (i.e. the original page, without TPS filtering)
\begin{itemize}
\item{}DOM tree processed: $1424$ nodes;
\item{}MDR results: $82$ records total ( $62$ noise / $20$ targets );
\end{itemize}

\item{}pruned web page (i.e. the web page after TPS filtering)
\begin{itemize}
\item{}DOM tree processed: $674$ nodes, size $47,33\%$ of the original page, reduction of $(-52,67\%)$
\item{}MDR results: $20$ records total ( $0$ noise / $20$ targets ), noise
removed $100\%$
\end{itemize}
\end{itemize}

In this result, we can see an improvement in the extraction of
records as well as a considerable reduction in the size of the DOM tree to be processed. 
A percentage of $52.67\%$ of the DOM tree was pruned without losing the
target records in the process. Everything pruned out of the DOM tree was noise. 
Figure \ref{fig:ex2} illustrates the web page and the main content region and in 
Figure \ref{fig:tps} we show the respective TPS and main content region detected by our algorithm.

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.26]{example2-gs.jpg}
  \caption{A page from the YouTube web site and the main content region delimited.}
  \label{fig:ex2}
\end{figure}

\begin{figure}[H]
  \centering
    \includegraphics[scale=0.40]{tps.jpg}
  \caption{The TPS from a YouTube web page and the detected main content region.}
  \label{fig:tps}
\end{figure}


Without applying TPS filtering, we get $82$ records in total and, since we know
there are $20$ target records in this page, we can consider the value of $62$ 
records to be $100\%$ of noise to be removed. When we use TPS filtering, this 
time we get only the $20$ target records in the extraction phase, yielding a 
precision of $100\%$, that means all noise was removed in this case. We calculate 
the percentage of noise removed to be
\begin{equation}\label{eq:noise}
NoiseRemoved=1-\frac{\#Rec_{totalTPS}-\#Rec_{targetTPS}}{\#Rec_{total}-\#Rec_{target}}
\end{equation}
Where $\#Rec_{total}$ and $\#Rec_{target}$ are the total number of records and the
number of target records, respectively, from the original web page, and $\#Rec_{totalTPS}$ 
and $\#Rec_{targetTPS}$ are the total number of records and the number of target records, 
respectively, from the filtered web page.

\section{Results}\label{sec:res}

In Table \ref{table:results} we present, in the first three columns, the size of the DOM tree
processed by MDR and the reduction obtained after filtering. The column ``Content Present''
indicates whether or not the filtering process preserved the main content region. The next 
four columns are the results of MDR alone and combined with TPS filtering, showing the total 
records and target records extracted for both approaches. The last column shows the percentage 
of noise removed, calculated using Equation \ref{eq:noise}.  

As we can see in Table \ref{table:results}, the total of column ``Content
Present'' indicates that the algorithm has worked in 84.21\% of the sites and it
has removed, for this test set, an average of 89.02\% of all noise present in the
data, as shown by the average of column ``Noise rem.". We consider these to be
good results.

The average DOM tree reduction of 47.93\% is an interesting result. First,
because that means almost half the DOM tree is noise in average. Second, because
this number matches the value reported, independently, by \cite{Volume05} as
page template size (about $50\%$), corroborating with literature work.

An interesting situation we can see in Table \ref{table:results} is the result
for the site ``Build'' (\#8). Without filtering, MDR has reported a total of $117$
records, included $12$ target records. After filtering is applied, a total of
$69$ records are reported, none of them targets, all noise. So, after filtering, if
we had reported the complementary DOM tree instead, we would get a result of
$48$ records in total ($117-69=48$), included here the $12$ target records, which
is a good result since it gives us a $65.71\%$ of noise removal. We can 
deduce from this, that the segmentation has worked just fine for this site, only 
the main content was not correctly identified, since it's relatively small.

\section{Results Discussion}

There are three main situations where the algorithm needs to be improved but, fortunately, 
only two of these can lead to loss of main content (content removal). In Table
\ref{table:results}, column ``Content Present'', these two situations account for 15.79\% 
of the cases, where the content region was removed in the filtering process.

\begin{enumerate}
  \item{\textbf{templates too homogeneous}}. These are pages with little difference between
  the regions. In this case, using this technique, there is
  not much to do. We simply do not have enough information to work with,
  since the entire page looks alike. We do not lose the target records, but the
  amount of noise removed is very low;
  \item{\textbf{templates too heterogeneous}}. These are pages where the main content is 
  subdivided in more than one region. In this case, the main region gets split
  over and over, and only the largest part passes through the filter (and it
  might be noise). We propose a way to work around this problem later in this Section;
  \item{\textbf{pages where the main content is smaller than the rest}}. That is a
  consequence of the second assumption we made in Section \ref{sec:probform}:
  ``the main region is denser/bigger than the rest''. In this case, noise will
  always be reported as content. The same proposal made for the former situation 
  can be used to deal with this one as well.
\end{enumerate}

In the case of heterogeneous templates, TPS filtering can still be used if we
make some slight modifications in the algorithm. One such case of heterogeneous
template are ``news sites'', where every record has a different structure, but
they are all records from the same domain (i.e. they belong to the same entity). 
In this specific situation, TPS segmentation could be used to split the page in 
several parts, and a semantic approach used to combine the regions, reporting the
main content as a set of regions instead of only one.

For situation described for the site ``Build'' (that happened for five other
sites we tested), when the content region is smaller than the rest, we
could apply a semantic technique to check whether or not the desired content is
present in the reported region, if not, report the complementary DOM tree  (i.e.
inverse the pruning) instead. The main algorithm would look like this:

\begin{algorithm}[H]
\caption{Filters out noise from a web page}
\label{alg:tpsfilter2}
\textbf{Input:} $inputFile$ - an HTML file \\
\textbf{Output:} pruned $inputFile$'s DOM tree
\begin{algorithmic}[1]
\Procedure{tagPathSequenceFilter}{$inputFile$}
\State $DOMTree \leftarrow parseHTML(inputFile)$
\State $convertToSeq(DOMTree.body,$`` ''$,TPS)$
\State $backupTPS \leftarrow TPS$
\State $searchRegion(TPS)$
\If {$TPS$ not $content$}
\State $TPS = backupTPS - TPS$   
\EndIf
\State $pruneDOMTree(DOMTree.body,TPS)$
\State return $DOMTree$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:tpsfilter2} is the same as Algorithm \ref{alg:tpsfilter}, except for Line $6$
where it checks if the main content is present in the reported region and, if not, we report
the complementary sequence instead (Line $7$), ensuring the presence of the main content. 

\chapter{Resultados}\label{ch:res}

Na Tabela \ref{table:results} são apresentados, nas primeiras três colunas, o
tamanho da árvore DOM processada pelo MDR (i.e. original) e a redução obtida
após remoção do ruído (i.e. podado). A coluna ``Conteúdo principal'' indica se o
processo de remoção de ruído preservou a região principal ou não. As próximas
quatro colunas são os resultados obtidos com o MDR sem e com remoção de ruído
respectivamente, exibindo o total geral de registros extraídos e o total de
registros de interesse ($target$) extraídos para ambas abordagens. A última
coluna mostra o percentual de ruído removido, calculado conforme a Equação
\ref{eq:noise}.

\begin{equation}\label{eq:noise}
NoiseRemoved=1-\frac{NumRec_{totalTPS}-NumRec_{targetTPS}}{NumRec_{total}-NumRec_{target}}
\end{equation}

\begin{table}[h]
\caption{Resultados}
\label{table:results}
\centering
\begin{tiny}
\begin{tabular}{l r r r c r r r r r}

\hline\hline
\multicolumn{5}{c}{} & \multicolumn{4}{c}{MDR (\# registros)} &  (eq. \ref
{eq:noise})\\
& \multicolumn{3}{c}{Tam. DOM (\# nós)} & Cont. & \multicolumn{2}{c}{Orig.} &
\multicolumn{2}{c}{Podado} & Ruído\\
Site & Orig. & Podado & Red. & princ. & Tot & Tgt & Tot & Tgt & rem. \\
\hline
acm.org & 601 & 340 & -43.43\% & Yes & 61 & 10 & 16 & 10 & 88.24\% \\
amazon.com & 3309 & 1054 & -68.15\% & Yes & 368 & 15 & 27 & 15 & 96.60\% \\
americanas.com.br & 2660 & 710 & -73.31\% & Yes & 211 & 20 & 20 & 20 & 100.00\% \\
bestbuy.com & 3632 & 1425 & -60.77\% & Yes & 299 & 15 & 15 & 15 & 100.00\% \\
bondfaro.com.br & 3897 & 3069 & -21.25\% & Yes & 231 & 28 & 178 & 28 & 26.11\% \\
bradesco.com.br & 1913 & 1113 & -41.82\% & Yes & 164 & 10 & 93 & 10 & 46.10\% \\
buscape.com.br & 3608 & 3514 & -2.61\% & Yes & 279 & 24 & 266 & 24 & 5.10\% \\
ebay.com & 2623 & 1801 & -31.34\% & Yes & 162 & 50 & 50 & 50 & 100.00\% \\
elsevier.com & 906 & 160 & -82.34\% & Yes & 120 & 10 & 32 & 10 & 80.00\% \\
g1.com.br & 900 & 619 & -31.22\% & No & 225 & 10 & 202 & 0 & N/A \\
globo.com & 400 & 193 & -51.75\% & Yes & 80 & 10 & 20 & 10 & 85.71\% \\
google.com & 1421 & 981 & -30.96\% & Yes & 118 & 11 & 61 & 11 & 53.27\% \\
itau.com.br & 1111 & 410 & -63.10\% & No & 77 & 10 & 11 & 0 & N/A \\
magazineluiza.com.br & 3167 & 1115 & -64.79\% & Yes & 314 & 40 & 44 & 40 & 98.54\% \\
mercadolivre.com.br & 2401 & 1771 & -26.24\% & Yes & 136 & 50 & 52 & 50 & 97.67\% \\
reuters.com & 1202 & 480 & -60.07\% & Yes & 136 & 10 & 54 & 10 & 65.08\% \\
scopus.com & 4929 & 4688 & -4.89\% & Yes & 114 & 20 & 75 & 20 & 41.49\% \\
submarino.com.br & 2389 & 1268 & -46.92\% & Yes & 116 & 20 & 22 & 20 & 97.92\% \\
terra.com.br & 869 & 588 & -32.34\% & Yes & 122 & 50 & 76 & 50 & 63.89\% \\
valor.com.br & 514 & 126 & -75.49\% & No & 55 & 10 & 2 & 0 & N/A \\
webmotors.com.br & 2119 & 1361 & -35.77\% & Yes & 113 & 14 & 19 & 14 & 94.95\% \\
%wikipedia.com & 3224 & 1949 & -39.55\% & Yes & N/A & N/A & N/A & N/A & N/A \\
yahoo.com & 760 & 290 & -61.84\% & Yes & 67 & 10 & 10 & 10 & 100.00\% \\
youtube.com & 1424 & 674 & -52.67\% & Yes & 82 & 20 & 20 & 20 & 100.00\% \\
\hline
Média/Total &  &  & -46.22\% & 86.96\% &  &  &  &  & 77.03\% \\
\hline

\end{tabular}
\end{tiny}
\end{table}

\chapter{Conclusion}\label{ch:conclusion}

As shown in the results, the method we have proposed for page segmentation and noise
removal is very effective for some commercial/institutional web sites.
In most cases, a very large amount of noise is removed without compromising the
main content region. Also, when applied in conjunction with MDR, we can see that
the extraction precision is greatly improved.

In the situations where our algorithm fails, other techniques have to/should/could
be combined depending on the targeted application. In extreme cases, where a
page has either too homogeneous structure (so we can not find a split anywhere
along the TPS) or too heterogeneous structure (then the main content itself gets
split in several parts), the main content block could be detected using,
perhaps, semantic approaches.


%techniques, like the one presented in \cite{Adaptive07}, that labels a sequence 
%of blocks as $content$ or $notContent$ could be used to implement 
%line $6$ of algorithm \ref{alg:tpsfilter2}.

The algorithm shows outstanding performance, as it works very well for the majority 
of large commercial web sites we have tested. It also outcomes the limitations 
(training requirements, HTML tag dependency, manual labeling, among others) of previous 
works in the area of data cleaning, page segmentation and noise removal as mentioned in 
Section \ref{sec:relatedwork}.

\bibliographystyle{ufscThesis/ufsc-alf}
\bibliography{bibliografia}

\end{document}
